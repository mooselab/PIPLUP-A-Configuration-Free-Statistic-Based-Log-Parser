[
 {
  "iter": 1,
  "logs_to_query": [
   "Times: total = 4458, boot = 536, init = 83, finish = 3839",
   "Times: total = 950, boot = -81, init = 83, finish = 948",
   "Times: total = 165, boot = -77, init = 83, finish = 159"
  ],
  "logs_to_query_regex": [
   "Times: total = 4458, boot = 536, init = 83, finish = 3839",
   "Times: total = 950, boot = -81, init = 83, finish = 948",
   "Times: total = 165, boot = -77, init = 83, finish = 159"
  ],
  "llm_template": "Times: total = <*> boot = <*> init = <*> finish = <*>",
  "cluster_id": 200,
  "update_success": true,
  "template": "Times: total = <*>, boot = <*>, init = <*>, finish = <*>"
 },
 {
  "iter": 2,
  "logs_to_query": [
   "Got assigned task 0",
   "Got assigned task 723046",
   "Got assigned task 590140"
  ],
  "logs_to_query_regex": [
   "Got assigned task 0",
   "Got assigned task 723046",
   "Got assigned task 590140"
  ],
  "llm_template": "Got assigned task <*>",
  "cluster_id": 101,
  "update_success": true,
  "template": "Got assigned task <*>"
 },
 {
  "iter": 3,
  "logs_to_query": [
   "Running task 0.0 in stage 0.0 (TID 0)",
   "Running task 9.0 in stage 208.0 (TID 8337)",
   "Running task 30.0 in stage 318.0 (TID 13436)"
  ],
  "logs_to_query_regex": [
   "Running task 0.0 in stage 0.0 (TID 0)",
   "Running task 9.0 in stage 208.0 (TID 8337)",
   "Running task 30.0 in stage 318.0 (TID 13436)"
  ],
  "llm_template": "Running task <*> in stage <*> (TID <*>)",
  "cluster_id": 165,
  "update_success": true,
  "template": "Running task <*> in stage <*> (TID <*>)"
 },
 {
  "iter": 4,
  "logs_to_query": [
   "Finished task 1.0 in stage 0.0 (TID 1). 14372091 bytes result sent to driver",
   "Finished task 10.0 in stage 692.0 (TID 28450). 2364 bytes result sent to driver",
   "Finished task 21.0 in stage 208.0 (TID 8581). 2241 bytes result sent to driver"
  ],
  "logs_to_query_regex": [
   "Finished task 1.0 in stage 0.0 (TID 1). 14372091 bytes result sent to driver",
   "Finished task 10.0 in stage 692.0 (TID 28450). 2364 bytes result sent to driver",
   "Finished task 21.0 in stage 208.0 (TID 8581). 2241 bytes result sent to driver"
  ],
  "llm_template": "Finished task <*> in stage <*> (TID <*>). <*> bytes result sent to driver",
  "cluster_id": 207,
  "update_success": true,
  "template": "Finished task <*> in stage <*> (TID <*>). <*> bytes result sent to driver"
 },
 {
  "iter": 5,
  "logs_to_query": [
   "Found block rdd_2_9 locally",
   "Found block rdd_754_20 locally",
   "Found block rdd_10691_24 locally"
  ],
  "logs_to_query_regex": [
   "Found block rdd_2_9 locally",
   "Found block rdd_754_20 locally",
   "Found block rdd_10691_24 locally"
  ],
  "llm_template": "Found block <*> locally",
  "cluster_id": 112,
  "update_success": true,
  "template": "Found block <*> locally"
 },
 {
  "iter": 6,
  "logs_to_query": [
   "Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.7 KB, free 3.7 KB)",
   "Block rdd_3045_17 stored as bytes in memory (estimated size 354.0 B, free 9.0 MB)",
   "Block broadcast_6_piece210 stored as bytes in memory (estimated size 4.0 MB, free 4.0 GB)"
  ],
  "logs_to_query_regex": [
   "Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.7 KB, free 3.7 KB)",
   "Block rdd_3045_17 stored as bytes in memory (estimated size 354.0 B, free 9.0 MB)",
   "Block broadcast_6_piece210 stored as bytes in memory (estimated size 4.0 MB, free 4.0 GB)"
  ],
  "llm_template": "Block <*> stored as bytes in memory (estimated size <*> free <*>)",
  "cluster_id": 205,
  "update_success": true,
  "template": "Block <*> stored as bytes in memory (estimated size <*>, free <*>)"
 },
 {
  "iter": 7,
  "logs_to_query": [
   "Added broadcast_0_piece0 in memory on 10.10.34.23:48030 (size: 21.4 KB, free: 26.4 GB)",
   "Added broadcast_101_piece0 in memory on mesos-slave-09:47971 (size: 2.2 KB, free: 2.6 GB)",
   "Added broadcast_93_piece0 in memory on mesos-slave-18:49085 (size: 5.6 KB, free: 27.8 GB)"
  ],
  "logs_to_query_regex": [
   "Added broadcast_0_piece0 in memory on 10.10.34.23:48030 (size: 21.4 KB, free: 26.4 GB)",
   "Added broadcast_101_piece0 in memory on mesos-slave-09:47971 (size: 2.2 KB, free: 2.6 GB)",
   "Added broadcast_93_piece0 in memory on mesos-slave-18:49085 (size: 5.6 KB, free: 27.8 GB)"
  ],
  "llm_template": "Added <*> in memory on <*> (size: <*> free: <*>)",
  "cluster_id": 196,
  "update_success": true,
  "template": "Added <*> in memory on <*> (size: <*>, free: <*>)"
 },
 {
  "iter": 8,
  "logs_to_query": [
   "File Output Committer Algorithm version is 1"
  ],
  "logs_to_query_regex": [
   "File Output Committer Algorithm version is 1"
  ],
  "llm_template": "File Output Committer Algorithm version is <*>",
  "cluster_id": 158,
  "update_success": true,
  "template": "File Output Committer Algorithm version is <*>"
 },
 {
  "iter": 9,
  "logs_to_query": [
   "attempt_201706101524_0006_m_000009_128: Committed"
  ],
  "logs_to_query_regex": [
   "attempt_201706101524_0006_m_000009_128: Committed"
  ],
  "llm_template": "<*>: Committed",
  "cluster_id": 10,
  "update_success": true,
  "template": "<*>: Committed"
 },
 {
  "iter": 10,
  "logs_to_query": [
   "Saved output of task 'attempt_201706101524_0006_m_000001_120' to hdfs://10.10.34.11:9000/pjhe/test/1/_temporary/0/task_201706101524_0006_m_000001",
   "Saved output of task 'attempt_201706081746_2150_m_000029_86025' to hdfs://10.10.34.11:9000/pjhe/test/11/_temporary/0/task_201706081746_2150_m_000029",
   "Saved output of task 'attempt_201706082218_22545_m_000015_901853' to hdfs://10.10.34.11:9000/pjhe/test/571/_temporary/0/task_201706082218_22545_m_000015"
  ],
  "logs_to_query_regex": [
   "Saved output of task 'attempt_201706101524_0006_m_000001_120' to hdfs://10.10.34.11:9000/pjhe/test/1/_temporary/0/task_201706101524_0006_m_000001",
   "Saved output of task 'attempt_201706081746_2150_m_000029_86025' to hdfs://10.10.34.11:9000/pjhe/test/11/_temporary/0/task_201706081746_2150_m_000029",
   "Saved output of task 'attempt_201706082218_22545_m_000015_901853' to hdfs://10.10.34.11:9000/pjhe/test/571/_temporary/0/task_201706082218_22545_m_000015"
  ],
  "llm_template": "Saved output of task <*> to <*>",
  "cluster_id": 159,
  "update_success": true,
  "template": "Saved output of task <*> to <*>"
 },
 {
  "iter": 11,
  "logs_to_query": [
   "Partition rdd_2_3 not found, computing it",
   "Partition rdd_21667_24 not found, computing it",
   "Partition rdd_3154_6 not found, computing it"
  ],
  "logs_to_query_regex": [
   "Partition rdd_2_3 not found, computing it",
   "Partition rdd_21667_24 not found, computing it",
   "Partition rdd_3154_6 not found, computing it"
  ],
  "llm_template": "Partition <*> not found, computing it",
  "cluster_id": 141,
  "update_success": true,
  "template": "Partition <*> not found, computing it"
 },
 {
  "iter": 12,
  "logs_to_query": [
   "Removing RDD 456",
   "Removing RDD 10401",
   "Removing RDD 12556"
  ],
  "logs_to_query_regex": [
   "Removing RDD 456",
   "Removing RDD 10401",
   "Removing RDD 12556"
  ],
  "llm_template": "Removing RDD <*>",
  "cluster_id": 95,
  "update_success": true,
  "template": "Removing RDD <*>"
 },
 {
  "iter": 13,
  "logs_to_query": [
   "Block broadcast_1 stored as values in memory (estimated size 6.0 KB, free 9.7 KB)",
   "Block broadcast_12236 stored as values in memory (estimated size 9.5 KB, free 9.1 MB)",
   "Block broadcast_0 stored as values in memory (estimated size 440.0 B, free 440.0 B)"
  ],
  "logs_to_query_regex": [
   "Block broadcast_1 stored as values in memory (estimated size 6.0 KB, free 9.7 KB)",
   "Block broadcast_12236 stored as values in memory (estimated size 9.5 KB, free 9.1 MB)",
   "Block broadcast_0 stored as values in memory (estimated size 440.0 B, free 440.0 B)"
  ],
  "llm_template": "Block <*> stored as values in memory (estimated size <*> free <*>)",
  "cluster_id": 206,
  "update_success": true,
  "template": "Block <*> stored as values in memory (estimated size <*>, free <*>)"
 },
 {
  "iter": 14,
  "logs_to_query": [
   "Started reading broadcast variable 1",
   "Started reading broadcast variable 7808",
   "Started reading broadcast variable 3634"
  ],
  "logs_to_query_regex": [
   "Started reading broadcast variable 1",
   "Started reading broadcast variable 7808",
   "Started reading broadcast variable 3634"
  ],
  "llm_template": "Started reading broadcast variable <*>",
  "cluster_id": 121,
  "update_success": true,
  "template": "Started reading broadcast variable <*>"
 },
 {
  "iter": 15,
  "logs_to_query": [
   "Reading broadcast variable 1 took 120 ms",
   "Reading broadcast variable 400 took 3 ms",
   "Reading broadcast variable 1707 took 5 ms"
  ],
  "logs_to_query_regex": [
   "Reading broadcast variable 1 took 120 ms",
   "Reading broadcast variable 400 took 3 ms",
   "Reading broadcast variable 1707 took 5 ms"
  ],
  "llm_template": "Reading broadcast variable <*> took <*> ms",
  "cluster_id": 149,
  "update_success": true,
  "template": "Reading broadcast variable <*> took <*> ms"
 },
 {
  "iter": 16,
  "logs_to_query": [
   "Started 0 remote fetches in 7 ms",
   "Started 4 remote fetches in 30 ms",
   "Started 3 remote fetches in 10 ms"
  ],
  "logs_to_query_regex": [
   "Started 0 remote fetches in 7 ms",
   "Started 4 remote fetches in 30 ms",
   "Started 3 remote fetches in 10 ms"
  ],
  "llm_template": "Started <*> remote fetches in <*> ms",
  "cluster_id": 162,
  "update_success": true,
  "template": "Started <*> remote fetches in <*> ms"
 },
 {
  "iter": 17,
  "logs_to_query": [
   "Getting 2 non-empty blocks out of 2 blocks",
   "Getting 226 non-empty blocks out of 240 blocks",
   "Getting 315 non-empty blocks out of 396 blocks"
  ],
  "logs_to_query_regex": [
   "Getting 2 non-empty blocks out of 2 blocks",
   "Getting 226 non-empty blocks out of 240 blocks",
   "Getting 315 non-empty blocks out of 396 blocks"
  ],
  "llm_template": "Getting <*> non-empty blocks out of <*> blocks",
  "cluster_id": 169,
  "update_success": true,
  "template": "Getting <*> non-empty blocks out of <*> blocks"
 },
 {
  "iter": 18,
  "logs_to_query": [
   "Removed broadcast_1_piece0 on 10.10.34.16:42124 in memory (size: 3.7 KB, free: 36.4 GB)",
   "Removed broadcast_153_piece0 on mesos-slave-19:51361 in memory (size: 2.2 KB, free: 2.6 GB)",
   "Removed broadcast_56_piece0 on mesos-slave-25:38626 in memory (size: 4.1 KB, free: 27.8 GB)"
  ],
  "logs_to_query_regex": [
   "Removed broadcast_1_piece0 on 10.10.34.16:42124 in memory (size: 3.7 KB, free: 36.4 GB)",
   "Removed broadcast_153_piece0 on mesos-slave-19:51361 in memory (size: 2.2 KB, free: 2.6 GB)",
   "Removed broadcast_56_piece0 on mesos-slave-25:38626 in memory (size: 4.1 KB, free: 27.8 GB)"
  ],
  "llm_template": "Removed <*> on <*> in memory (size: <*> free: <*>)",
  "cluster_id": 196,
  "update_success": true,
  "template": "Removed <*> on <*> in memory (size: <*>, free: <*>)"
 },
 {
  "iter": 19,
  "logs_to_query": [
   "Starting task 0.0 in stage 0.0 (TID 0, mesos-master-1, partition 0,NODE_LOCAL, 2159 bytes)",
   "Starting task 14.0 in stage 14.0 (TID 584, mesos-master-1, partition 14,PROCESS_LOCAL, 1850809 bytes)",
   "Starting task 1.0 in stage 1.0 (TID 3, mesos-master-1, partition 1,PROCESS_LOCAL, 19689129 bytes)"
  ],
  "logs_to_query_regex": [
   "Starting task 0.0 in stage 0.0 (TID 0, mesos-master-1, partition 0,NODE_LOCAL, 2159 bytes)",
   "Starting task 14.0 in stage 14.0 (TID 584, mesos-master-1, partition 14,PROCESS_LOCAL, 1850809 bytes)",
   "Starting task 1.0 in stage 1.0 (TID 3, mesos-master-1, partition 1,PROCESS_LOCAL, 19689129 bytes)"
  ],
  "llm_template": "Starting task <*> in stage <*> (TID <*> mesos-master-<*>, partition <*>, <*> bytes)",
  "cluster_id": 203,
  "update_success": true,
  "template": "Starting task <*> in stage <*> (TID <*>, <*>, partition <*>,<*>, <*> bytes)"
 },
 {
  "iter": 20,
  "logs_to_query": [
   "Finished task 1.0 in stage 0.0 (TID 1) in 5931 ms on mesos-master-1 (1/2)",
   "Finished task 0.0 in stage 1.0 (TID 2) in 1743 ms on mesos-slave-13 (2/3)",
   "Finished task 2.0 in stage 1.0 (TID 4) in 973 ms on mesos-master-1 (3/40)"
  ],
  "logs_to_query_regex": [
   "Finished task 1.0 in stage 0.0 (TID 1) in 5931 ms on mesos-master-1 (1/2)",
   "Finished task 0.0 in stage 1.0 (TID 2) in 1743 ms on mesos-slave-13 (2/3)",
   "Finished task 2.0 in stage 1.0 (TID 4) in 973 ms on mesos-master-1 (3/40)"
  ],
  "llm_template": "Finished task <*> in stage <*> (TID <*>) in <*> ms on <*> (<*>)",
  "cluster_id": 208,
  "update_success": true,
  "template": "Finished task <*> in stage <*> (TID <*>) in <*> ms on <*> (<*>)"
 },
 {
  "iter": 21,
  "logs_to_query": [
   "Starting task 0.0 in stage 0.0 (TID 0, mesos-slave-20, partition 0,RACK_LOCAL, 2159 bytes)",
   "Starting task 1.0 in stage 1.0 (TID 3, mesos-slave-20, partition 1,PROCESS_LOCAL, 1242986 bytes)",
   "Starting task 2.0 in stage 2.0 (TID 8, mesos-slave-20, partition 2,PROCESS_LOCAL, 14758160 bytes)"
  ],
  "logs_to_query_regex": [
   "Starting task 0.0 in stage 0.0 (TID 0, mesos-slave-20, partition 0,RACK_LOCAL, 2159 bytes)",
   "Starting task 1.0 in stage 1.0 (TID 3, mesos-slave-20, partition 1,PROCESS_LOCAL, 1242986 bytes)",
   "Starting task 2.0 in stage 2.0 (TID 8, mesos-slave-20, partition 2,PROCESS_LOCAL, 14758160 bytes)"
  ],
  "llm_template": "Starting task <*> in stage <*> (TID <*> partition <*>, <*> bytes)",
  "cluster_id": 203,
  "update_success": true,
  "template": "Starting task <*> in stage <*> (TID <*>, <*>, partition <*>,<*>, <*> bytes)"
 },
 {
  "iter": 22,
  "logs_to_query": [
   "Don't have map outputs for shuffle 0, fetching them",
   "Don't have map outputs for shuffle 13, fetching them",
   "Don't have map outputs for shuffle 51, fetching them"
  ],
  "logs_to_query_regex": [
   "Don't have map outputs for shuffle 0, fetching them",
   "Don't have map outputs for shuffle 13, fetching them",
   "Don't have map outputs for shuffle 51, fetching them"
  ],
  "llm_template": "Don't have map outputs for shuffle <*> fetching them",
  "cluster_id": 178,
  "update_success": true,
  "template": "Don't have map outputs for shuffle <*>, fetching them"
 },
 {
  "iter": 23,
  "logs_to_query": [
   "Changing view acls to: yarn,curi",
   "Changing view acls to: yarn,yxsu",
   "Changing modify acls to: yarn,curi"
  ],
  "logs_to_query_regex": [
   "Changing view acls to: yarn,curi",
   "Changing view acls to: yarn,yxsu",
   "Changing modify acls to: yarn,curi"
  ],
  "llm_template": "Changing view acls to: yarn,<*>",
  "cluster_id": 119,
  "update_success": true,
  "template": "Changing view acls to: <*>,<*>"
 },
 {
  "iter": 24,
  "logs_to_query": [
   "Input split: hdfs://10.10.34.11:9000/user/niuxy/com-amazon.ungraph.txt:0+6292942",
   "Input split: Paths:/yxsu/dataset/ImageNet/ILSVRC2012_img_test/ILSVRC2012_test_00068234.JPEG:0+120820,/yxsu/dataset/ImageNet/ILSVRC2012_img_test/ILSVRC2012_test_00068243.JPEG:0+167078",
   "Input split: Paths:/yxsu/dataset/ImageNet/ILSVRC2012_img_test/ILSVRC2012_test_00004651.JPEG:0+90898,/yxsu/dataset/ImageNet/ILSVRC2012_img_test/ILSVRC2012_test_00004657.JPEG:0+151959"
  ],
  "logs_to_query_regex": [
   "Input split: hdfs://10.10.34.11:9000/user/niuxy/com-amazon.ungraph.txt:0+6292942",
   "Input split: Paths:/yxsu/dataset/ImageNet/ILSVRC2012_img_test/ILSVRC2012_test_00068234.JPEG:0+120820,/yxsu/dataset/ImageNet/ILSVRC2012_img_test/ILSVRC2012_test_00068243.JPEG:0+167078",
   "Input split: Paths:/yxsu/dataset/ImageNet/ILSVRC2012_img_test/ILSVRC2012_test_00004651.JPEG:0+90898,/yxsu/dataset/ImageNet/ILSVRC2012_img_test/ILSVRC2012_test_00004657.JPEG:0+151959"
  ],
  "llm_template": "Input split: Paths:<*>",
  "cluster_id": 92,
  "update_success": true,
  "template": "Input split: <*>+<*>"
 },
 {
  "iter": 25,
  "logs_to_query": [
   "Input split: hdfs://10.10.34.11:9000/user/niuxy/com-amazon.ungraph.txt:0+6292942",
   "Input split: hdfs://10.10.34.11:9000/pjhe/logs/2kBGL.log:32604+8151",
   "Input split: hdfs://10.10.34.11:9000/pjhe/logs/2kBGL.log:97812+8151"
  ],
  "logs_to_query_regex": [
   "Input split: hdfs://10.10.34.11:9000/user/niuxy/com-amazon.ungraph.txt:0+6292942",
   "Input split: hdfs://10.10.34.11:9000/pjhe/logs/2kBGL.log:32604+8151",
   "Input split: hdfs://10.10.34.11:9000/pjhe/logs/2kBGL.log:97812+8151"
  ],
  "llm_template": "Input split: hdfs://<*>",
  "cluster_id": 92,
  "update_success": true,
  "template": "Input split: <*>+<*>"
 },
 {
  "iter": 26,
  "logs_to_query": [
   "Changing modify acls to: yarn,curi",
   "Changing modify acls to: yarn,yxsu"
  ],
  "logs_to_query_regex": [
   "Changing modify acls to: yarn,curi",
   "Changing modify acls to: yarn,yxsu"
  ],
  "llm_template": "Changing modify acls to: yarn,<*>",
  "cluster_id": 119,
  "update_success": true,
  "template": "Changing modify acls to: <*>,<*>"
 },
 {
  "iter": 27,
  "logs_to_query": [
   "SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(yarn, curi); users with modify permissions: Set(yarn, curi)",
   "SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(yarn, yxsu); users with modify permissions: Set(yarn, yxsu)"
  ],
  "logs_to_query_regex": [
   "SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(yarn, curi); users with modify permissions: Set(yarn, curi)",
   "SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(yarn, yxsu); users with modify permissions: Set(yarn, yxsu)"
  ],
  "llm_template": "SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(yarn, <*>); users with modify permissions: Set(yarn, <*>)",
  "cluster_id": 215,
  "update_success": true,
  "template": "SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(<*>, <*>); users with modify permissions: Set(<*>, <*>)"
 },
 {
  "iter": 28,
  "logs_to_query": [
   "Successfully started service 'sparkExecutorActorSystem' on port 40424.",
   "Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59755.",
   "Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43120."
  ],
  "logs_to_query_regex": [
   "Successfully started service 'sparkExecutorActorSystem' on port 40424.",
   "Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59755.",
   "Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43120."
  ],
  "llm_template": "Successfully started service <*> on port <*>.",
  "cluster_id": 147,
  "update_success": true,
  "template": "Successfully started service <*> on port <*>."
 },
 {
  "iter": 29,
  "logs_to_query": [
   "Updating epoch to 1 and clearing cache",
   "Updating epoch to 162 and clearing cache",
   "Updating epoch to 152 and clearing cache"
  ],
  "logs_to_query_regex": [
   "Updating epoch to 1 and clearing cache",
   "Updating epoch to 162 and clearing cache",
   "Updating epoch to 152 and clearing cache"
  ],
  "llm_template": "Updating epoch to <*> and clearing cache",
  "cluster_id": 160,
  "update_success": true,
  "template": "Updating epoch to <*> and clearing cache"
 },
 {
  "iter": 30,
  "logs_to_query": [
   "Shutdown hook called"
  ],
  "logs_to_query_regex": [
   "Shutdown hook called"
  ],
  "llm_template": "Shutdown hook called",
  "cluster_id": 12,
  "update_success": true,
  "template": "Shutdown hook called"
 },
 {
  "iter": 31,
  "logs_to_query": [
   "Got the output locations"
  ],
  "logs_to_query_regex": [
   "Got the output locations"
  ],
  "llm_template": "Got the output locations",
  "cluster_id": 114,
  "update_success": true,
  "template": "Got the output locations"
 },
 {
  "iter": 32,
  "logs_to_query": [
   "Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.10.34.11:43537)",
   "Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.10.34.11:51535)",
   "Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.10.34.36:44803)"
  ],
  "logs_to_query_regex": [
   "Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.10.34.11:43537)",
   "Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.10.34.11:51535)",
   "Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.10.34.36:44803)"
  ],
  "llm_template": "Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@<*>)",
  "cluster_id": 161,
  "update_success": true,
  "template": "Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://<*>)"
 },
 {
  "iter": 33,
  "logs_to_query": [
   "Container request (host: Any, capability: <memory:22528, vCores:8>)",
   "Container request (host: Any, capability: <memory:28160, vCores:6>)",
   "Container request (host: Any, capability: <memory:6758, vCores:5>)"
  ],
  "logs_to_query_regex": [
   "Container request (host: Any, capability: <memory:22528, vCores:8>)",
   "Container request (host: Any, capability: <memory:28160, vCores:6>)",
   "Container request (host: Any, capability: <memory:6758, vCores:5>)"
  ],
  "llm_template": "Container request (host: Any, capability: <memory:<*>, vCores:<*>)",
  "cluster_id": 150,
  "update_success": true,
  "template": "Container request (host: <*>, capability: <memory:<*>, vCores:<*>)"
 },
 {
  "iter": 34,
  "logs_to_query": [
   "Found inactive connection to mesos-slave-20/10.10.34.30:33900, creating a new one.",
   "Found inactive connection to mesos-slave-21/10.10.34.31:51927, creating a new one.",
   "Found inactive connection to mesos-slave-26/10.10.34.36:39635, creating a new one."
  ],
  "logs_to_query_regex": [
   "Found inactive connection to mesos-slave-20/10.10.34.30:33900, creating a new one.",
   "Found inactive connection to mesos-slave-21/10.10.34.31:51927, creating a new one.",
   "Found inactive connection to mesos-slave-26/10.10.34.36:39635, creating a new one."
  ],
  "llm_template": "Found inactive connection to mesos-slave-<*>, creating a new one.",
  "cluster_id": 174,
  "update_success": true,
  "template": "Found inactive connection to <*>, creating a new one."
 },
 {
  "iter": 35,
  "logs_to_query": [
   "Registered signal handlers for [TERM, HUP, INT]"
  ],
  "logs_to_query_regex": [
   "Registered signal handlers for [TERM, HUP, INT]"
  ],
  "llm_template": "Registered signal handlers for [TERM, HUP, INT]",
  "cluster_id": 146,
  "update_success": true,
  "template": "Registered signal handlers for <*>"
 },
 {
  "iter": 36,
  "logs_to_query": [
   "Starting Executor Container"
  ],
  "logs_to_query_regex": [
   "Starting Executor Container"
  ],
  "llm_template": "Starting Executor Container",
  "cluster_id": 14,
  "update_success": true,
  "template": "Starting Executor Container"
 },
 {
  "iter": 37,
  "logs_to_query": [
   "Setting up ContainerLaunchContext"
  ],
  "logs_to_query_regex": [
   "Setting up ContainerLaunchContext"
  ],
  "llm_template": "Setting up ContainerLaunchContext",
  "cluster_id": 15,
  "update_success": true,
  "template": "Setting up ContainerLaunchContext"
 },
 {
  "iter": 38,
  "logs_to_query": [
   "Preparing Local resources"
  ],
  "logs_to_query_regex": [
   "Preparing Local resources"
  ],
  "llm_template": "Preparing Local resources",
  "cluster_id": 16,
  "update_success": true,
  "template": "Preparing Local resources"
 },
 {
  "iter": 39,
  "logs_to_query": [
   "Launching ExecutorRunnable. driverUrl: spark://CoarseGrainedScheduler@10.10.34.23:47891, executorHostname: mesos-slave-19",
   "Launching ExecutorRunnable. driverUrl: spark://CoarseGrainedScheduler@10.10.34.11:32921, executorHostname: mesos-master-2",
   "Launching ExecutorRunnable. driverUrl: spark://CoarseGrainedScheduler@10.10.34.11:39185, executorHostname: mesos-slave-20"
  ],
  "logs_to_query_regex": [
   "Launching ExecutorRunnable. driverUrl: spark://CoarseGrainedScheduler@10.10.34.23:47891, executorHostname: mesos-slave-19",
   "Launching ExecutorRunnable. driverUrl: spark://CoarseGrainedScheduler@10.10.34.11:32921, executorHostname: mesos-master-2",
   "Launching ExecutorRunnable. driverUrl: spark://CoarseGrainedScheduler@10.10.34.11:39185, executorHostname: mesos-slave-20"
  ],
  "llm_template": "Launching ExecutorRunnable. driverUrl: spark://CoarseGrainedScheduler@<*>, executorHostname: <*>",
  "cluster_id": 137,
  "update_success": true,
  "template": "Launching ExecutorRunnable. driverUrl: <*>, executorHostname: <*>"
 },
 {
  "iter": 40,
  "logs_to_query": [
   "Launching container container_1485248649253_0020_02_000002 for on host mesos-slave-19",
   "Launching container container_1485248649253_0004_01_000002 for on host mesos-slave-05",
   "Launching container container_1485248649253_0130_01_000006 for on host mesos-slave-27"
  ],
  "logs_to_query_regex": [
   "Launching container container_1485248649253_0020_02_000002 for on host mesos-slave-19",
   "Launching container container_1485248649253_0004_01_000002 for on host mesos-slave-05",
   "Launching container container_1485248649253_0130_01_000006 for on host mesos-slave-27"
  ],
  "llm_template": "Launching container <*> for on host mesos-slave-<*>",
  "cluster_id": 151,
  "update_success": true,
  "template": "Launching container <*> for on host <*>"
 },
 {
  "iter": 41,
  "logs_to_query": [
   "Prepared Local resources Map(__spark__.jar -> resource { scheme: \"hdfs\" host: \"10.10.34.11\" port: 9000 file: \"/user/curi/.sparkStaging/application_1485248649253_0020/spark-assembly-1.6.0-hadoop2.2.0.jar\" } size: 109525492 timestamp: 1489498812773 type: FILE visibility: PRIVATE, pyspark.zip -> resource { scheme: \"hdfs\" host: \"10.10.34.11\" port: 9000 file: \"/user/curi/.sparkStaging/application_1485248649253_0020/pyspark.zip\" } size: 355358 timestamp: 1489498812856 type: FILE visibility: PRIVATE, py4j-0.9-src.zip -> resource { scheme: \"hdfs\" host: \"10.10.34.11\" port: 9000 file: \"/user/curi/.sparkStaging/application_1485248649253_0020/py4j-0.9-src.zip\" } size: 44846 timestamp: 1489498812874 type: FILE visibility: PRIVATE)",
   "Prepared Local resources Map(__spark__.jar -> resource { scheme: \"hdfs\" host: \"10.10.34.11\" port: 9000 file: \"/user/curi/.sparkStaging/application_1485248649253_0159/spark-assembly-1.6.0-hadoop2.2.0.jar\" } size: 109525492 timestamp: 1497078835931 type: FILE visibility: PRIVATE, pyspark.zip -> resource { scheme: \"hdfs\" host: \"10.10.34.11\" port: 9000 file: \"/user/curi/.sparkStaging/application_1485248649253_0159/pyspark.zip\" } size: 355358 timestamp: 1497078836001 type: FILE visibility: PRIVATE, py4j-0.9-src.zip -> resource { scheme: \"hdfs\" host: \"10.10.34.11\" port: 9000 file: \"/user/curi/.sparkStaging/application_1485248649253_0159/py4j-0.9-src.zip\" } size: 44846 timestamp: 1497078836021 type: FILE visibility: PRIVATE)",
   "Prepared Local resources Map(__spark__.jar -> resource { scheme: \"hdfs\" host: \"10.10.34.11\" port: 9000 file: \"/user/curi/.sparkStaging/application_1485248649253_0174/spark-assembly-1.6.0-hadoop2.2.0.jar\" } size: 109525492 timestamp: 1501137161217 type: FILE visibility: PRIVATE, pyspark.zip -> resource { scheme: \"hdfs\" host: \"10.10.34.11\" port: 9000 file: \"/user/curi/.sparkStaging/application_1485248649253_0174/pyspark.zip\" } size: 355358 timestamp: 1501137161295 type: FILE visibility: PRIVATE, py4j-0.9-src.zip -> resource { scheme: \"hdfs\" host: \"10.10.34.11\" port: 9000 file: \"/user/curi/.sparkStaging/application_1485248649253_0174/py4j-0.9-src.zip\" } size: 44846 timestamp: 1501137161411 type: FILE visibility: PRIVATE)"
  ],
  "logs_to_query_regex": [
   "Prepared Local resources Map(__spark__.jar -> resource { scheme: \"hdfs\" host: \"10.10.34.11\" port: 9000 file: \"/user/curi/.sparkStaging/application_1485248649253_0020/spark-assembly-1.6.0-hadoop2.2.0.jar\" } size: 109525492 timestamp: 1489498812773 type: FILE visibility: PRIVATE, pyspark.zip -> resource { scheme: \"hdfs\" host: \"10.10.34.11\" port: 9000 file: \"/user/curi/.sparkStaging/application_1485248649253_0020/pyspark.zip\" } size: 355358 timestamp: 1489498812856 type: FILE visibility: PRIVATE, py4j-0.9-src.zip -> resource { scheme: \"hdfs\" host: \"10.10.34.11\" port: 9000 file: \"/user/curi/.sparkStaging/application_1485248649253_0020/py4j-0.9-src.zip\" } size: 44846 timestamp: 1489498812874 type: FILE visibility: PRIVATE)",
   "Prepared Local resources Map(__spark__.jar -> resource { scheme: \"hdfs\" host: \"10.10.34.11\" port: 9000 file: \"/user/curi/.sparkStaging/application_1485248649253_0159/spark-assembly-1.6.0-hadoop2.2.0.jar\" } size: 109525492 timestamp: 1497078835931 type: FILE visibility: PRIVATE, pyspark.zip -> resource { scheme: \"hdfs\" host: \"10.10.34.11\" port: 9000 file: \"/user/curi/.sparkStaging/application_1485248649253_0159/pyspark.zip\" } size: 355358 timestamp: 1497078836001 type: FILE visibility: PRIVATE, py4j-0.9-src.zip -> resource { scheme: \"hdfs\" host: \"10.10.34.11\" port: 9000 file: \"/user/curi/.sparkStaging/application_1485248649253_0159/py4j-0.9-src.zip\" } size: 44846 timestamp: 1497078836021 type: FILE visibility: PRIVATE)",
   "Prepared Local resources Map(__spark__.jar -> resource { scheme: \"hdfs\" host: \"10.10.34.11\" port: 9000 file: \"/user/curi/.sparkStaging/application_1485248649253_0174/spark-assembly-1.6.0-hadoop2.2.0.jar\" } size: 109525492 timestamp: 1501137161217 type: FILE visibility: PRIVATE, pyspark.zip -> resource { scheme: \"hdfs\" host: \"10.10.34.11\" port: 9000 file: \"/user/curi/.sparkStaging/application_1485248649253_0174/pyspark.zip\" } size: 355358 timestamp: 1501137161295 type: FILE visibility: PRIVATE, py4j-0.9-src.zip -> resource { scheme: \"hdfs\" host: \"10.10.34.11\" port: 9000 file: \"/user/curi/.sparkStaging/application_1485248649253_0174/py4j-0.9-src.zip\" } size: 44846 timestamp: 1501137161411 type: FILE visibility: PRIVATE)"
  ],
  "llm_template": "Prepared Local resources Map(__spark__.jar -> resource <*>\" port: <*> file: <*> } size: <*> timestamp: <*> type: FILE visibility: PRIVATE, pyspark.zip -> resource <*>\" port: <*> file: <*> } size: <*> timestamp: <*> type: FILE visibility: PRIVATE, py4j-<*>-src.zip -> resource <*>\" port: <*> file: <*> } size: <*> timestamp: <*> type: FILE visibility: PRIVATE)",
  "cluster_id": 238,
  "update_success": true,
  "template": "Prepared Local resources Map(__spark__.jar -> resource { scheme: <*> host: <*> port: <*> file: <*> } size: <*> timestamp: <*> type: <*> visibility: <*>, pyspark.zip -> resource { scheme: <*> host: <*> port: <*> file: <*> } size: <*> timestamp: <*> type: <*> visibility: <*>, py4j-<*>-src.zip -> resource { scheme: <*> host: <*> port: <*> file: <*> } size: <*> timestamp: <*> type: <*> visibility: <*>)"
 },
 {
  "iter": 42,
  "logs_to_query": [
   "Received new token for : mesos-slave-19:35680",
   "Received new token for : mesos-slave-29:38147",
   "Received new token for : mesos-slave-23:58316"
  ],
  "logs_to_query_regex": [
   "Received new token for : mesos-slave-19:35680",
   "Received new token for : mesos-slave-29:38147",
   "Received new token for : mesos-slave-23:58316"
  ],
  "llm_template": "Received new token for : <*>",
  "cluster_id": 136,
  "update_success": true,
  "template": "Received new token for : <*>"
 },
 {
  "iter": 43,
  "logs_to_query": [
   "Slf4jLogger started"
  ],
  "logs_to_query_regex": [
   "Slf4jLogger started"
  ],
  "llm_template": "Slf4jLogger started",
  "cluster_id": 0,
  "update_success": true,
  "template": "Slf4jLogger started"
 },
 {
  "iter": 44,
  "logs_to_query": [
   "Starting remoting"
  ],
  "logs_to_query_regex": [
   "Starting remoting"
  ],
  "llm_template": "Starting remoting",
  "cluster_id": 1,
  "update_success": true,
  "template": "Starting remoting"
 },
 {
  "iter": 45,
  "logs_to_query": [
   "Remoting started; listening on addresses :[akka.tcp://sparkExecutorActorSystem@mesos-master-1:40424]",
   "Remoting started; listening on addresses :[akka.tcp://sparkExecutorActorSystem@mesos-master-1:36373]",
   "Remoting started; listening on addresses :[akka.tcp://sparkExecutorActorSystem@mesos-slave-14:53880]"
  ],
  "logs_to_query_regex": [
   "Remoting started; listening on addresses :[akka.tcp://sparkExecutorActorSystem@mesos-master-1:40424]",
   "Remoting started; listening on addresses :[akka.tcp://sparkExecutorActorSystem@mesos-master-1:36373]",
   "Remoting started; listening on addresses :[akka.tcp://sparkExecutorActorSystem@mesos-slave-14:53880]"
  ],
  "llm_template": "Remoting started; listening on addresses :[akka.tcp://sparkExecutorActorSystem@<*>]",
  "cluster_id": 133,
  "update_success": true,
  "template": "Remoting started; listening on addresses :[<*>]"
 },
 {
  "iter": 46,
  "logs_to_query": [
   "Registered BlockManager"
  ],
  "logs_to_query_regex": [
   "Registered BlockManager"
  ],
  "llm_template": "Registered BlockManager",
  "cluster_id": 2,
  "update_success": true,
  "template": "Registered BlockManager"
 },
 {
  "iter": 47,
  "logs_to_query": [
   "Trying to register BlockManager"
  ],
  "logs_to_query_regex": [
   "Trying to register BlockManager"
  ],
  "llm_template": "Trying to register BlockManager",
  "cluster_id": 100,
  "update_success": true,
  "template": "Trying to register BlockManager"
 },
 {
  "iter": 48,
  "logs_to_query": [
   "Server created on 48869",
   "Server created on 44677",
   "Server created on 60885"
  ],
  "logs_to_query_regex": [
   "Server created on 48869",
   "Server created on 44677",
   "Server created on 60885"
  ],
  "llm_template": "Server created on <*>",
  "cluster_id": 116,
  "update_success": true,
  "template": "Server created on <*>"
 },
 {
  "iter": 49,
  "logs_to_query": [
   "Created local directory at /opt/hdfs/nodemanager/usercache/curi/appcache/application_1485248649253_0020/blockmgr-416b8a6f-4ed7-49bb-b2f3-71d51930d48d",
   "Created local directory at /opt/hdfs/nodemanager/usercache/curi/appcache/application_1485248649253_0179/blockmgr-374f57ab-7657-4682-a6a0-9eff3c3f7cb0",
   "Created local directory at /opt/hdfs/nodemanager/usercache/curi/appcache/application_1485248649253_0072/blockmgr-ad553ad0-5432-432e-8721-ed722a30028f"
  ],
  "logs_to_query_regex": [
   "Created local directory at /opt/hdfs/nodemanager/usercache/curi/appcache/application_1485248649253_0020/blockmgr-416b8a6f-4ed7-49bb-b2f3-71d51930d48d",
   "Created local directory at /opt/hdfs/nodemanager/usercache/curi/appcache/application_1485248649253_0179/blockmgr-374f57ab-7657-4682-a6a0-9eff3c3f7cb0",
   "Created local directory at /opt/hdfs/nodemanager/usercache/curi/appcache/application_1485248649253_0072/blockmgr-ad553ad0-5432-432e-8721-ed722a30028f"
  ],
  "llm_template": "Created local directory at /opt/hdfs/nodemanager/usercache/curi/appcache/application_<*>/blockmgr-<*>",
  "cluster_id": 120,
  "update_success": true,
  "template": "Created local directory at <*>"
 },
 {
  "iter": 50,
  "logs_to_query": [
   "MemoryStore started with capacity 14.2 GB",
   "MemoryStore started with capacity 19.8 GB",
   "MemoryStore started with capacity 530.3 MB"
  ],
  "logs_to_query_regex": [
   "MemoryStore started with capacity 14.2 GB",
   "MemoryStore started with capacity 19.8 GB",
   "MemoryStore started with capacity 530.3 MB"
  ],
  "llm_template": "MemoryStore started with capacity <*>",
  "cluster_id": 134,
  "update_success": true,
  "template": "MemoryStore started with capacity <*>"
 },
 {
  "iter": 51,
  "logs_to_query": [
   "Retrying fetch (1/3) for 1 outstanding blocks after 5000 ms",
   "Retrying fetch (2/3) for 9 outstanding blocks after 5000 ms",
   "Retrying fetch (3/3) for 2 outstanding blocks after 5000 ms"
  ],
  "logs_to_query_regex": [
   "Retrying fetch (1/3) for 1 outstanding blocks after 5000 ms",
   "Retrying fetch (2/3) for 9 outstanding blocks after 5000 ms",
   "Retrying fetch (3/3) for 2 outstanding blocks after 5000 ms"
  ],
  "llm_template": "Retrying fetch (<*>) for <*> outstanding blocks after <*> ms",
  "cluster_id": 184,
  "update_success": true,
  "template": "Retrying fetch (<*>) for <*> outstanding blocks after <*> ms"
 },
 {
  "iter": 52,
  "logs_to_query": [
   "Successfully registered with driver"
  ],
  "logs_to_query_regex": [
   "Successfully registered with driver"
  ],
  "llm_template": "Successfully registered with driver",
  "cluster_id": 99,
  "update_success": true,
  "template": "Successfully registered with driver"
 },
 {
  "iter": 53,
  "logs_to_query": [
   "Starting executor ID 6 on host mesos-master-1",
   "Starting executor ID 56 on host mesos-slave-22",
   "Starting executor ID 3 on host mesos-slave-23"
  ],
  "logs_to_query_regex": [
   "Starting executor ID 6 on host mesos-master-1",
   "Starting executor ID 56 on host mesos-slave-22",
   "Starting executor ID 3 on host mesos-slave-23"
  ],
  "llm_template": "Starting executor ID <*> on host <*>",
  "cluster_id": 148,
  "update_success": true,
  "template": "Starting executor ID <*> on host <*>"
 },
 {
  "iter": 54,
  "logs_to_query": [
   "Connecting to driver: spark://CoarseGrainedScheduler@10.10.34.23:47891",
   "Connecting to driver: spark://CoarseGrainedScheduler@10.10.34.16:37380",
   "Connecting to driver: spark://CoarseGrainedScheduler@10.10.34.11:53494"
  ],
  "logs_to_query_regex": [
   "Connecting to driver: spark://CoarseGrainedScheduler@10.10.34.23:47891",
   "Connecting to driver: spark://CoarseGrainedScheduler@10.10.34.16:37380",
   "Connecting to driver: spark://CoarseGrainedScheduler@10.10.34.11:53494"
  ],
  "llm_template": "Connecting to driver: spark://CoarseGrainedScheduler@<*>",
  "cluster_id": 115,
  "update_success": true,
  "template": "Connecting to driver: spark://<*>"
 },
 {
  "iter": 55,
  "logs_to_query": [
   "Exception while beginning fetch of 1 outstanding blocks (after 1 retries)",
   "Exception while beginning fetch of 2 outstanding blocks (after 2 retries)",
   "Exception while beginning fetch of 9 outstanding blocks (after 3 retries)"
  ],
  "logs_to_query_regex": [
   "Exception while beginning fetch of 1 outstanding blocks (after 1 retries)",
   "Exception while beginning fetch of 2 outstanding blocks (after 2 retries)",
   "Exception while beginning fetch of 9 outstanding blocks (after 3 retries)"
  ],
  "llm_template": "Exception while beginning fetch of <*> outstanding blocks (after <*> retries)",
  "cluster_id": 191,
  "update_success": true,
  "template": "Exception while beginning fetch of <*> outstanding blocks (after <*> retries)"
 },
 {
  "iter": 56,
  "logs_to_query": [
   "mapred.tip.id is deprecated. Instead, use mapreduce.task.id",
   "mapred.job.id is deprecated. Instead, use mapreduce.job.id",
   "mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id"
  ],
  "logs_to_query_regex": [
   "mapred.tip.id is deprecated. Instead, use mapreduce.task.id",
   "mapred.job.id is deprecated. Instead, use mapreduce.job.id",
   "mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id"
  ],
  "llm_template": "mapred.<*> is deprecated. Instead, use mapreduce.<*>",
  "cluster_id": 135,
  "update_success": true,
  "template": "<*> is deprecated. Instead, use <*>"
 },
 {
  "iter": 57,
  "logs_to_query": [
   "Lost task 7.2 in stage 1.0 (TID 53, mesos-slave-27): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 54.7 GB of 46.2 GB virtual memory used. Consider boosting spark.yarn.executor.memoryOverhead.",
   "Lost task 52.0 in stage 11.1 (TID 1003, mesos-slave-13): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 31.3 GB of 28 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.",
   "Lost task 17.0 in stage 3.0 (TID 68, mesos-slave-25): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 28.4 GB of 22 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead."
  ],
  "logs_to_query_regex": [
   "Lost task 7.2 in stage 1.0 (TID 53, mesos-slave-27): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 54.7 GB of 46.2 GB virtual memory used. Consider boosting spark.yarn.executor.memoryOverhead.",
   "Lost task 52.0 in stage 11.1 (TID 1003, mesos-slave-13): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 31.3 GB of 28 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.",
   "Lost task 17.0 in stage 3.0 (TID 68, mesos-slave-25): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 28.4 GB of 22 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead."
  ],
  "llm_template": "Lost task <*> in stage <*> (TID <*> mesos-slave-<*>): ExecutorLostFailure (executor <*> exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. <*> of <*> physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.",
  "cluster_id": 235,
  "update_success": true,
  "template": "Lost task <*> in stage <*> (TID <*>, <*>): ExecutorLostFailure (executor <*> exited caused by one of the running tasks) Reason: Container killed by <*> for exceeding memory limits. <*> of <*> <*> memory used. Consider boosting <*>."
 },
 {
  "iter": 58,
  "logs_to_query": [
   "Opening proxy : mesos-slave-18:56143",
   "Opening proxy : mesos-slave-29:49983",
   "Opening proxy : mesos-slave-10:36903"
  ],
  "logs_to_query_regex": [
   "Opening proxy : mesos-slave-18:56143",
   "Opening proxy : mesos-slave-29:49983",
   "Opening proxy : mesos-slave-10:36903"
  ],
  "llm_template": "Opening proxy : <*>",
  "cluster_id": 117,
  "update_success": true,
  "template": "Opening proxy : <*>"
 },
 {
  "iter": 59,
  "logs_to_query": [
   "Driver 10.10.34.23:47891 disassociated! Shutting down.",
   "Driver 10.10.34.15:53495 disassociated! Shutting down.",
   "Driver 10.10.34.11:50076 disassociated! Shutting down."
  ],
  "logs_to_query_regex": [
   "Driver 10.10.34.23:47891 disassociated! Shutting down.",
   "Driver 10.10.34.15:53495 disassociated! Shutting down.",
   "Driver 10.10.34.11:50076 disassociated! Shutting down."
  ],
  "llm_template": "Driver <*> disassociated! Shutting down.",
  "cluster_id": 123,
  "update_success": true,
  "template": "Driver <*> disassociated! Shutting down."
 },
 {
  "iter": 60,
  "logs_to_query": [
   "An unknown (mesos-slave-13:47891) driver disconnected.",
   "An unknown (mesos-slave-14:48530) driver disconnected.",
   "An unknown (mesos-master-1:59844) driver disconnected."
  ],
  "logs_to_query_regex": [
   "An unknown (mesos-slave-13:47891) driver disconnected.",
   "An unknown (mesos-slave-14:48530) driver disconnected.",
   "An unknown (mesos-master-1:59844) driver disconnected."
  ],
  "llm_template": "An unknown <*> driver disconnected.",
  "cluster_id": 122,
  "update_success": true,
  "template": "An unknown (<*>) driver disconnected."
 },
 {
  "iter": 61,
  "logs_to_query": [
   "MemoryStore cleared"
  ],
  "logs_to_query_regex": [
   "MemoryStore cleared"
  ],
  "llm_template": "MemoryStore cleared",
  "cluster_id": 3,
  "update_success": true,
  "template": "MemoryStore cleared"
 },
 {
  "iter": 62,
  "logs_to_query": [
   "BlockManager stopped"
  ],
  "logs_to_query_regex": [
   "BlockManager stopped"
  ],
  "llm_template": "BlockManager stopped",
  "cluster_id": 4,
  "update_success": true,
  "template": "BlockManager stopped"
 },
 {
  "iter": 63,
  "logs_to_query": [
   "Driver commanded a shutdown"
  ],
  "logs_to_query_regex": [
   "Driver commanded a shutdown"
  ],
  "llm_template": "Driver commanded a shutdown",
  "cluster_id": 102,
  "update_success": true,
  "template": "Driver commanded a shutdown"
 },
 {
  "iter": 64,
  "logs_to_query": [
   "stopped o.s.j.s.ServletContextHandler{/metrics/json,null}"
  ],
  "logs_to_query_regex": [
   "stopped o.s.j.s.ServletContextHandler{/metrics/json,null}"
  ],
  "llm_template": "stopped o.s.j.s.<*>",
  "cluster_id": 8,
  "update_success": true,
  "template": "stopped o.s.j.s.ServletContextHandler{<*>}"
 },
 {
  "iter": 65,
  "logs_to_query": [
   "Lost task 7.2 in stage 1.0 (TID 53, mesos-slave-27): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 54.7 GB of 46.2 GB virtual memory used. Consider boosting spark.yarn.executor.memoryOverhead.",
   "Lost task 45.0 in stage 11.3 (TID 1407, mesos-master-2): ExecutorLostFailure (executor 16 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 28.1 GB of 28 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.",
   "Lost task 3.0 in stage 3.0 (TID 54, mesos-master-1): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 22.1 GB of 22 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead."
  ],
  "logs_to_query_regex": [
   "Lost task 7.2 in stage 1.0 (TID 53, mesos-slave-27): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 54.7 GB of 46.2 GB virtual memory used. Consider boosting spark.yarn.executor.memoryOverhead.",
   "Lost task 45.0 in stage 11.3 (TID 1407, mesos-master-2): ExecutorLostFailure (executor 16 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 28.1 GB of 28 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.",
   "Lost task 3.0 in stage 3.0 (TID 54, mesos-master-1): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 22.1 GB of 22 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead."
  ],
  "llm_template": "Lost task <*> in stage <*> (TID <*> <*>): ExecutorLostFailure (executor <*> exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. <*> of <*> memory used. Consider boosting spark.yarn.executor.memoryOverhead.",
  "cluster_id": 235,
  "update_success": true,
  "template": "Lost task <*> in stage <*> (TID <*>, <*>): ExecutorLostFailure (executor <*> exited caused by one of the running tasks) Reason: Container killed by <*> for exceeding memory limits. <*> of <*> <*> memory used. Consider boosting <*>."
 },
 {
  "iter": 66,
  "logs_to_query": [
   "Asked to send map output locations for shuffle 0 to mesos-slave-19:44974",
   "Asked to send map output locations for shuffle 28 to mesos-slave-26:60793",
   "Asked to send map output locations for shuffle 38 to mesos-slave-21:33490"
  ],
  "logs_to_query_regex": [
   "Asked to send map output locations for shuffle 0 to mesos-slave-19:44974",
   "Asked to send map output locations for shuffle 28 to mesos-slave-26:60793",
   "Asked to send map output locations for shuffle 38 to mesos-slave-21:33490"
  ],
  "llm_template": "Asked to send map output locations for shuffle <*> to <*>",
  "cluster_id": 194,
  "update_success": true,
  "template": "Asked to send map output locations for shuffle <*> to <*>"
 },
 {
  "iter": 67,
  "logs_to_query": [
   "Failed to connect to driver at 10.10.34.11:49939, retrying ..."
  ],
  "logs_to_query_regex": [
   "Failed to connect to driver at 10.10.34.11:49939, retrying ..."
  ],
  "llm_template": "Failed to connect to driver at <*> retrying ...",
  "cluster_id": 180,
  "update_success": true,
  "template": "Failed to connect to driver at <*>, retrying ..."
 },
 {
  "iter": 68,
  "logs_to_query": [
   "Failed to get block(s) from mesos-master-1:45950",
   "Failed to get block(s) from mesos-slave-07:58702",
   "Failed to get block(s) from mesos-slave-22:39839"
  ],
  "logs_to_query_regex": [
   "Failed to get block(s) from mesos-master-1:45950",
   "Failed to get block(s) from mesos-slave-07:58702",
   "Failed to get block(s) from mesos-slave-22:39839"
  ],
  "llm_template": "Failed to get block(s) from <*>",
  "cluster_id": 143,
  "update_success": true,
  "template": "Failed to get block(s) from <*>"
 },
 {
  "iter": 69,
  "logs_to_query": [
   "Executor killed task 46.0 in stage 1.0 (TID 48)",
   "Executor killed task 0.1 in stage 2.0 (TID 113)",
   "Executor killed task 22.0 in stage 108.0 (TID 4706)"
  ],
  "logs_to_query_regex": [
   "Executor killed task 46.0 in stage 1.0 (TID 48)",
   "Executor killed task 0.1 in stage 2.0 (TID 113)",
   "Executor killed task 22.0 in stage 108.0 (TID 4706)"
  ],
  "llm_template": "Executor killed task <*> in stage <*> (TID <*>)",
  "cluster_id": 175,
  "update_success": true,
  "template": "Executor killed task <*> in stage <*> (TID <*>)"
 },
 {
  "iter": 70,
  "logs_to_query": [
   "Received 4 containers from YARN, launching executors on 4 of them.",
   "Received 17 containers from YARN, launching executors on 0 of them.",
   "Received 11 containers from YARN, launching executors on 8 of them."
  ],
  "logs_to_query_regex": [
   "Received 4 containers from YARN, launching executors on 4 of them.",
   "Received 17 containers from YARN, launching executors on 0 of them.",
   "Received 11 containers from YARN, launching executors on 8 of them."
  ],
  "llm_template": "Received <*> containers from YARN, launching executors on <*> of them.",
  "cluster_id": 189,
  "update_success": true,
  "template": "Received <*> containers from YARN, launching executors on <*> of them."
 },
 {
  "iter": 71,
  "logs_to_query": [
   "Deleting directory /opt/hdfs/nodemanager/usercache/curi/appcache/application_1485248649253_0020/spark-d820458f-e3bb-471c-af2c-6395418161b0/pyspark-11c108f1-169b-4205-983c-752295e704c0",
   "Deleting directory /opt/hdfs/nodemanager/usercache/curi/appcache/application_1485248649253_0060/spark-c1813f84-7d58-454e-89d3-3c787b644921/pyspark-1082e59f-e55b-449f-a467-79daf1472c46",
   "Deleting directory /opt/hdfs/nodemanager/usercache/curi/appcache/application_1485248649253_0068/spark-5c7c0821-46cd-4f6d-9d69-2d8f5392fd0d"
  ],
  "logs_to_query_regex": [
   "Deleting directory /opt/hdfs/nodemanager/usercache/curi/appcache/application_1485248649253_0020/spark-d820458f-e3bb-471c-af2c-6395418161b0/pyspark-11c108f1-169b-4205-983c-752295e704c0",
   "Deleting directory /opt/hdfs/nodemanager/usercache/curi/appcache/application_1485248649253_0060/spark-c1813f84-7d58-454e-89d3-3c787b644921/pyspark-1082e59f-e55b-449f-a467-79daf1472c46",
   "Deleting directory /opt/hdfs/nodemanager/usercache/curi/appcache/application_1485248649253_0068/spark-5c7c0821-46cd-4f6d-9d69-2d8f5392fd0d"
  ],
  "llm_template": "Deleting directory /opt/hdfs/nodemanager/usercache/curi/appcache/application_<*>/spark-<*>/pyspark-<*>",
  "cluster_id": 93,
  "update_success": true,
  "template": "Deleting directory <*>"
 },
 {
  "iter": 72,
  "logs_to_query": [
   "Registering block manager 10.10.34.23:48030 with 26.4 GB RAM, BlockManagerId(driver, 10.10.34.23, 48030)",
   "Registering block manager 10.10.34.23:50850 with 36.4 GB RAM, BlockManagerId(driver, 10.10.34.23, 50850)",
   "Registering block manager 10.10.34.24:57861 with 26.4 GB RAM, BlockManagerId(driver, 10.10.34.24, 57861)"
  ],
  "logs_to_query_regex": [
   "Registering block manager 10.10.34.23:48030 with 26.4 GB RAM, BlockManagerId(driver, 10.10.34.23, 48030)",
   "Registering block manager 10.10.34.23:50850 with 36.4 GB RAM, BlockManagerId(driver, 10.10.34.23, 50850)",
   "Registering block manager 10.10.34.24:57861 with 26.4 GB RAM, BlockManagerId(driver, 10.10.34.24, 57861)"
  ],
  "llm_template": "Registering block manager <*> with <*> RAM, BlockManagerId(driver, <*> <*>)",
  "cluster_id": 188,
  "update_success": true,
  "template": "Registering block manager <*> with <*> RAM, BlockManagerId(driver, <*>, <*>)"
 },
 {
  "iter": 73,
  "logs_to_query": [
   "Shutting down remote daemon."
  ],
  "logs_to_query_regex": [
   "Shutting down remote daemon."
  ],
  "llm_template": "Shutting down remote daemon.",
  "cluster_id": 103,
  "update_success": true,
  "template": "Shutting down remote daemon."
 },
 {
  "iter": 74,
  "logs_to_query": [
   "Adding task set 0.0 with 2 tasks",
   "Adding task set 9.3 with 74 tasks",
   "Adding task set 35.0 with 48 tasks"
  ],
  "logs_to_query_regex": [
   "Adding task set 0.0 with 2 tasks",
   "Adding task set 9.3 with 74 tasks",
   "Adding task set 35.0 with 48 tasks"
  ],
  "llm_template": "Adding task set <*> with <*> tasks",
  "cluster_id": 154,
  "update_success": true,
  "template": "Adding task set <*> with <*> tasks"
 },
 {
  "iter": 75,
  "logs_to_query": [
   "Deleting directory /opt/hdfs/nodemanager/usercache/curi/appcache/application_1485248649253_0020/spark-d820458f-e3bb-471c-af2c-6395418161b0",
   "Deleting directory /opt/hdfs/nodemanager/usercache/curi/appcache/application_1485248649253_0036/spark-071ab875-81fb-4697-9ac4-7294a998dbb7",
   "Deleting directory /opt/hdfs/nodemanager/usercache/curi/appcache/application_1460011102909_0176/spark-f67381da-e65e-4751-ad13-75ac4d707fcc"
  ],
  "logs_to_query_regex": [
   "Deleting directory /opt/hdfs/nodemanager/usercache/curi/appcache/application_1485248649253_0020/spark-d820458f-e3bb-471c-af2c-6395418161b0",
   "Deleting directory /opt/hdfs/nodemanager/usercache/curi/appcache/application_1485248649253_0036/spark-071ab875-81fb-4697-9ac4-7294a998dbb7",
   "Deleting directory /opt/hdfs/nodemanager/usercache/curi/appcache/application_1460011102909_0176/spark-f67381da-e65e-4751-ad13-75ac4d707fcc"
  ],
  "llm_template": "Deleting directory /opt/hdfs/nodemanager/usercache/curi/appcache/application_<*>/spark-<*>",
  "cluster_id": 93,
  "update_success": true,
  "template": "Deleting directory <*>"
 },
 {
  "iter": 76,
  "logs_to_query": [
   "Registering block manager mesos-master-1:48869 with 14.2 GB RAM, BlockManagerId(6, mesos-master-1, 48869)",
   "Registering block manager mesos-master-1:53100 with 14.2 GB RAM, BlockManagerId(1, mesos-master-1, 53100)",
   "Registering block manager mesos-slave-23:44528 with 14.2 GB RAM, BlockManagerId(6, mesos-slave-23, 44528)"
  ],
  "logs_to_query_regex": [
   "Registering block manager mesos-master-1:48869 with 14.2 GB RAM, BlockManagerId(6, mesos-master-1, 48869)",
   "Registering block manager mesos-master-1:53100 with 14.2 GB RAM, BlockManagerId(1, mesos-master-1, 53100)",
   "Registering block manager mesos-slave-23:44528 with 14.2 GB RAM, BlockManagerId(6, mesos-slave-23, 44528)"
  ],
  "llm_template": "Registering block manager <*> with <*> RAM, BlockManagerId(<*>, <*> <*>)",
  "cluster_id": 188,
  "update_success": true,
  "template": "Registering block manager <*> with <*> RAM, BlockManagerId(<*>, <*>, <*>)"
 },
 {
  "iter": 77,
  "logs_to_query": [
   "Registered executor NettyRpcEndpointRef(null) (mesos-slave-19:57937) with ID 1",
   "Registered executor NettyRpcEndpointRef(null) (mesos-slave-22:51367) with ID 3",
   "Registered executor NettyRpcEndpointRef(null) (mesos-slave-14:36310) with ID 21"
  ],
  "logs_to_query_regex": [
   "Registered executor NettyRpcEndpointRef(null) (mesos-slave-19:57937) with ID 1",
   "Registered executor NettyRpcEndpointRef(null) (mesos-slave-22:51367) with ID 3",
   "Registered executor NettyRpcEndpointRef(null) (mesos-slave-14:36310) with ID 21"
  ],
  "llm_template": "Registered executor NettyRpcEndpointRef(null) (<*>) with ID <*>",
  "cluster_id": 152,
  "update_success": true,
  "template": "Registered executor NettyRpcEndpointRef(<*>) (<*>) with ID <*>"
 },
 {
  "iter": 78,
  "logs_to_query": [
   "Removed TaskSet 0.0, whose tasks have all completed, from pool",
   "Removed TaskSet 221.0, whose tasks have all completed, from pool",
   "Removed TaskSet 201.0, whose tasks have all completed, from pool"
  ],
  "logs_to_query_regex": [
   "Removed TaskSet 0.0, whose tasks have all completed, from pool",
   "Removed TaskSet 221.0, whose tasks have all completed, from pool",
   "Removed TaskSet 201.0, whose tasks have all completed, from pool"
  ],
  "llm_template": "Removed TaskSet <*> whose tasks have all completed, from pool",
  "cluster_id": 183,
  "update_success": true,
  "template": "Removed TaskSet <*>, whose tasks have all completed, from pool"
 },
 {
  "iter": 79,
  "logs_to_query": [
   "Executor is trying to kill task 28.0 in stage 1.0 (TID 30)",
   "Executor is trying to kill task 18.0 in stage 108.0 (TID 4702)",
   "Executor is trying to kill task 45.2 in stage 3.0 (TID 176)"
  ],
  "logs_to_query_regex": [
   "Executor is trying to kill task 28.0 in stage 1.0 (TID 30)",
   "Executor is trying to kill task 18.0 in stage 108.0 (TID 4702)",
   "Executor is trying to kill task 45.2 in stage 3.0 (TID 176)"
  ],
  "llm_template": "Executor is trying to kill task <*> in stage <*> (TID <*>)",
  "cluster_id": 198,
  "update_success": true,
  "template": "Executor is trying to kill task <*> in stage <*> (TID <*>)"
 },
 {
  "iter": 80,
  "logs_to_query": [
   "Will request 6 executor containers, each with 8 cores and 22528 MB memory including 2048 MB overhead",
   "Will request 64 executor containers, each with 5 cores and 28160 MB memory including 2560 MB overhead",
   "Will request 1 executor containers, each with 1 cores and 1408 MB memory including 384 MB overhead"
  ],
  "logs_to_query_regex": [
   "Will request 6 executor containers, each with 8 cores and 22528 MB memory including 2048 MB overhead",
   "Will request 64 executor containers, each with 5 cores and 28160 MB memory including 2560 MB overhead",
   "Will request 1 executor containers, each with 1 cores and 1408 MB memory including 384 MB overhead"
  ],
  "llm_template": "Will request <*> executor containers, each with <*> cores and <*> MB memory including <*> MB overhead",
  "cluster_id": 213,
  "update_success": true,
  "template": "Will request <*> executor containers, each with <*> cores and <*> MB memory including <*> MB overhead"
 },
 {
  "iter": 81,
  "logs_to_query": [
   "Submitting 2 missing tasks from ResultStage 0 (PythonRDD[2] at collect at pnmf4.py:225)",
   "Submitting 40 missing tasks from ResultStage 40 (PythonRDD[82] at collect at pnmf.py:279)",
   "Submitting 3 missing tasks from ResultStage 3 (PythonRDD[9] at collect at pnmf4.py:376)"
  ],
  "logs_to_query_regex": [
   "Submitting 2 missing tasks from ResultStage 0 (PythonRDD[2] at collect at pnmf4.py:225)",
   "Submitting 40 missing tasks from ResultStage 40 (PythonRDD[82] at collect at pnmf.py:279)",
   "Submitting 3 missing tasks from ResultStage 3 (PythonRDD[9] at collect at pnmf4.py:376)"
  ],
  "llm_template": "Submitting <*> missing tasks from ResultStage <*> (<*> at collect at <*>)",
  "cluster_id": 197,
  "update_success": true,
  "template": "Submitting <*> missing tasks from ResultStage <*> (<*> at <*>)"
 },
 {
  "iter": 82,
  "logs_to_query": [
   "Completed container container_1485248649253_0037_01_000002 on host: mesos-slave-16 (state: COMPLETE, exit status: -100)",
   "Completed container container_1485248649253_0060_01_000016 on host: mesos-master-1 (state: COMPLETE, exit status: -104)",
   "Completed container container_1485248649253_0171_01_000052 on host: mesos-slave-10 (state: COMPLETE, exit status: 1)"
  ],
  "logs_to_query_regex": [
   "Completed container container_1485248649253_0037_01_000002 on host: mesos-slave-16 (state: COMPLETE, exit status: -100)",
   "Completed container container_1485248649253_0060_01_000016 on host: mesos-master-1 (state: COMPLETE, exit status: -104)",
   "Completed container container_1485248649253_0171_01_000052 on host: mesos-slave-10 (state: COMPLETE, exit status: 1)"
  ],
  "llm_template": "Completed container <*> on host: <*> (state: <*> exit status: <*>)",
  "cluster_id": 192,
  "update_success": true,
  "template": "Completed container <*> on host: <*> (state: <*>, exit status: <*>)"
 },
 {
  "iter": 83,
  "logs_to_query": [
   "Remote daemon shut down; proceeding with flushing remote transports."
  ],
  "logs_to_query_regex": [
   "Remote daemon shut down; proceeding with flushing remote transports."
  ],
  "llm_template": "Remote daemon shut down; proceeding with flushing remote transports.",
  "cluster_id": 173,
  "update_success": true,
  "template": "Remote daemon shut down; proceeding with flushing remote transports."
 },
 {
  "iter": 84,
  "logs_to_query": [
   "Found inactive connection to mesos-master-1/10.10.34.11:60970, creating a new one.",
   "Found inactive connection to mesos-master-3/10.10.34.13:52453, creating a new one.",
   "Found inactive connection to mesos-master-2/10.10.34.12:49207, creating a new one."
  ],
  "logs_to_query_regex": [
   "Found inactive connection to mesos-master-1/10.10.34.11:60970, creating a new one.",
   "Found inactive connection to mesos-master-3/10.10.34.13:52453, creating a new one.",
   "Found inactive connection to mesos-master-2/10.10.34.12:49207, creating a new one."
  ],
  "llm_template": "Found inactive connection to mesos-master-<*>, creating a new one.",
  "cluster_id": 174,
  "update_success": true,
  "template": "Found inactive connection to <*>, creating a new one."
 },
 {
  "iter": 85,
  "logs_to_query": [
   "Size of output statuses for shuffle 0 is 342 bytes",
   "Size of output statuses for shuffle 11 is 590 bytes",
   "Size of output statuses for shuffle 3 is 1413 bytes"
  ],
  "logs_to_query_regex": [
   "Size of output statuses for shuffle 0 is 342 bytes",
   "Size of output statuses for shuffle 11 is 590 bytes",
   "Size of output statuses for shuffle 3 is 1413 bytes"
  ],
  "llm_template": "Size of output statuses for shuffle <*> is <*> bytes",
  "cluster_id": 185,
  "update_success": true,
  "template": "Size of output statuses for shuffle <*> is <*> bytes"
 },
 {
  "iter": 86,
  "logs_to_query": [
   "Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter"
  ],
  "logs_to_query_regex": [
   "Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter"
  ],
  "llm_template": "Adding filter: <*>",
  "cluster_id": 98,
  "update_success": true,
  "template": "Adding filter: <*>"
 },
 {
  "iter": 87,
  "logs_to_query": [
   "Submitting ResultStage 0 (PythonRDD[2] at collect at pnmf4.py:225), which has no missing parents",
   "Submitting ResultStage 44 (MapPartitionsRDD[90] at saveAsTextFile at NativeMethodAccessorImpl.java:-2), which has no missing parents",
   "Submitting ResultStage 4 (PythonRDD[10] at RDD at PythonRDD.scala:43), which has no missing parents"
  ],
  "logs_to_query_regex": [
   "Submitting ResultStage 0 (PythonRDD[2] at collect at pnmf4.py:225), which has no missing parents",
   "Submitting ResultStage 44 (MapPartitionsRDD[90] at saveAsTextFile at NativeMethodAccessorImpl.java:-2), which has no missing parents",
   "Submitting ResultStage 4 (PythonRDD[10] at RDD at PythonRDD.scala:43), which has no missing parents"
  ],
  "llm_template": "Submitting ResultStage <*> (<*>[<*>] at <*>), which has no missing parents",
  "cluster_id": 202,
  "update_success": true,
  "template": "Submitting ResultStage <*> (<*> at <*>), which has no missing parents"
 },
 {
  "iter": 88,
  "logs_to_query": [
   "Waiting for spark context initialization ..."
  ],
  "logs_to_query_regex": [
   "Waiting for spark context initialization ..."
  ],
  "llm_template": "Waiting for spark context initialization ...",
  "cluster_id": 145,
  "update_success": true,
  "template": "Waiting for spark context initialization ..."
 },
 {
  "iter": 89,
  "logs_to_query": [
   "Running Spark version 1.6.0"
  ],
  "logs_to_query_regex": [
   "Running Spark version 1.6.0"
  ],
  "llm_template": "Running Spark version <*>",
  "cluster_id": 118,
  "update_success": true,
  "template": "Running Spark version <*>"
 },
 {
  "iter": 90,
  "logs_to_query": [
   "Exception while beginning fetch of 1 outstanding blocks",
   "Exception while beginning fetch of 9 outstanding blocks",
   "Exception while beginning fetch of 2 outstanding blocks"
  ],
  "logs_to_query_regex": [
   "Exception while beginning fetch of 1 outstanding blocks",
   "Exception while beginning fetch of 9 outstanding blocks",
   "Exception while beginning fetch of 2 outstanding blocks"
  ],
  "llm_template": "Exception while beginning fetch of <*> outstanding blocks",
  "cluster_id": 166,
  "update_success": true,
  "template": "Exception while beginning fetch of <*> outstanding blocks"
 },
 {
  "iter": 91,
  "logs_to_query": [
   "RECEIVED SIGNAL 15: SIGTERM"
  ],
  "logs_to_query_regex": [
   "RECEIVED SIGNAL 15: SIGTERM"
  ],
  "llm_template": "RECEIVED SIGNAL <*>: SIGTERM",
  "cluster_id": 108,
  "update_success": true,
  "template": "RECEIVED SIGNAL <*>: SIGTERM"
 },
 {
  "iter": 92,
  "logs_to_query": [
   "Container killed by YARN for exceeding memory limits. 54.7 GB of 46.2 GB virtual memory used. Consider boosting spark.yarn.executor.memoryOverhead.",
   "Container killed by YARN for exceeding memory limits. 46.5 GB of 46.2 GB virtual memory used. Consider boosting spark.yarn.executor.memoryOverhead.",
   "Container killed by YARN for exceeding memory limits. 52.0 GB of 46.2 GB virtual memory used. Consider boosting spark.yarn.executor.memoryOverhead."
  ],
  "logs_to_query_regex": [
   "Container killed by YARN for exceeding memory limits. 54.7 GB of 46.2 GB virtual memory used. Consider boosting spark.yarn.executor.memoryOverhead.",
   "Container killed by YARN for exceeding memory limits. 46.5 GB of 46.2 GB virtual memory used. Consider boosting spark.yarn.executor.memoryOverhead.",
   "Container killed by YARN for exceeding memory limits. 52.0 GB of 46.2 GB virtual memory used. Consider boosting spark.yarn.executor.memoryOverhead."
  ],
  "llm_template": "Container killed by YARN for exceeding memory limits. <*> GB of <*> GB virtual memory used. Consider boosting spark.yarn.executor.memoryOverhead.",
  "cluster_id": 218,
  "update_success": true,
  "template": "Container killed by YARN for exceeding memory limits. <*> of <*> virtual memory used. Consider boosting spark.yarn.executor.memoryOverhead."
 },
 {
  "iter": 93,
  "logs_to_query": [
   "Successfully stopped SparkContext"
  ],
  "logs_to_query_regex": [
   "Successfully stopped SparkContext"
  ],
  "llm_template": "Successfully stopped SparkContext",
  "cluster_id": 98,
  "update_success": true,
  "template": "Successfully stopped SparkContext"
 },
 {
  "iter": 94,
  "logs_to_query": [
   "Exception in task 2.0 in stage 1.0 (TID 4)",
   "Exception in task 17.0 in stage 1.0 (TID 19)",
   "Exception in task 25.0 in stage 25.0 (TID 1075)"
  ],
  "logs_to_query_regex": [
   "Exception in task 2.0 in stage 1.0 (TID 4)",
   "Exception in task 17.0 in stage 1.0 (TID 19)",
   "Exception in task 25.0 in stage 25.0 (TID 1075)"
  ],
  "llm_template": "Exception in task <*> in stage <*> (TID <*>)",
  "cluster_id": 175,
  "update_success": true,
  "template": "Exception in task <*> in stage <*> (TID <*>)"
 },
 {
  "iter": 95,
  "logs_to_query": [
   "Executor lost: 6 (epoch 0)",
   "Executor lost: 2 (epoch 0)",
   "Executor lost: 6 (epoch 2)"
  ],
  "logs_to_query_regex": [
   "Executor lost: 6 (epoch 0)",
   "Executor lost: 2 (epoch 0)",
   "Executor lost: 6 (epoch 2)"
  ],
  "llm_template": "Executor lost: <*> (epoch <*>)",
  "cluster_id": 129,
  "update_success": true,
  "template": "Executor lost: <*> (epoch <*>)"
 },
 {
  "iter": 96,
  "logs_to_query": [
   "Removed 6 successfully in removeExecutor",
   "Removed 3 successfully in removeExecutor",
   "Removed 1 successfully in removeExecutor"
  ],
  "logs_to_query_regex": [
   "Removed 6 successfully in removeExecutor",
   "Removed 3 successfully in removeExecutor",
   "Removed 1 successfully in removeExecutor"
  ],
  "llm_template": "Removed <*> successfully in removeExecutor",
  "cluster_id": 130,
  "update_success": true,
  "template": "Removed <*> successfully in removeExecutor"
 },
 {
  "iter": 97,
  "logs_to_query": [
   "Trying to remove executor 6 from BlockManagerMaster.",
   "Trying to remove executor 5 from BlockManagerMaster.",
   "Trying to remove executor 1 from BlockManagerMaster."
  ],
  "logs_to_query_regex": [
   "Trying to remove executor 6 from BlockManagerMaster.",
   "Trying to remove executor 5 from BlockManagerMaster.",
   "Trying to remove executor 1 from BlockManagerMaster."
  ],
  "llm_template": "Trying to remove executor <*> from BlockManagerMaster.",
  "cluster_id": 156,
  "update_success": true,
  "template": "Trying to remove executor <*> from BlockManagerMaster."
 },
 {
  "iter": 98,
  "logs_to_query": [
   "Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:-2",
   "Created broadcast 5 from textFile at NativeMethodAccessorImpl.java:-2"
  ],
  "logs_to_query_regex": [
   "Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:-2",
   "Created broadcast 5 from textFile at NativeMethodAccessorImpl.java:-2"
  ],
  "llm_template": "Created broadcast <*> from textFile at NativeMethodAccessorImpl.java:-<*>",
  "cluster_id": 164,
  "update_success": true,
  "template": "Created broadcast <*> from textFile at <*>"
 },
 {
  "iter": 99,
  "logs_to_query": [
   "Cleaned accumulator 2",
   "Cleaned accumulator 37",
   "Cleaned accumulator 6"
  ],
  "logs_to_query_regex": [
   "Cleaned accumulator 2",
   "Cleaned accumulator 37",
   "Cleaned accumulator 6"
  ],
  "llm_template": "Cleaned accumulator <*>",
  "cluster_id": 19,
  "update_success": true,
  "template": "Cleaned accumulator <*>"
 },
 {
  "iter": 100,
  "logs_to_query": [
   "Waiting for spark context initialization"
  ],
  "logs_to_query_regex": [
   "Waiting for spark context initialization"
  ],
  "llm_template": "Waiting for spark context initialization",
  "cluster_id": 132,
  "update_success": true,
  "template": "Waiting for spark context initialization"
 },
 {
  "iter": 101,
  "logs_to_query": [
   "Stopped Spark web UI at http://10.10.34.23:33134",
   "Stopped Spark web UI at http://10.10.34.36:48724",
   "Stopped Spark web UI at http://10.10.34.17:51244"
  ],
  "logs_to_query_regex": [
   "Stopped Spark web UI at http://10.10.34.23:33134",
   "Stopped Spark web UI at http://10.10.34.36:48724",
   "Stopped Spark web UI at http://10.10.34.17:51244"
  ],
  "llm_template": "Stopped Spark web UI at http://<*>",
  "cluster_id": 145,
  "update_success": true,
  "template": "Stopped Spark web UI at <*>"
 },
 {
  "iter": 102,
  "logs_to_query": [
   "Asked to remove non-existent executor 1",
   "Asked to remove non-existent executor 13",
   "Asked to remove non-existent executor 10"
  ],
  "logs_to_query_regex": [
   "Asked to remove non-existent executor 1",
   "Asked to remove non-existent executor 13",
   "Asked to remove non-existent executor 10"
  ],
  "llm_template": "Asked to remove non-existent executor <*>",
  "cluster_id": 138,
  "update_success": true,
  "template": "Asked to remove non-existent executor <*>"
 },
 {
  "iter": 103,
  "logs_to_query": [
   "Started SparkUI at http://10.10.34.23:33134",
   "Started SparkUI at http://10.10.34.24:47193",
   "Started SparkUI at http://10.10.34.38:32805"
  ],
  "logs_to_query_regex": [
   "Started SparkUI at http://10.10.34.23:33134",
   "Started SparkUI at http://10.10.34.24:47193",
   "Started SparkUI at http://10.10.34.38:32805"
  ],
  "llm_template": "Started SparkUI at http://<*>",
  "cluster_id": 118,
  "update_success": true,
  "template": "Started SparkUI at <*>"
 },
 {
  "iter": 104,
  "logs_to_query": [
   "Remoting shut down."
  ],
  "logs_to_query_regex": [
   "Remoting shut down."
  ],
  "llm_template": "Remoting shut down.",
  "cluster_id": 18,
  "update_success": true,
  "template": "Remoting shut down."
 },
 {
  "iter": 105,
  "logs_to_query": [
   "Missing parents: List()"
  ],
  "logs_to_query_regex": [
   "Missing parents: List()"
  ],
  "llm_template": "Missing parents: List()",
  "cluster_id": 17,
  "update_success": true,
  "template": "Missing parents: List(<*>)"
 },
 {
  "iter": 106,
  "logs_to_query": [
   "Removing block manager BlockManagerId(6, mesos-slave-27, 54380)",
   "Removing block manager BlockManagerId(10, mesos-slave-27, 59217)",
   "Removing block manager BlockManagerId(1, mesos-slave-27, 37801)"
  ],
  "logs_to_query_regex": [
   "Removing block manager BlockManagerId(6, mesos-slave-27, 54380)",
   "Removing block manager BlockManagerId(10, mesos-slave-27, 59217)",
   "Removing block manager BlockManagerId(1, mesos-slave-27, 37801)"
  ],
  "llm_template": "Removing block manager BlockManagerId(<*>, mesos-slave-<*>, <*>)",
  "cluster_id": 140,
  "update_success": true,
  "template": "Removing block manager BlockManagerId(<*>, <*>, <*>)"
 },
 {
  "iter": 107,
  "logs_to_query": [
   "Launching container container_1485248649253_0020_02_000007 for on host mesos-master-1",
   "Launching container container_1485248649253_0131_01_000032 for on host mesos-master-3",
   "Launching container container_1485248649253_0139_01_000010 for on host mesos-master-2"
  ],
  "logs_to_query_regex": [
   "Launching container container_1485248649253_0020_02_000007 for on host mesos-master-1",
   "Launching container container_1485248649253_0131_01_000032 for on host mesos-master-3",
   "Launching container container_1485248649253_0139_01_000010 for on host mesos-master-2"
  ],
  "llm_template": "Launching container <*> for on host mesos-master-<*>",
  "cluster_id": 151,
  "update_success": true,
  "template": "Launching container <*> for on host <*>"
 },
 {
  "iter": 108,
  "logs_to_query": [
   "ShuffleMapStage 1 (reduceByKey at pnmf4.py:332) failed in 76.251 s",
   "ShuffleMapStage 2 (reduceByKey at pnmf4.py:371) failed in 266.276 s",
   "ShuffleMapStage 3 (reduceByKey at pnmf4.py:357) failed in 253.608 s"
  ],
  "logs_to_query_regex": [
   "ShuffleMapStage 1 (reduceByKey at pnmf4.py:332) failed in 76.251 s",
   "ShuffleMapStage 2 (reduceByKey at pnmf4.py:371) failed in 266.276 s",
   "ShuffleMapStage 3 (reduceByKey at pnmf4.py:357) failed in 253.608 s"
  ],
  "llm_template": "ShuffleMapStage <*> (reduceByKey at pnmf4.py:<*>) failed in <*> s",
  "cluster_id": 181,
  "update_success": true,
  "template": "ShuffleMapStage <*> (<*> at <*>) failed in <*> s"
 },
 {
  "iter": 109,
  "logs_to_query": [
   "Cancelling stage 1",
   "Cancelling stage 3",
   "Cancelling stage 108"
  ],
  "logs_to_query_regex": [
   "Cancelling stage 1",
   "Cancelling stage 3",
   "Cancelling stage 108"
  ],
  "llm_template": "Cancelling stage <*>",
  "cluster_id": 98,
  "update_success": true,
  "template": "Cancelling stage <*>"
 },
 {
  "iter": 110,
  "logs_to_query": [
   "Failed while starting block fetches"
  ],
  "logs_to_query_regex": [
   "Failed while starting block fetches"
  ],
  "llm_template": "Failed while starting block fetches",
  "cluster_id": 127,
  "update_success": true,
  "template": "Failed while starting block fetches"
 },
 {
  "iter": 111,
  "logs_to_query": [
   "Removing RDD 52 from persistence list",
   "Removing RDD 247 from persistence list",
   "Removing RDD 167 from persistence list"
  ],
  "logs_to_query_regex": [
   "Removing RDD 52 from persistence list",
   "Removing RDD 247 from persistence list",
   "Removing RDD 167 from persistence list"
  ],
  "llm_template": "Removing RDD <*> from persistence list",
  "cluster_id": 142,
  "update_success": true,
  "template": "Removing RDD <*>"
 },
 {
  "iter": 112,
  "logs_to_query": [
   "Total input paths to process : 1"
  ],
  "logs_to_query_regex": [
   "Total input paths to process : 1"
  ],
  "llm_template": "Total input paths to process : <*>",
  "cluster_id": 164,
  "update_success": true,
  "template": "Total input paths to process : <*>"
 },
 {
  "iter": 113,
  "logs_to_query": [
   "ShuffleMapStage 11 (reduceByKey at pnmf_dblp.py:430) failed in 24.026 s",
   "ShuffleMapStage 11 (reduceByKey at pnmf_dblp.py:430) failed in 137.358 s",
   "ShuffleMapStage 11 (reduceByKey at pnmf_amz.py:430) failed in 174.941 s"
  ],
  "logs_to_query_regex": [
   "ShuffleMapStage 11 (reduceByKey at pnmf_dblp.py:430) failed in 24.026 s",
   "ShuffleMapStage 11 (reduceByKey at pnmf_dblp.py:430) failed in 137.358 s",
   "ShuffleMapStage 11 (reduceByKey at pnmf_amz.py:430) failed in 174.941 s"
  ],
  "llm_template": "ShuffleMapStage <*> (reduceByKey at <*>) failed in <*> s",
  "cluster_id": 181,
  "update_success": true,
  "template": "ShuffleMapStage <*> (<*> at <*>) failed in <*> s"
 },
 {
  "iter": 114,
  "logs_to_query": [
   "Stage 1 contains a task of very large size (1181 KB). The maximum recommended task size is 100 KB.",
   "Stage 90 contains a task of very large size (1807 KB). The maximum recommended task size is 100 KB.",
   "Stage 11 contains a task of very large size (80425 KB). The maximum recommended task size is 100 KB."
  ],
  "logs_to_query_regex": [
   "Stage 1 contains a task of very large size (1181 KB). The maximum recommended task size is 100 KB.",
   "Stage 90 contains a task of very large size (1807 KB). The maximum recommended task size is 100 KB.",
   "Stage 11 contains a task of very large size (80425 KB). The maximum recommended task size is 100 KB."
  ],
  "llm_template": "Stage <*> contains a task of very large size (<*> KB). The maximum recommended task size is <*> KB.",
  "cluster_id": 217,
  "update_success": true,
  "template": "Stage <*> contains a task of very large size (<*> KB). The maximum recommended task size is <*> KB."
 },
 {
  "iter": 115,
  "logs_to_query": [
   "Invoking stop() from shutdown hook"
  ],
  "logs_to_query_regex": [
   "Invoking stop() from shutdown hook"
  ],
  "llm_template": "Invoking stop() from shutdown hook",
  "cluster_id": 132,
  "update_success": true,
  "template": "Invoking stop() from shutdown hook"
 },
 {
  "iter": 116,
  "logs_to_query": [
   "Asking each executor to shut down"
  ],
  "logs_to_query_regex": [
   "Asking each executor to shut down"
  ],
  "llm_template": "Asking each executor to shut down",
  "cluster_id": 145,
  "update_success": true,
  "template": "Asking each executor to shut down"
 },
 {
  "iter": 117,
  "logs_to_query": [
   "org.apache.spark.SparkException: Exception while starting container container_1485248649253_0037_01_000002 on host mesos-slave-16",
   "org.apache.spark.SparkException: Exception while starting container container_1485248649253_0143_01_000011 on host mesos-slave-16",
   "org.apache.spark.SparkException: Exception while starting container container_1485248649253_0101_01_000013 on host mesos-slave-09"
  ],
  "logs_to_query_regex": [
   "org.apache.spark.SparkException: Exception while starting container container_1485248649253_0037_01_000002 on host mesos-slave-16",
   "org.apache.spark.SparkException: Exception while starting container container_1485248649253_0143_01_000011 on host mesos-slave-16",
   "org.apache.spark.SparkException: Exception while starting container container_1485248649253_0101_01_000013 on host mesos-slave-09"
  ],
  "llm_template": "org.apache.spark.SparkException: Exception while starting container <*> on host mesos-slave-<*>",
  "cluster_id": 177,
  "update_success": true,
  "template": "org.apache.spark.SparkException: Exception while starting container <*> on host <*>"
 },
 {
  "iter": 118,
  "logs_to_query": [
   "Failed to fetch remote block broadcast_6_piece283 from BlockManagerId(4, mesos-slave-20, 33900) (failed attempt 1)",
   "Failed to fetch remote block broadcast_4_piece215 from BlockManagerId(16, mesos-slave-19, 50105) (failed attempt 1)",
   "Failed to fetch remote block broadcast_6_piece89 from BlockManagerId(2, mesos-slave-27, 36033) (failed attempt 1)"
  ],
  "logs_to_query_regex": [
   "Failed to fetch remote block broadcast_6_piece283 from BlockManagerId(4, mesos-slave-20, 33900) (failed attempt 1)",
   "Failed to fetch remote block broadcast_4_piece215 from BlockManagerId(16, mesos-slave-19, 50105) (failed attempt 1)",
   "Failed to fetch remote block broadcast_6_piece89 from BlockManagerId(2, mesos-slave-27, 36033) (failed attempt 1)"
  ],
  "llm_template": "Failed to fetch remote block <*> from BlockManagerId(<*>, <*> <*>) (failed attempt <*>)",
  "cluster_id": 204,
  "update_success": true,
  "template": "Failed to fetch remote block <*> from BlockManagerId(<*>, <*>, <*>) (failed attempt <*>)"
 },
 {
  "iter": 119,
  "logs_to_query": [
   "Job 1 failed: count at pnmf4.py:333, took 76.277073 s",
   "Job 2 failed: count at pnmf4.py:353, took 204.671906 s",
   "Job 3 failed: count at pnmf4.py:358, took 253.643796 s"
  ],
  "logs_to_query_regex": [
   "Job 1 failed: count at pnmf4.py:333, took 76.277073 s",
   "Job 2 failed: count at pnmf4.py:353, took 204.671906 s",
   "Job 3 failed: count at pnmf4.py:358, took 253.643796 s"
  ],
  "llm_template": "Job <*> failed: count at pnmf4.py:<*>, took <*> s",
  "cluster_id": 181,
  "update_success": true,
  "template": "Job <*> failed: count at <*>, took <*> s"
 },
 {
  "iter": 120,
  "logs_to_query": [
   "ApplicationMaster registered as NettyRpcEndpointRef(spark://YarnAM@10.10.34.23:47891)",
   "ApplicationMaster registered as NettyRpcEndpointRef(spark://YarnAM@10.10.34.38:36751)",
   "ApplicationMaster registered as NettyRpcEndpointRef(spark://YarnAM@10.10.34.30:45563)"
  ],
  "logs_to_query_regex": [
   "ApplicationMaster registered as NettyRpcEndpointRef(spark://YarnAM@10.10.34.23:47891)",
   "ApplicationMaster registered as NettyRpcEndpointRef(spark://YarnAM@10.10.34.38:36751)",
   "ApplicationMaster registered as NettyRpcEndpointRef(spark://YarnAM@10.10.34.30:45563)"
  ],
  "llm_template": "ApplicationMaster registered as NettyRpcEndpointRef(spark://YarnAM@<*>)",
  "cluster_id": 118,
  "update_success": true,
  "template": "ApplicationMaster registered as NettyRpcEndpointRef(spark://<*>)"
 },
 {
  "iter": 121,
  "logs_to_query": [
   "Message RemoteProcessDisconnected(mesos-slave-11:45333) dropped.",
   "Message RemoteProcessDisconnected(mesos-slave-21:45292) dropped.",
   "Message RemoteProcessDisconnected(mesos-slave-10:56061) dropped."
  ],
  "logs_to_query_regex": [
   "Message RemoteProcessDisconnected(mesos-slave-11:45333) dropped.",
   "Message RemoteProcessDisconnected(mesos-slave-21:45292) dropped.",
   "Message RemoteProcessDisconnected(mesos-slave-10:56061) dropped."
  ],
  "llm_template": "Message RemoteProcessDisconnected(<*>) dropped.",
  "cluster_id": 98,
  "update_success": true,
  "template": "Message RemoteProcessDisconnected(<*>) dropped."
 },
 {
  "iter": 122,
  "logs_to_query": [
   "Job 4 failed: count at pnmf_dblp.py:432, took 469.055113 s",
   "Job 4 failed: count at pnmf_dblp.py:432, took 690.315495 s",
   "Job 3 failed: count at pnmf_dblp.py:425, took 912.227876 s"
  ],
  "logs_to_query_regex": [
   "Job 4 failed: count at pnmf_dblp.py:432, took 469.055113 s",
   "Job 4 failed: count at pnmf_dblp.py:432, took 690.315495 s",
   "Job 3 failed: count at pnmf_dblp.py:425, took 912.227876 s"
  ],
  "llm_template": "Job <*> failed: count at <*> s",
  "cluster_id": 181,
  "update_success": true,
  "template": "Job <*> failed: count at <*>, took <*> s"
 },
 {
  "iter": 123,
  "logs_to_query": [
   "Registering MapOutputTracker"
  ],
  "logs_to_query_regex": [
   "Registering MapOutputTracker"
  ],
  "llm_template": "Registering MapOutputTracker",
  "cluster_id": 11,
  "update_success": true,
  "template": "Registering MapOutputTracker"
 },
 {
  "iter": 124,
  "logs_to_query": [
   "Add WebUI Filter. AddWebUIFilter(org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter,Map(PROXY_HOSTS -> mesos-master-1, PROXY_URI_BASES -> http://mesos-master-1:8088/proxy/application_1485248649253_0166),/proxy/application_1485248649253_0166)",
   "Add WebUI Filter. AddWebUIFilter(org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter,Map(PROXY_HOSTS -> mesos-master-1, PROXY_URI_BASES -> http://mesos-master-1:8088/proxy/application_1485248649253_0091),/proxy/application_1485248649253_0091)",
   "Add WebUI Filter. AddWebUIFilter(org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter,Map(PROXY_HOSTS -> mesos-master-1, PROXY_URI_BASES -> http://mesos-master-1:8088/proxy/application_1485248649253_0096),/proxy/application_1485248649253_0096)"
  ],
  "logs_to_query_regex": [
   "Add WebUI Filter. AddWebUIFilter(org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter,Map(PROXY_HOSTS -> mesos-master-1, PROXY_URI_BASES -> http://mesos-master-1:8088/proxy/application_1485248649253_0166),/proxy/application_1485248649253_0166)",
   "Add WebUI Filter. AddWebUIFilter(org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter,Map(PROXY_HOSTS -> mesos-master-1, PROXY_URI_BASES -> http://mesos-master-1:8088/proxy/application_1485248649253_0091),/proxy/application_1485248649253_0091)",
   "Add WebUI Filter. AddWebUIFilter(org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter,Map(PROXY_HOSTS -> mesos-master-1, PROXY_URI_BASES -> http://mesos-master-1:8088/proxy/application_1485248649253_0096),/proxy/application_1485248649253_0096)"
  ],
  "llm_template": "Add WebUI Filter. AddWebUIFilter(org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter,Map(PROXY_HOSTS -> mesos-master-<*>, PROXY_URI_BASES -> http://mesos-master-<*>:8088/proxy/application_<*>),/proxy/application_<*>)",
  "cluster_id": 181,
  "update_success": true,
  "template": "Add WebUI Filter. AddWebUIFilter(<*>,Map(PROXY_HOSTS -> <*>, PROXY_URI_BASES -> <*>),<*>)"
 },
 {
  "iter": 125,
  "logs_to_query": [
   "Error sending message [message = Heartbeat(14,[Lscala.Tuple2;@12236bdc,BlockManagerId(14, mesos-master-1, 42483))] in 1 attempts",
   "Error sending message [message = Heartbeat(29,[Lscala.Tuple2;@48b53def,BlockManagerId(29, mesos-slave-19, 59688))] in 1 attempts",
   "Error sending message [message = Heartbeat(65,[Lscala.Tuple2;@17b2c9aa,BlockManagerId(65, mesos-slave-22, 34767))] in 1 attempts"
  ],
  "logs_to_query_regex": [
   "Error sending message [message = Heartbeat(14,[Lscala.Tuple2;@12236bdc,BlockManagerId(14, mesos-master-1, 42483))] in 1 attempts",
   "Error sending message [message = Heartbeat(29,[Lscala.Tuple2;@48b53def,BlockManagerId(29, mesos-slave-19, 59688))] in 1 attempts",
   "Error sending message [message = Heartbeat(65,[Lscala.Tuple2;@17b2c9aa,BlockManagerId(65, mesos-slave-22, 34767))] in 1 attempts"
  ],
  "llm_template": "Error sending message [message = Heartbeat(<*>,[Lscala.Tuple2;@12236bdc,BlockManagerId(<*>, mesos-master-<*>, <*>))] in <*> attempts",
  "cluster_id": 193,
  "update_success": true,
  "template": "Error sending message [message = <*>] in <*> attempts"
 },
 {
  "iter": 126,
  "logs_to_query": [
   "Submitting 48 missing tasks from ResultStage 1 (PythonRDD[4] at count at pnmf4.py:353)",
   "Submitting 4 missing tasks from ResultStage 1 (PythonRDD[4] at count at pnmf4.py:358)",
   "Submitting 40 missing tasks from ResultStage 1 (PythonRDD[4] at count at pnmf4.py:356)"
  ],
  "logs_to_query_regex": [
   "Submitting 48 missing tasks from ResultStage 1 (PythonRDD[4] at count at pnmf4.py:353)",
   "Submitting 4 missing tasks from ResultStage 1 (PythonRDD[4] at count at pnmf4.py:358)",
   "Submitting 40 missing tasks from ResultStage 1 (PythonRDD[4] at count at pnmf4.py:356)"
  ],
  "llm_template": "Submitting <*> missing tasks from ResultStage <*> (<*>[<*>] at count at pnmf4.py:<*>)",
  "cluster_id": 197,
  "update_success": true,
  "template": "Submitting <*> missing tasks from ResultStage <*> (<*> at <*>)"
 },
 {
  "iter": 127,
  "logs_to_query": [
   "Error sending message [message = Heartbeat(7,[Lscala.Tuple2;@32a6ba4e,BlockManagerId(7, mesos-slave-14, 41961))] in 1 attempts",
   "Error sending message [message = Heartbeat(1,[Lscala.Tuple2;@37ce3e31,BlockManagerId(1, mesos-slave-27, 53608))] in 1 attempts",
   "Error sending message [message = Heartbeat(62,[Lscala.Tuple2;@130eff16,BlockManagerId(62, mesos-slave-22, 46382))] in 1 attempts"
  ],
  "logs_to_query_regex": [
   "Error sending message [message = Heartbeat(7,[Lscala.Tuple2;@32a6ba4e,BlockManagerId(7, mesos-slave-14, 41961))] in 1 attempts",
   "Error sending message [message = Heartbeat(1,[Lscala.Tuple2;@37ce3e31,BlockManagerId(1, mesos-slave-27, 53608))] in 1 attempts",
   "Error sending message [message = Heartbeat(62,[Lscala.Tuple2;@130eff16,BlockManagerId(62, mesos-slave-22, 46382))] in 1 attempts"
  ],
  "llm_template": "Error sending message [message = Heartbeat(<*>,[Lscala.Tuple2;@32a6ba4e,BlockManagerId(<*>, mesos-slave-<*>, <*>))] in <*> attempts",
  "cluster_id": 193,
  "update_success": true,
  "template": "Error sending message [message = <*>] in <*> attempts"
 },
 {
  "iter": 128,
  "logs_to_query": [
   "Still have 1 requests outstanding when connection from mesos-slave-20/10.10.34.30:33900 is closed",
   "Still have 4 requests outstanding when connection from mesos-slave-20/10.10.34.30:45641 is closed",
   "Still have 2 requests outstanding when connection from mesos-slave-18/10.10.34.28:37604 is closed"
  ],
  "logs_to_query_regex": [
   "Still have 1 requests outstanding when connection from mesos-slave-20/10.10.34.30:33900 is closed",
   "Still have 4 requests outstanding when connection from mesos-slave-20/10.10.34.30:45641 is closed",
   "Still have 2 requests outstanding when connection from mesos-slave-18/10.10.34.28:37604 is closed"
  ],
  "llm_template": "Still have <*> requests outstanding when connection from mesos-slave-20/<*> is closed",
  "cluster_id": 190,
  "update_success": true,
  "template": "Still have <*> requests outstanding when connection from <*> is closed"
 },
 {
  "iter": 129,
  "logs_to_query": [
   "Waiting for Spark driver to be reachable."
  ],
  "logs_to_query_regex": [
   "Waiting for Spark driver to be reachable."
  ],
  "llm_template": "Waiting for Spark driver to be reachable.",
  "cluster_id": 164,
  "update_success": true,
  "template": "Waiting for Spark driver to be reachable."
 },
 {
  "iter": 130,
  "logs_to_query": [
   "Starting job: collect at pnmf4.py:225",
   "Starting job: collect at IPLoM.py:124",
   "Starting job: collect at pnmf.py:279"
  ],
  "logs_to_query_regex": [
   "Starting job: collect at pnmf4.py:225",
   "Starting job: collect at IPLoM.py:124",
   "Starting job: collect at pnmf.py:279"
  ],
  "llm_template": "Starting job: collect at <*>",
  "cluster_id": 125,
  "update_success": true,
  "template": "Starting job: collect at <*>"
 },
 {
  "iter": 131,
  "logs_to_query": [
   "Got job 0 (collect at pnmf4.py:225) with 2 output partitions",
   "Got job 71 (collect at pnmf.py:279) with 40 output partitions",
   "Got job 3 (collect at pnmf4.py:377) with 3 output partitions"
  ],
  "logs_to_query_regex": [
   "Got job 0 (collect at pnmf4.py:225) with 2 output partitions",
   "Got job 71 (collect at pnmf.py:279) with 40 output partitions",
   "Got job 3 (collect at pnmf4.py:377) with 3 output partitions"
  ],
  "llm_template": "Got job <*> (collect at <*>) with <*> output partitions",
  "cluster_id": 182,
  "update_success": true,
  "template": "Got job <*> (collect at <*>) with <*> output partitions"
 },
 {
  "iter": 132,
  "logs_to_query": [
   "Submitting 1 missing tasks from ResultStage 2 (PythonRDD[5] at RDD at PythonRDD.scala:43)",
   "Submitting 1 missing tasks from ResultStage 4 (PythonRDD[10] at RDD at PythonRDD.scala:43)"
  ],
  "logs_to_query_regex": [
   "Submitting 1 missing tasks from ResultStage 2 (PythonRDD[5] at RDD at PythonRDD.scala:43)",
   "Submitting 1 missing tasks from ResultStage 4 (PythonRDD[10] at RDD at PythonRDD.scala:43)"
  ],
  "llm_template": "Submitting <*> missing tasks from ResultStage <*> (PythonRDD[<*>] at RDD at PythonRDD.scala:<*>)",
  "cluster_id": 197,
  "update_success": true,
  "template": "Submitting <*> missing tasks from ResultStage <*> (<*> at <*>)"
 },
 {
  "iter": 133,
  "logs_to_query": [
   "Starting job: runJob at PythonRDD.scala:393"
  ],
  "logs_to_query_regex": [
   "Starting job: runJob at PythonRDD.scala:393"
  ],
  "llm_template": "Starting job: runJob at <*>",
  "cluster_id": 132,
  "update_success": true,
  "template": "Starting job: runJob at <*>"
 },
 {
  "iter": 134,
  "logs_to_query": [
   "Still have 1 requests outstanding when connection from mesos-master-1/10.10.34.11:60970 is closed",
   "Still have 0 requests outstanding when connection from mesos-master-1/10.10.34.11:33750 is closed",
   "Still have 2 requests outstanding when connection from mesos-slave-18/10.10.34.28:37604 is closed"
  ],
  "logs_to_query_regex": [
   "Still have 1 requests outstanding when connection from mesos-master-1/10.10.34.11:60970 is closed",
   "Still have 0 requests outstanding when connection from mesos-master-1/10.10.34.11:33750 is closed",
   "Still have 2 requests outstanding when connection from mesos-slave-18/10.10.34.28:37604 is closed"
  ],
  "llm_template": "Still have <*> requests outstanding when connection from <*> is closed",
  "cluster_id": 190,
  "update_success": true,
  "template": "Still have <*> requests outstanding when connection from <*> is closed"
 },
 {
  "iter": 135,
  "logs_to_query": [
   "Registering RDD 5 (reduceByKey at pnmf4.py:332)",
   "Registering RDD 7 (reduceByKey at pnmf_amz.py:389)",
   "Registering RDD 107 (reduceByKey at pnmf4.py:295)"
  ],
  "logs_to_query_regex": [
   "Registering RDD 5 (reduceByKey at pnmf4.py:332)",
   "Registering RDD 7 (reduceByKey at pnmf_amz.py:389)",
   "Registering RDD 107 (reduceByKey at pnmf4.py:295)"
  ],
  "llm_template": "Registering RDD <*> (reduceByKey at <*>)",
  "cluster_id": 145,
  "update_success": true,
  "template": "Registering RDD <*> (reduceByKey at <*>)"
 },
 {
  "iter": 136,
  "logs_to_query": [
   "Disabling executor 6.",
   "Disabling executor 4.",
   "Disabling executor 8."
  ],
  "logs_to_query_regex": [
   "Disabling executor 6.",
   "Disabling executor 4.",
   "Disabling executor 8."
  ],
  "llm_template": "Disabling executor <*>.",
  "cluster_id": 20,
  "update_success": true,
  "template": "Disabling executor <*>."
 },
 {
  "iter": 137,
  "logs_to_query": [
   "Final stage: ResultStage 0 (collect at pnmf4.py:225)",
   "Final stage: ResultStage 54 (collect at pnmf.py:279)",
   "Final stage: ResultStage 2 (collect at pnmf4.py:333)"
  ],
  "logs_to_query_regex": [
   "Final stage: ResultStage 0 (collect at pnmf4.py:225)",
   "Final stage: ResultStage 54 (collect at pnmf.py:279)",
   "Final stage: ResultStage 2 (collect at pnmf4.py:333)"
  ],
  "llm_template": "Final stage: ResultStage <*> (collect at <*>)",
  "cluster_id": 153,
  "update_success": true,
  "template": "Final stage: ResultStage <*> (collect at <*>)"
 },
 {
  "iter": 138,
  "logs_to_query": [
   "Unregistering ApplicationMaster with SUCCEEDED"
  ],
  "logs_to_query_regex": [
   "Unregistering ApplicationMaster with SUCCEEDED"
  ],
  "llm_template": "Unregistering ApplicationMaster with <*>",
  "cluster_id": 118,
  "update_success": true,
  "template": "Unregistering ApplicationMaster with <*>"
 },
 {
  "iter": 139,
  "logs_to_query": [
   "Submitting 13 missing tasks from ResultStage 3 (PythonRDD[11] at min at IPLoM.py:143)",
   "Submitting 3 missing tasks from ResultStage 3 (PythonRDD[9] at countByKey at pnmf4.py:372)",
   "Submitting 13 missing tasks from ResultStage 13 (PythonRDD[26] at reduce at IPLoM.py:168)"
  ],
  "logs_to_query_regex": [
   "Submitting 13 missing tasks from ResultStage 3 (PythonRDD[11] at min at IPLoM.py:143)",
   "Submitting 3 missing tasks from ResultStage 3 (PythonRDD[9] at countByKey at pnmf4.py:372)",
   "Submitting 13 missing tasks from ResultStage 13 (PythonRDD[26] at reduce at IPLoM.py:168)"
  ],
  "llm_template": "Submitting <*> missing tasks from ResultStage <*> (<*>[<*>] at <*> at <*>)",
  "cluster_id": 197,
  "update_success": true,
  "template": "Submitting <*> missing tasks from ResultStage <*> (<*> at <*>)"
 },
 {
  "iter": 140,
  "logs_to_query": [
   "running: Set()"
  ],
  "logs_to_query_regex": [
   "running: Set()"
  ],
  "llm_template": "running: Set()",
  "cluster_id": 5,
  "update_success": true,
  "template": "running: Set()"
 },
 {
  "iter": 141,
  "logs_to_query": [
   "failed: Set()"
  ],
  "logs_to_query_regex": [
   "failed: Set()"
  ],
  "llm_template": "failed: Set()",
  "cluster_id": 6,
  "update_success": true,
  "template": "failed: Set()"
 },
 {
  "iter": 142,
  "logs_to_query": [
   "looking for newly runnable stages"
  ],
  "logs_to_query_regex": [
   "looking for newly runnable stages"
  ],
  "llm_template": "looking for newly runnable stages",
  "cluster_id": 131,
  "update_success": true,
  "template": "looking for newly runnable stages"
 },
 {
  "iter": 143,
  "logs_to_query": [
   "ResultStage 0 (collect at pnmf4.py:225) finished in 6.049 s",
   "ResultStage 0 (collect at pnmf4.py:292) finished in 5.747 s",
   "ResultStage 79 (collect at pnmf.py:279) finished in 1.914 s"
  ],
  "logs_to_query_regex": [
   "ResultStage 0 (collect at pnmf4.py:225) finished in 6.049 s",
   "ResultStage 0 (collect at pnmf4.py:292) finished in 5.747 s",
   "ResultStage 79 (collect at pnmf.py:279) finished in 1.914 s"
  ],
  "llm_template": "ResultStage <*> (collect at <*>) finished in <*> s",
  "cluster_id": 171,
  "update_success": true,
  "template": "ResultStage <*> (collect at <*>) finished in <*> s"
 },
 {
  "iter": 144,
  "logs_to_query": [
   "Job 0 finished: collect at pnmf4.py:225, took 6.458519 s",
   "Job 0 finished: collect at pnmf4.py:218, took 8.573524 s",
   "Job 10 finished: collect at pnmf.py:279, took 1.604490 s"
  ],
  "logs_to_query_regex": [
   "Job 0 finished: collect at pnmf4.py:225, took 6.458519 s",
   "Job 0 finished: collect at pnmf4.py:218, took 8.573524 s",
   "Job 10 finished: collect at pnmf.py:279, took 1.604490 s"
  ],
  "llm_template": "Job <*> finished: collect at <*> took <*> s",
  "cluster_id": 172,
  "update_success": true,
  "template": "Job <*> finished: collect at <*>, took <*> s"
 },
 {
  "iter": 145,
  "logs_to_query": [
   "Using REPL class URI: http://10.10.18.32:65475"
  ],
  "logs_to_query_regex": [
   "Using REPL class URI: http://10.10.18.32:65475"
  ],
  "llm_template": "Using REPL class URI: http://<*>",
  "cluster_id": 132,
  "update_success": true,
  "template": "Using REPL class URI: <*>"
 },
 {
  "iter": 146,
  "logs_to_query": [
   "Cleaned shuffle 8",
   "Cleaned shuffle 34",
   "Cleaned shuffle 11"
  ],
  "logs_to_query_regex": [
   "Cleaned shuffle 8",
   "Cleaned shuffle 34",
   "Cleaned shuffle 11"
  ],
  "llm_template": "Cleaned shuffle <*>",
  "cluster_id": 98,
  "update_success": true,
  "template": "Cleaned shuffle <*>"
 },
 {
  "iter": 147,
  "logs_to_query": [
   "error=2, No such file or directory"
  ],
  "logs_to_query_regex": [
   "error=2, No such file or directory"
  ],
  "llm_template": "error=<*>, No such file or directory",
  "cluster_id": 144,
  "update_success": true,
  "template": "error=<*>, No such file or directory"
 },
 {
  "iter": 148,
  "logs_to_query": [
   "Error while invoking RpcHandler#receive() for one-way message."
  ],
  "logs_to_query_regex": [
   "Error while invoking RpcHandler#receive() for one-way message."
  ],
  "llm_template": "Error while invoking RpcHandler#receive() for one-way message.",
  "cluster_id": 157,
  "update_success": true,
  "template": "Error while invoking RpcHandler#receive() for one-way message."
 },
 {
  "iter": 149,
  "logs_to_query": [
   "Started SelectChannelConnector@0.0.0.0:33134"
  ],
  "logs_to_query_regex": [
   "Started SelectChannelConnector@0.0.0.0:33134"
  ],
  "llm_template": "Started SelectChannelConnector@<*>",
  "cluster_id": 11,
  "update_success": true,
  "template": "Started SelectChannelConnector@<*>"
 },
 {
  "iter": 150,
  "logs_to_query": [
   "Exception in connection from mesos-slave-20/10.10.34.30:33900",
   "Exception in connection from mesos-slave-10/10.10.34.20:45733",
   "Exception in connection from mesos-slave-25/10.10.34.35:51600"
  ],
  "logs_to_query_regex": [
   "Exception in connection from mesos-slave-20/10.10.34.30:33900",
   "Exception in connection from mesos-slave-10/10.10.34.20:45733",
   "Exception in connection from mesos-slave-25/10.10.34.35:51600"
  ],
  "llm_template": "Exception in connection from mesos-slave-<*>",
  "cluster_id": 126,
  "update_success": true,
  "template": "Exception in connection from <*>"
 },
 {
  "iter": 151,
  "logs_to_query": [
   "Lost executor 6 on mesos-slave-27: Container killed by YARN for exceeding memory limits. 54.7 GB of 46.2 GB virtual memory used. Consider boosting spark.yarn.executor.memoryOverhead.",
   "Lost executor 2 on mesos-slave-23: Container killed by YARN for exceeding memory limits. 57.4 GB of 46.2 GB virtual memory used. Consider boosting spark.yarn.executor.memoryOverhead.",
   "Lost executor 15 on mesos-slave-11: Container killed by YARN for exceeding memory limits. 46.4 GB of 46.2 GB virtual memory used. Consider boosting spark.yarn.executor.memoryOverhead."
  ],
  "logs_to_query_regex": [
   "Lost executor 6 on mesos-slave-27: Container killed by YARN for exceeding memory limits. 54.7 GB of 46.2 GB virtual memory used. Consider boosting spark.yarn.executor.memoryOverhead.",
   "Lost executor 2 on mesos-slave-23: Container killed by YARN for exceeding memory limits. 57.4 GB of 46.2 GB virtual memory used. Consider boosting spark.yarn.executor.memoryOverhead.",
   "Lost executor 15 on mesos-slave-11: Container killed by YARN for exceeding memory limits. 46.4 GB of 46.2 GB virtual memory used. Consider boosting spark.yarn.executor.memoryOverhead."
  ],
  "llm_template": "Lost executor <*> on <*>: Container killed by YARN for exceeding memory limits. <*> GB of <*> GB virtual memory used. Consider boosting spark.yarn.executor.memoryOverhead.",
  "cluster_id": 224,
  "update_success": true,
  "template": "Lost executor <*> on <*>: Container killed by YARN for exceeding memory limits. <*> of <*> virtual memory used. Consider boosting spark.yarn.executor.memoryOverhead."
 },
 {
  "iter": 152,
  "logs_to_query": [
   "ensureFreeSpace(1097) called with curMem=0, maxMem=556038881"
  ],
  "logs_to_query_regex": [
   "ensureFreeSpace(1097) called with curMem=0, maxMem=556038881"
  ],
  "llm_template": "ensureFreeSpace(<*>) called with curMem=<*> maxMem=<*>",
  "cluster_id": 132,
  "update_success": true,
  "template": "ensureFreeSpace(<*>) called with curMem=<*>, maxMem=<*>"
 },
 {
  "iter": 153,
  "logs_to_query": [
   "ShuffleMapStage 2 (aggregateByKey at IPLoM.py:140) finished in 32.076 s",
   "ShuffleMapStage 46 (aggregateByKey at IPLoM.py:140) finished in 30.693 s",
   "ShuffleMapStage 241 (aggregateByKey at IPLoM.py:518) finished in 29.213 s"
  ],
  "logs_to_query_regex": [
   "ShuffleMapStage 2 (aggregateByKey at IPLoM.py:140) finished in 32.076 s",
   "ShuffleMapStage 46 (aggregateByKey at IPLoM.py:140) finished in 30.693 s",
   "ShuffleMapStage 241 (aggregateByKey at IPLoM.py:518) finished in 29.213 s"
  ],
  "llm_template": "ShuffleMapStage <*> (aggregateByKey at IPLoM.py:<*>) finished in <*> s",
  "cluster_id": 181,
  "update_success": true,
  "template": "ShuffleMapStage <*> (<*> at <*>) finished in <*> s"
 },
 {
  "iter": 154,
  "logs_to_query": [
   "Parents of final stage: List(ShuffleMapStage 1)",
   "Parents of final stage: List(ShuffleMapStage 255)",
   "Parents of final stage: List(ShuffleMapStage 29)"
  ],
  "logs_to_query_regex": [
   "Parents of final stage: List(ShuffleMapStage 1)",
   "Parents of final stage: List(ShuffleMapStage 255)",
   "Parents of final stage: List(ShuffleMapStage 29)"
  ],
  "llm_template": "Parents of final stage: List(ShuffleMapStage <*>)",
  "cluster_id": 139,
  "update_success": true,
  "template": "Parents of final stage: List(ShuffleMapStage <*>)"
 },
 {
  "iter": 155,
  "logs_to_query": [
   "Incomplete task interrupted: Attempting to kill Python Worker"
  ],
  "logs_to_query_regex": [
   "Incomplete task interrupted: Attempting to kill Python Worker"
  ],
  "llm_template": "Incomplete task interrupted: Attempting to kill Python Worker",
  "cluster_id": 167,
  "update_success": true,
  "template": "Incomplete task interrupted: Attempting to kill Python Worker"
 },
 {
  "iter": 156,
  "logs_to_query": [
   "ShuffleMapStage 4 is now unavailable on executor 2 (0/2, false)",
   "ShuffleMapStage 11 is now unavailable on executor 19 (25/118, false)",
   "ShuffleMapStage 7 is now unavailable on executor 3 (48/80, false)"
  ],
  "logs_to_query_regex": [
   "ShuffleMapStage 4 is now unavailable on executor 2 (0/2, false)",
   "ShuffleMapStage 11 is now unavailable on executor 19 (25/118, false)",
   "ShuffleMapStage 7 is now unavailable on executor 3 (48/80, false)"
  ],
  "llm_template": "ShuffleMapStage <*> is now unavailable on executor <*> (<*>, false)",
  "cluster_id": 186,
  "update_success": true,
  "template": "ShuffleMapStage <*> is now unavailable on executor <*> (<*>, <*>)"
 },
 {
  "iter": 157,
  "logs_to_query": [
   "Lost task 9.0 in stage 1.0 (TID 11, mesos-slave-13): TaskKilled (killed intentionally)",
   "Lost task 21.0 in stage 108.0 (TID 4705, mesos-slave-06): TaskKilled (killed intentionally)",
   "Lost task 25.3 in stage 2.0 (TID 181, mesos-slave-23): TaskKilled (killed intentionally)"
  ],
  "logs_to_query_regex": [
   "Lost task 9.0 in stage 1.0 (TID 11, mesos-slave-13): TaskKilled (killed intentionally)",
   "Lost task 21.0 in stage 108.0 (TID 4705, mesos-slave-06): TaskKilled (killed intentionally)",
   "Lost task 25.3 in stage 2.0 (TID 181, mesos-slave-23): TaskKilled (killed intentionally)"
  ],
  "llm_template": "Lost task <*> in stage <*> (TID <*> mesos-slave-<*>): TaskKilled (killed intentionally)",
  "cluster_id": 198,
  "update_success": true,
  "template": "Lost task <*> in stage <*> (TID <*>, <*>): TaskKilled (killed intentionally)"
 },
 {
  "iter": 158,
  "logs_to_query": [
   "waiting: Set(ResultStage 1)",
   "waiting: Set(ResultStage 24)",
   "waiting: Set(ResultStage 75)"
  ],
  "logs_to_query_regex": [
   "waiting: Set(ResultStage 1)",
   "waiting: Set(ResultStage 24)",
   "waiting: Set(ResultStage 75)"
  ],
  "llm_template": "waiting: Set(ResultStage <*>)",
  "cluster_id": 96,
  "update_success": true,
  "template": "waiting: Set(ResultStage <*>)"
 },
 {
  "iter": 159,
  "logs_to_query": [
   "Cleaned RDD 72",
   "Cleaned RDD 107",
   "Cleaned RDD 97"
  ],
  "logs_to_query_regex": [
   "Cleaned RDD 72",
   "Cleaned RDD 107",
   "Cleaned RDD 97"
  ],
  "llm_template": "Cleaned RDD <*>",
  "cluster_id": 98,
  "update_success": true,
  "template": "Cleaned RDD <*>"
 },
 {
  "iter": 160,
  "logs_to_query": [
   "ShuffleMapStage 0 (reduceByKey at IPLoM.py:121) finished in 33.787 s",
   "ShuffleMapStage 2 (reduceByKey at pnmf_dblp.py:390) finished in 1.809 s",
   "ShuffleMapStage 7 (reduceByKey at pnmf_dblp.py:423) finished in 218.982 s"
  ],
  "logs_to_query_regex": [
   "ShuffleMapStage 0 (reduceByKey at IPLoM.py:121) finished in 33.787 s",
   "ShuffleMapStage 2 (reduceByKey at pnmf_dblp.py:390) finished in 1.809 s",
   "ShuffleMapStage 7 (reduceByKey at pnmf_dblp.py:423) finished in 218.982 s"
  ],
  "llm_template": "ShuffleMapStage <*> (reduceByKey at IPLoM.py:<*>) finished in <*> s",
  "cluster_id": 179,
  "update_success": true,
  "template": "ShuffleMapStage <*> (<*> at <*>) finished in <*> s"
 },
 {
  "iter": 161,
  "logs_to_query": [
   "Error sending message [message = GetLocations(broadcast_5_piece265)] in 1 attempts",
   "Error sending message [message = GetLocations(broadcast_5_piece294)] in 1 attempts",
   "Error sending message [message = GetLocations(broadcast_44_piece0)] in 2 attempts"
  ],
  "logs_to_query_regex": [
   "Error sending message [message = GetLocations(broadcast_5_piece265)] in 1 attempts",
   "Error sending message [message = GetLocations(broadcast_5_piece294)] in 1 attempts",
   "Error sending message [message = GetLocations(broadcast_44_piece0)] in 2 attempts"
  ],
  "llm_template": "Error sending message [message = GetLocations(<*>)] in <*> attempts",
  "cluster_id": 176,
  "update_success": true,
  "template": "Error sending message [message = <*>] in <*> attempts"
 },
 {
  "iter": 162,
  "logs_to_query": [
   "ApplicationAttemptId: appattempt_1485248649253_0020_000002"
  ],
  "logs_to_query_regex": [
   "ApplicationAttemptId: appattempt_1485248649253_0020_000002"
  ],
  "llm_template": "ApplicationAttemptId: <*>",
  "cluster_id": 7,
  "update_success": true,
  "template": "ApplicationAttemptId: <*>"
 },
 {
  "iter": 163,
  "logs_to_query": [
   "ShuffleMapStage 1 (reduceByKey at pnmf4.py:295) finished in 22.521 s",
   "ShuffleMapStage 7 (reduceByKey at pnmf_dblp.py:423) finished in 251.234 s",
   "ShuffleMapStage 6 (reduceByKey at pnmf_dblp.py:418) finished in 65.238 s"
  ],
  "logs_to_query_regex": [
   "ShuffleMapStage 1 (reduceByKey at pnmf4.py:295) finished in 22.521 s",
   "ShuffleMapStage 7 (reduceByKey at pnmf_dblp.py:423) finished in 251.234 s",
   "ShuffleMapStage 6 (reduceByKey at pnmf_dblp.py:418) finished in 65.238 s"
  ],
  "llm_template": "ShuffleMapStage <*> (reduceByKey at pnmf4.py:<*>) finished in <*> s",
  "cluster_id": 179,
  "update_success": true,
  "template": "ShuffleMapStage <*> (<*> at <*>) finished in <*> s"
 },
 {
  "iter": 164,
  "logs_to_query": [
   "Registering the ApplicationMaster"
  ],
  "logs_to_query_regex": [
   "Registering the ApplicationMaster"
  ],
  "llm_template": "Registering the <*>",
  "cluster_id": 13,
  "update_success": true,
  "template": "Registering the ApplicationMaster"
 },
 {
  "iter": 165,
  "logs_to_query": [
   "Connecting to ResourceManager at mesos-master-1/10.10.34.11:8030"
  ],
  "logs_to_query_regex": [
   "Connecting to ResourceManager at mesos-master-1/10.10.34.11:8030"
  ],
  "llm_template": "Connecting to ResourceManager at mesos-master-1/<*>",
  "cluster_id": 124,
  "update_success": true,
  "template": "Connecting to ResourceManager at <*>"
 },
 {
  "iter": 166,
  "logs_to_query": [
   "Started progress reporter thread with (heartbeat : 3000, initial allocation : 200) intervals"
  ],
  "logs_to_query_regex": [
   "Started progress reporter thread with (heartbeat : 3000, initial allocation : 200) intervals"
  ],
  "llm_template": "Started progress reporter thread with (heartbeat : <*> initial allocation : <*>) intervals",
  "cluster_id": 201,
  "update_success": true,
  "template": "Started progress reporter thread with (heartbeat : <*>, initial allocation : <*>) intervals"
 },
 {
  "iter": 167,
  "logs_to_query": [
   "Missing an output location for shuffle 0",
   "Missing an output location for shuffle 3",
   "Missing an output location for shuffle 78"
  ],
  "logs_to_query_regex": [
   "Missing an output location for shuffle 0",
   "Missing an output location for shuffle 3",
   "Missing an output location for shuffle 78"
  ],
  "llm_template": "Missing an output location for shuffle <*>",
  "cluster_id": 163,
  "update_success": true,
  "template": "Missing an output location for shuffle <*>"
 },
 {
  "iter": 168,
  "logs_to_query": [
   "Lost task 0.2 in stage 5.1 (TID 18, mesos-slave-27): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=0, message=",
   "Lost task 0.1 in stage 3.3 (TID 30, mesos-slave-23): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=0, message=",
   "Lost task 0.1 in stage 5.1 (TID 23, mesos-slave-25): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=2, message="
  ],
  "logs_to_query_regex": [
   "Lost task 0.2 in stage 5.1 (TID 18, mesos-slave-27): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=0, message=",
   "Lost task 0.1 in stage 3.3 (TID 30, mesos-slave-23): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=0, message=",
   "Lost task 0.1 in stage 5.1 (TID 23, mesos-slave-25): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=2, message="
  ],
  "llm_template": "Lost task <*> in stage <*> (TID <*> mesos-slave-<*>): FetchFailed(null, shuffleId=<*>, mapId=-<*>, reduceId=<*>, message=",
  "cluster_id": 208,
  "update_success": true,
  "template": "Lost task <*> in stage <*> (TID <*>, <*>): FetchFailed(<*>, shuffleId=<*>, mapId=-<*>, reduceId=<*>, message=<*>"
 },
 {
  "iter": 169,
  "logs_to_query": [
   "Resubmitting failed stages"
  ],
  "logs_to_query_regex": [
   "Resubmitting failed stages"
  ],
  "llm_template": "Resubmitting failed stages",
  "cluster_id": 83,
  "update_success": true,
  "template": "Resubmitting failed stages"
 },
 {
  "iter": 170,
  "logs_to_query": [
   "Exception in createBlockOutputStream"
  ],
  "logs_to_query_regex": [
   "Exception in createBlockOutputStream"
  ],
  "llm_template": "Exception in createBlockOutputStream",
  "cluster_id": 98,
  "update_success": true,
  "template": "Exception in createBlockOutputStream"
 },
 {
  "iter": 171,
  "logs_to_query": [
   "Missing parents: List(ShuffleMapStage 1)",
   "Missing parents: List(ShuffleMapStage 33)",
   "Missing parents: List(ShuffleMapStage 9)"
  ],
  "logs_to_query_regex": [
   "Missing parents: List(ShuffleMapStage 1)",
   "Missing parents: List(ShuffleMapStage 33)",
   "Missing parents: List(ShuffleMapStage 9)"
  ],
  "llm_template": "Missing parents: List(ShuffleMapStage <*>)",
  "cluster_id": 111,
  "update_success": true,
  "template": "Missing parents: List(<*>)"
 },
 {
  "iter": 172,
  "logs_to_query": [
   "Lost task 7.0 in stage 1.0 (TID 9) on executor mesos-slave-27: org.apache.spark.api.python.PythonException (Traceback (most recent call last):",
   "Lost task 20.0 in stage 1.0 (TID 22) on executor mesos-master-3: org.apache.spark.api.python.PythonException (Traceback (most recent call last):",
   "Lost task 38.0 in stage 2.0 (TID 80) on executor mesos-slave-11: org.apache.spark.api.python.PythonException (Traceback (most recent call last):"
  ],
  "logs_to_query_regex": [
   "Lost task 7.0 in stage 1.0 (TID 9) on executor mesos-slave-27: org.apache.spark.api.python.PythonException (Traceback (most recent call last):",
   "Lost task 20.0 in stage 1.0 (TID 22) on executor mesos-master-3: org.apache.spark.api.python.PythonException (Traceback (most recent call last):",
   "Lost task 38.0 in stage 2.0 (TID 80) on executor mesos-slave-11: org.apache.spark.api.python.PythonException (Traceback (most recent call last):"
  ],
  "llm_template": "Lost task <*> in stage <*> (TID <*>) on executor <*>: org.apache.spark.api.python.PythonException (Traceback (most recent call last):",
  "cluster_id": 214,
  "update_success": true,
  "template": "Lost task <*> in stage <*> (TID <*>) on executor <*>: org.apache.spark.api.python.PythonException (Traceback (most recent call last):"
 },
 {
  "iter": 173,
  "logs_to_query": [
   "MapOutputTrackerMasterEndpoint stopped!"
  ],
  "logs_to_query_regex": [
   "MapOutputTrackerMasterEndpoint stopped!"
  ],
  "llm_template": "MapOutputTrackerMasterEndpoint stopped!",
  "cluster_id": 9,
  "update_success": true,
  "template": "MapOutputTrackerMasterEndpoint stopped!"
 },
 {
  "iter": 174,
  "logs_to_query": [
   "YarnClusterScheduler.postStartHook done"
  ],
  "logs_to_query_regex": [
   "YarnClusterScheduler.postStartHook done"
  ],
  "llm_template": "YarnClusterScheduler.postStartHook done",
  "cluster_id": 11,
  "update_success": true,
  "template": "YarnClusterScheduler.postStartHook done"
 },
 {
  "iter": 175,
  "logs_to_query": [
   "Container marked as failed: container_1485248649253_0126_01_000013 on host: mesos-slave-22. Exit status: 1. Diagnostics: Exception from container-launch.",
   "Container marked as failed: container_1485248649253_0132_01_000053 on host: mesos-slave-26. Exit status: 1. Diagnostics: Exception from container-launch.",
   "Container marked as failed: container_1485248649253_0076_01_000003 on host: mesos-slave-21. Exit status: 52. Diagnostics: Exception from container-launch."
  ],
  "logs_to_query_regex": [
   "Container marked as failed: container_1485248649253_0126_01_000013 on host: mesos-slave-22. Exit status: 1. Diagnostics: Exception from container-launch.",
   "Container marked as failed: container_1485248649253_0132_01_000053 on host: mesos-slave-26. Exit status: 1. Diagnostics: Exception from container-launch.",
   "Container marked as failed: container_1485248649253_0076_01_000003 on host: mesos-slave-21. Exit status: 52. Diagnostics: Exception from container-launch."
  ],
  "llm_template": "Container marked as failed: <*> on host: <*> Exit status: <*> Diagnostics: Exception from container-launch.",
  "cluster_id": 210,
  "update_success": true,
  "template": "Container marked as failed: <*> on host: <*>. Exit status: <*>. Diagnostics: Exception from container-launch."
 },
 {
  "iter": 176,
  "logs_to_query": [
   "Failed to send RPC 7566235838710499706 to mesos-slave-06/10.10.34.16:33023: java.nio.channels.ClosedChannelException",
   "Failed to send RPC 6364117827745496549 to mesos-slave-20/10.10.34.30:60510: java.nio.channels.ClosedChannelException",
   "Failed to send RPC 8551390897890018762 to mesos-master-3/10.10.34.13:46011: java.nio.channels.ClosedChannelException"
  ],
  "logs_to_query_regex": [
   "Failed to send RPC 7566235838710499706 to mesos-slave-06/10.10.34.16:33023: java.nio.channels.ClosedChannelException",
   "Failed to send RPC 6364117827745496549 to mesos-slave-20/10.10.34.30:60510: java.nio.channels.ClosedChannelException",
   "Failed to send RPC 8551390897890018762 to mesos-master-3/10.10.34.13:46011: java.nio.channels.ClosedChannelException"
  ],
  "llm_template": "Failed to send RPC <*> to mesos-slave-<*>: java.nio.channels.ClosedChannelException",
  "cluster_id": 168,
  "update_success": true,
  "template": "Failed to send RPC <*> to <*>: <*>"
 },
 {
  "iter": 177,
  "logs_to_query": [
   "Stage 1 was cancelled",
   "Stage 3 was cancelled",
   "Stage 108 was cancelled"
  ],
  "logs_to_query_regex": [
   "Stage 1 was cancelled",
   "Stage 3 was cancelled",
   "Stage 108 was cancelled"
  ],
  "llm_template": "Stage <*> was cancelled",
  "cluster_id": 118,
  "update_success": true,
  "template": "Stage <*> was cancelled"
 },
 {
  "iter": 178,
  "logs_to_query": [
   "Lost an executor 7 (already removed): Pending loss reason.",
   "Lost an executor 2 (already removed): Pending loss reason.",
   "Lost an executor 17 (already removed): Pending loss reason."
  ],
  "logs_to_query_regex": [
   "Lost an executor 7 (already removed): Pending loss reason.",
   "Lost an executor 2 (already removed): Pending loss reason.",
   "Lost an executor 17 (already removed): Pending loss reason."
  ],
  "llm_template": "Lost an executor <*> (already removed): Pending loss reason.",
  "cluster_id": 181,
  "update_success": true,
  "template": "Lost an executor <*> (already removed): Pending loss reason."
 },
 {
  "iter": 179,
  "logs_to_query": [
   "Driver terminated or disconnected! Shutting down. mesos-master-1:35910",
   "Driver terminated or disconnected! Shutting down. mesos-master-1:42303",
   "Driver terminated or disconnected! Shutting down. 10.10.34.11:51096"
  ],
  "logs_to_query_regex": [
   "Driver terminated or disconnected! Shutting down. mesos-master-1:35910",
   "Driver terminated or disconnected! Shutting down. mesos-master-1:42303",
   "Driver terminated or disconnected! Shutting down. 10.10.34.11:51096"
  ],
  "llm_template": "Driver terminated or disconnected! Shutting down. mesos-master-<*>",
  "cluster_id": 164,
  "update_success": true,
  "template": "Driver terminated or disconnected! Shutting down. <*>"
 },
 {
  "iter": 180,
  "logs_to_query": [
   "User application exited with status 1"
  ],
  "logs_to_query_regex": [
   "User application exited with status 1"
  ],
  "llm_template": "User application exited with status <*>",
  "cluster_id": 145,
  "update_success": true,
  "template": "User application exited with status <*>"
 },
 {
  "iter": 181,
  "logs_to_query": [
   "Waiting for application to be successfully unregistered."
  ],
  "logs_to_query_regex": [
   "Waiting for application to be successfully unregistered."
  ],
  "llm_template": "Waiting for application to be successfully unregistered.",
  "cluster_id": 155,
  "update_success": true,
  "template": "Waiting for application to be successfully unregistered."
 },
 {
  "iter": 182,
  "logs_to_query": [
   "Deleting staging directory .sparkStaging/application_1485248649253_0020",
   "Deleting staging directory .sparkStaging/application_1485248649253_0059",
   "Deleting staging directory .sparkStaging/application_1485248649253_0062"
  ],
  "logs_to_query_regex": [
   "Deleting staging directory .sparkStaging/application_1485248649253_0020",
   "Deleting staging directory .sparkStaging/application_1485248649253_0059",
   "Deleting staging directory .sparkStaging/application_1485248649253_0062"
  ],
  "llm_template": "Deleting staging directory .sparkStaging/application_<*>",
  "cluster_id": 107,
  "update_success": true,
  "template": "Deleting staging directory <*>"
 },
 {
  "iter": 183,
  "logs_to_query": [
   "Lost task 12.0 in stage 1.0 (TID 14, mesos-slave-24): java.io.IOException: Cannot run program \"/home/curi/anaconda2/bin/python\": error=2, No such file or directory",
   "Lost task 4.0 in stage 7.0 (TID 264, mesos-slave-24): java.io.IOException: Cannot run program \"/home/curi/anaconda2/bin/python\": error=2, No such file or directory",
   "Lost task 11.0 in stage 53.0 (TID 2269, mesos-master-2): java.io.IOException: Cannot run program \"/home/curi/anaconda2/bin/python\": error=2, No such file or directory"
  ],
  "logs_to_query_regex": [
   "Lost task 12.0 in stage 1.0 (TID 14, mesos-slave-24): java.io.IOException: Cannot run program \"/home/curi/anaconda2/bin/python\": error=2, No such file or directory",
   "Lost task 4.0 in stage 7.0 (TID 264, mesos-slave-24): java.io.IOException: Cannot run program \"/home/curi/anaconda2/bin/python\": error=2, No such file or directory",
   "Lost task 11.0 in stage 53.0 (TID 2269, mesos-master-2): java.io.IOException: Cannot run program \"/home/curi/anaconda2/bin/python\": error=2, No such file or directory"
  ],
  "llm_template": "Lost task <*> in stage <*> (TID <*> <*>): java.io.IOException: Cannot run program \"<*>\": error=<*>, No such file or directory",
  "cluster_id": 220,
  "update_success": true,
  "template": "Lost task <*> in stage <*> (TID <*>, <*>): java.io.IOException: Cannot run program <*>: error=<*>, No such file or directory"
 },
 {
  "iter": 184,
  "logs_to_query": [
   "Container marked as failed: container_1485248649253_0037_01_000002 on host: mesos-slave-16. Exit status: -100. Diagnostics: Container expired since it was unused",
   "Container marked as failed: container_1485248649253_0072_02_000014 on host: mesos-slave-09. Exit status: -100. Diagnostics: Container expired since it was unused",
   "Container marked as failed: container_1485248649253_0174_01_000012 on host: mesos-slave-15. Exit status: -100. Diagnostics: Container expired since it was unused"
  ],
  "logs_to_query_regex": [
   "Container marked as failed: container_1485248649253_0037_01_000002 on host: mesos-slave-16. Exit status: -100. Diagnostics: Container expired since it was unused",
   "Container marked as failed: container_1485248649253_0072_02_000014 on host: mesos-slave-09. Exit status: -100. Diagnostics: Container expired since it was unused",
   "Container marked as failed: container_1485248649253_0174_01_000012 on host: mesos-slave-15. Exit status: -100. Diagnostics: Container expired since it was unused"
  ],
  "llm_template": "Container marked as failed: <*> on host: mesos-slave-<*>. Exit status: -<*>. Diagnostics: Container expired since it was unused",
  "cluster_id": 216,
  "update_success": true,
  "template": "Container marked as failed: <*> on host: <*>. Exit status: <*>. Diagnostics: Container expired since it was unused"
 },
 {
  "iter": 185,
  "logs_to_query": [
   "Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.10.34.23:45586]",
   "Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.10.34.30:53059]",
   "Remoting started; listening on addresses :[akka.tcp://sparkExecutor@mesos-slave-30:52168]"
  ],
  "logs_to_query_regex": [
   "Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.10.34.23:45586]",
   "Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.10.34.30:53059]",
   "Remoting started; listening on addresses :[akka.tcp://sparkExecutor@mesos-slave-30:52168]"
  ],
  "llm_template": "Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@<*>]",
  "cluster_id": 133,
  "update_success": true,
  "template": "Remoting started; listening on addresses :[<*>]"
 },
 {
  "iter": 186,
  "logs_to_query": [
   "This may have been caused by a prior exception:"
  ],
  "logs_to_query_regex": [
   "This may have been caused by a prior exception:"
  ],
  "llm_template": "This may have been caused by a prior exception:",
  "cluster_id": 181,
  "update_success": true,
  "template": "This may have been caused by a prior exception:"
 },
 {
  "iter": 187,
  "logs_to_query": [
   "Starting job: count at pnmf4.py:333",
   "Starting job: count at pnmf4.py:353",
   "Starting job: count at IPLoM.py:161"
  ],
  "logs_to_query_regex": [
   "Starting job: count at pnmf4.py:333",
   "Starting job: count at pnmf4.py:353",
   "Starting job: count at IPLoM.py:161"
  ],
  "llm_template": "Starting job: count at <*>",
  "cluster_id": 128,
  "update_success": true,
  "template": "Starting job: count at <*>"
 },
 {
  "iter": 188,
  "logs_to_query": [
   "Connection to mesos-master-1/10.10.34.11:51096 has been quiet for 120000 ms while there are outstanding requests. Assuming connection is dead; please adjust spark.network.timeout if this is wrong.",
   "Connection to mesos-master-1/10.10.34.11:34641 has been quiet for 120000 ms while there are outstanding requests. Assuming connection is dead; please adjust spark.network.timeout if this is wrong.",
   "Connection to /10.10.34.11:51096 has been quiet for 120000 ms while there are outstanding requests. Assuming connection is dead; please adjust spark.network.timeout if this is wrong."
  ],
  "logs_to_query_regex": [
   "Connection to mesos-master-1/10.10.34.11:51096 has been quiet for 120000 ms while there are outstanding requests. Assuming connection is dead; please adjust spark.network.timeout if this is wrong.",
   "Connection to mesos-master-1/10.10.34.11:34641 has been quiet for 120000 ms while there are outstanding requests. Assuming connection is dead; please adjust spark.network.timeout if this is wrong.",
   "Connection to /10.10.34.11:51096 has been quiet for 120000 ms while there are outstanding requests. Assuming connection is dead; please adjust spark.network.timeout if this is wrong."
  ],
  "llm_template": "Connection to mesos-master-1/<*> has been quiet for <*> ms while there are outstanding requests. Assuming connection is dead; please adjust spark.network.timeout if this is wrong.",
  "cluster_id": 225,
  "update_success": true,
  "template": "Connection to <*> has been quiet for <*> ms while there are outstanding requests. Assuming connection is dead; please adjust spark.network.timeout if this is wrong."
 },
 {
  "iter": 189,
  "logs_to_query": [
   "Unregistering ApplicationMaster with FAILED (diag message: User application exited with status 1)"
  ],
  "logs_to_query_regex": [
   "Unregistering ApplicationMaster with FAILED (diag message: User application exited with status 1)"
  ],
  "llm_template": "Unregistering ApplicationMaster with FAILED (diag message: User application exited with status <*>)",
  "cluster_id": 199,
  "update_success": true,
  "template": "Unregistering ApplicationMaster with <*>"
 },
 {
  "iter": 190,
  "logs_to_query": [
   "Driver now available: 10.10.34.11:35910",
   "Driver now available: 10.10.34.11:35430",
   "Driver now available: 10.10.34.11:32921"
  ],
  "logs_to_query_regex": [
   "Driver now available: 10.10.34.11:35910",
   "Driver now available: 10.10.34.11:35430",
   "Driver now available: 10.10.34.11:32921"
  ],
  "llm_template": "Driver now available: <*>",
  "cluster_id": 118,
  "update_success": true,
  "template": "Driver now available: <*>"
 },
 {
  "iter": 191,
  "logs_to_query": [
   "SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8"
  ],
  "logs_to_query_regex": [
   "SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8"
  ],
  "llm_template": "SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: <*>",
  "cluster_id": 187,
  "update_success": true,
  "template": "SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: <*>"
 },
 {
  "iter": 192,
  "logs_to_query": [
   "Python worker exited unexpectedly (crashed)"
  ],
  "logs_to_query_regex": [
   "Python worker exited unexpectedly (crashed)"
  ],
  "llm_template": "Python worker exited unexpectedly (crashed)",
  "cluster_id": 132,
  "update_success": true,
  "template": "Python worker exited unexpectedly (crashed)"
 },
 {
  "iter": 193,
  "logs_to_query": [
   "Starting the user application in a separate Thread"
  ],
  "logs_to_query_regex": [
   "Starting the user application in a separate Thread"
  ],
  "llm_template": "Starting the user application in a separate Thread",
  "cluster_id": 170,
  "update_success": true,
  "template": "Starting the user application in a separate Thread"
 },
 {
  "iter": 194,
  "logs_to_query": [
   "Final app status: FAILED, exitCode: 1, (reason: User application exited with status 1)"
  ],
  "logs_to_query_regex": [
   "Final app status: FAILED, exitCode: 1, (reason: User application exited with status 1)"
  ],
  "llm_template": "Final app status: FAILED, exitCode: <*> (reason: User application exited with status <*>)",
  "cluster_id": 204,
  "update_success": true,
  "template": "Final app status: <*>, exitCode: <*>, (reason: <*>)"
 },
 {
  "iter": 195,
  "logs_to_query": [
   "Ignoring response for RPC 6373440704166081624 from mesos-master-1/10.10.34.11:51096 (81 bytes) since it is not outstanding",
   "Ignoring response for RPC 4844382251313928929 from mesos-master-3/10.10.34.13:40989 (273 bytes) since it is not outstanding",
   "Ignoring response for RPC 5623435829345208755 from mesos-master-3/10.10.34.13:46011 (305 bytes) since it is not outstanding"
  ],
  "logs_to_query_regex": [
   "Ignoring response for RPC 6373440704166081624 from mesos-master-1/10.10.34.11:51096 (81 bytes) since it is not outstanding",
   "Ignoring response for RPC 4844382251313928929 from mesos-master-3/10.10.34.13:40989 (273 bytes) since it is not outstanding",
   "Ignoring response for RPC 5623435829345208755 from mesos-master-3/10.10.34.13:46011 (305 bytes) since it is not outstanding"
  ],
  "llm_template": "Ignoring response for RPC <*> from mesos-master-3/<*> (<*> bytes) since it is not outstanding",
  "cluster_id": 209,
  "update_success": true,
  "template": "Ignoring response for RPC <*> from <*> (<*> bytes) since it is not outstanding"
 },
 {
  "iter": 196,
  "logs_to_query": [
   "ShuffleMapStage 1 (reduceByKey at pnmf_dblp.py:389) finished in 22.465 s",
   "ShuffleMapStage 6 (reduceByKey at pnmf_dblp.py:418) finished in 39.313 s",
   "ShuffleMapStage 9 (reduceByKey at pnmf_dblp.py:418) finished in 18.747 s"
  ],
  "logs_to_query_regex": [
   "ShuffleMapStage 1 (reduceByKey at pnmf_dblp.py:389) finished in 22.465 s",
   "ShuffleMapStage 6 (reduceByKey at pnmf_dblp.py:418) finished in 39.313 s",
   "ShuffleMapStage 9 (reduceByKey at pnmf_dblp.py:418) finished in 18.747 s"
  ],
  "llm_template": "ShuffleMapStage <*> (reduceByKey at pnmf_dblp.py:<*>) finished in <*> s",
  "cluster_id": 179,
  "update_success": true,
  "template": "ShuffleMapStage <*> (<*> at <*>) finished in <*> s"
 },
 {
  "iter": 197,
  "logs_to_query": [
   "Lost task 25.0 in stage 1.0 (TID 27, mesos-slave-27): org.apache.spark.api.python.PythonException: Traceback (most recent call last):",
   "Lost task 13.0 in stage 2.0 (TID 55, mesos-slave-11): org.apache.spark.api.python.PythonException: Traceback (most recent call last):",
   "Lost task 0.0 in stage 2.1 (TID 12, mesos-master-2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):"
  ],
  "logs_to_query_regex": [
   "Lost task 25.0 in stage 1.0 (TID 27, mesos-slave-27): org.apache.spark.api.python.PythonException: Traceback (most recent call last):",
   "Lost task 13.0 in stage 2.0 (TID 55, mesos-slave-11): org.apache.spark.api.python.PythonException: Traceback (most recent call last):",
   "Lost task 0.0 in stage 2.1 (TID 12, mesos-master-2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):"
  ],
  "llm_template": "Lost task <*> in stage <*> (TID <*> mesos-<*>): org.apache.spark.api.python.PythonException: Traceback (most recent call last):",
  "cluster_id": 211,
  "update_success": true,
  "template": "Lost task <*> in stage <*> (TID <*>, <*>): org.apache.spark.api.python.PythonException: Traceback (most recent call last):"
 },
 {
  "iter": 198,
  "logs_to_query": [
   "Excluding datanode DatanodeInfoWithStorage[10.10.34.15:50010,DS-8de4ae37-fac2-404a-a8e3-dcecd440f907,DISK]"
  ],
  "logs_to_query_regex": [
   "Excluding datanode DatanodeInfoWithStorage[10.10.34.15:50010,DS-8de4ae37-fac2-404a-a8e3-dcecd440f907,DISK]"
  ],
  "llm_template": "Excluding datanode DatanodeInfoWithStorage[<*>]",
  "cluster_id": 98,
  "update_success": true,
  "template": "Excluding datanode DatanodeInfoWithStorage[<*>,<*>,<*>]"
 },
 {
  "iter": 199,
  "logs_to_query": [
   "ResultStage 5 (collect at pnmf4.py:377) failed in 1052.368 s",
   "ResultStage 108 (collect at pnmf.py:279) failed in 0.354 s",
   "ResultStage 3 (collect at pnmf4.py:376) failed in 733.152 s"
  ],
  "logs_to_query_regex": [
   "ResultStage 5 (collect at pnmf4.py:377) failed in 1052.368 s",
   "ResultStage 108 (collect at pnmf.py:279) failed in 0.354 s",
   "ResultStage 3 (collect at pnmf4.py:376) failed in 733.152 s"
  ],
  "llm_template": "ResultStage <*> (collect at <*>) failed in <*> s",
  "cluster_id": 181,
  "update_success": true,
  "template": "ResultStage <*> (collect at <*>) failed in <*> s"
 },
 {
  "iter": 200,
  "logs_to_query": [
   "Lost task 33.3 in stage 1.0 (TID 72, mesos-master-3): TaskKilled (killed intentionally)"
  ],
  "logs_to_query_regex": [
   "Lost task 33.3 in stage 1.0 (TID 72, mesos-master-3): TaskKilled (killed intentionally)"
  ],
  "llm_template": "Lost task <*> in stage <*> (TID <*> <*>): <*>",
  "cluster_id": 199,
  "update_success": true,
  "template": "Lost task <*> in stage <*> (TID <*>, <*>): TaskKilled (killed intentionally)"
 },
 {
  "iter": 201,
  "logs_to_query": [
   "Error sending result ChunkFetchSuccess{streamChunkId=StreamChunkId{streamId=1003843761537, chunkIndex=0}, buffer=FileSegmentManagedBuffer{file=/opt/hdfs/nodemanager/usercache/curi/appcache/application_1485248649253_0076/blockmgr-dc037334-5e92-45ae-b7b7-642e0abd235d/0c/shuffle_0_0_0.data, offset=0, length=2045089323}} to /10.10.34.27:58087; closing connection",
   "Error sending result ChunkFetchSuccess{streamChunkId=StreamChunkId{streamId=563371592329, chunkIndex=0}, buffer=FileSegmentManagedBuffer{file=/opt/hdfs/nodemanager/usercache/curi/appcache/application_1485248649253_0068/blockmgr-93ca9a72-613d-4f49-b981-c57886eb2c85/0c/shuffle_0_0_0.data, offset=0, length=1269361191}} to /10.10.34.16:45985; closing connection"
  ],
  "logs_to_query_regex": [
   "Error sending result ChunkFetchSuccess{streamChunkId=StreamChunkId{streamId=1003843761537, chunkIndex=0}, buffer=FileSegmentManagedBuffer{file=/opt/hdfs/nodemanager/usercache/curi/appcache/application_1485248649253_0076/blockmgr-dc037334-5e92-45ae-b7b7-642e0abd235d/0c/shuffle_0_0_0.data, offset=0, length=2045089323}} to /10.10.34.27:58087; closing connection",
   "Error sending result ChunkFetchSuccess{streamChunkId=StreamChunkId{streamId=563371592329, chunkIndex=0}, buffer=FileSegmentManagedBuffer{file=/opt/hdfs/nodemanager/usercache/curi/appcache/application_1485248649253_0068/blockmgr-93ca9a72-613d-4f49-b981-c57886eb2c85/0c/shuffle_0_0_0.data, offset=0, length=1269361191}} to /10.10.34.16:45985; closing connection"
  ],
  "llm_template": "Error sending result <*> buffer=<*>} to /<*>; closing connection",
  "cluster_id": 199,
  "update_success": true,
  "template": "Error sending result ChunkFetchSuccess{streamChunkId=<*>, buffer=<*>} to <*>; closing connection"
 },
 {
  "iter": 202,
  "logs_to_query": [
   "BlockManagerMaster stopped"
  ],
  "logs_to_query_regex": [
   "BlockManagerMaster stopped"
  ],
  "llm_template": "BlockManagerMaster stopped",
  "cluster_id": 8,
  "update_success": true,
  "template": "BlockManagerMaster stopped"
 },
 {
  "iter": 203,
  "logs_to_query": [
   "OutputCommitCoordinator stopped!"
  ],
  "logs_to_query_regex": [
   "OutputCommitCoordinator stopped!"
  ],
  "llm_template": "OutputCommitCoordinator stopped!",
  "cluster_id": 9,
  "update_success": true,
  "template": "OutputCommitCoordinator stopped!"
 },
 {
  "iter": 204,
  "logs_to_query": [
   "Abandoning BP-108841162-10.10.34.11-1440074360971:blk_1076204381_2463557"
  ],
  "logs_to_query_regex": [
   "Abandoning BP-108841162-10.10.34.11-1440074360971:blk_1076204381_2463557"
  ],
  "llm_template": "Abandoning BP-<*>:blk_1076204381_2463557",
  "cluster_id": 11,
  "update_success": true,
  "template": "Abandoning <*>"
 },
 {
  "iter": 205,
  "logs_to_query": [
   "Reporting 1066 blocks to the master.",
   "Reporting 1068 blocks to the master.",
   "Reporting 1064 blocks to the master."
  ],
  "logs_to_query_regex": [
   "Reporting 1066 blocks to the master.",
   "Reporting 1068 blocks to the master.",
   "Reporting 1064 blocks to the master."
  ],
  "llm_template": "Reporting <*> blocks to the master.",
  "cluster_id": 145,
  "update_success": true,
  "template": "Reporting <*> blocks to the master."
 },
 {
  "iter": 206,
  "logs_to_query": [
   "Abandoning BP-108841162-10.10.34.11-1440074360971:blk_1076204399_2463575"
  ],
  "logs_to_query_regex": [
   "Abandoning BP-108841162-10.10.34.11-1440074360971:blk_1076204399_2463575"
  ],
  "llm_template": "Abandoning BP-<*>:blk_1076204399_2463575",
  "cluster_id": 11,
  "update_success": true,
  "template": "Abandoning <*>"
 },
 {
  "iter": 207,
  "logs_to_query": [
   "Told to re-register on heartbeat"
  ],
  "logs_to_query_regex": [
   "Told to re-register on heartbeat"
  ],
  "llm_template": "Told to re-register on heartbeat",
  "cluster_id": 132,
  "update_success": true,
  "template": "Told to re-register on heartbeat"
 },
 {
  "iter": 208,
  "logs_to_query": [
   "Interrupted while trying for connection"
  ],
  "logs_to_query_regex": [
   "Interrupted while trying for connection"
  ],
  "llm_template": "Interrupted while trying for connection",
  "cluster_id": 132,
  "update_success": true,
  "template": "Interrupted while trying for connection"
 },
 {
  "iter": 209,
  "logs_to_query": [
   "Requesting to kill executor(s) 8",
   "Requesting to kill executor(s) 13",
   "Requesting to kill executor(s) 4"
  ],
  "logs_to_query_regex": [
   "Requesting to kill executor(s) 8",
   "Requesting to kill executor(s) 13",
   "Requesting to kill executor(s) 4"
  ],
  "llm_template": "Requesting to kill executor(s) <*>",
  "cluster_id": 132,
  "update_success": true,
  "template": "Requesting to kill executor(s) <*>"
 },
 {
  "iter": 210,
  "logs_to_query": [
   "Error occurred while fetching local blocks"
  ],
  "logs_to_query_regex": [
   "Error occurred while fetching local blocks"
  ],
  "llm_template": "Error occurred while fetching local blocks",
  "cluster_id": 145,
  "update_success": true,
  "template": "Error occurred while fetching local blocks"
 },
 {
  "iter": 211,
  "logs_to_query": [
   "Issue communicating with driver in heartbeater"
  ],
  "logs_to_query_regex": [
   "Issue communicating with driver in heartbeater"
  ],
  "llm_template": "Issue communicating with driver in heartbeater",
  "cluster_id": 145,
  "update_success": true,
  "template": "Issue communicating with driver in heartbeater"
 },
 {
  "iter": 212,
  "logs_to_query": [
   "Error sending message [message = RetrieveSparkProps] in 1 attempts",
   "Error sending message [message = RetrieveSparkProps] in 2 attempts",
   "Error sending message [message = RetrieveSparkProps] in 3 attempts"
  ],
  "logs_to_query_regex": [
   "Error sending message [message = RetrieveSparkProps] in 1 attempts",
   "Error sending message [message = RetrieveSparkProps] in 2 attempts",
   "Error sending message [message = RetrieveSparkProps] in 3 attempts"
  ],
  "llm_template": "Error sending message [message = <*>] in <*> attempts",
  "cluster_id": 176,
  "update_success": true,
  "template": "Error sending message [message = <*>] in <*> attempts"
 },
 {
  "iter": 213,
  "logs_to_query": [
   "Error sending result RpcResponse{requestId=8054183328166237564, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=274 cap=274]}} to mesos-slave-05/10.10.34.15:34469; closing connection",
   "Error sending result RpcResponse{requestId=6963534815674450023, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=307 cap=307]}} to mesos-slave-05/10.10.34.15:34469; closing connection"
  ],
  "logs_to_query_regex": [
   "Error sending result RpcResponse{requestId=8054183328166237564, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=274 cap=274]}} to mesos-slave-05/10.10.34.15:34469; closing connection",
   "Error sending result RpcResponse{requestId=6963534815674450023, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=307 cap=307]}} to mesos-slave-05/10.10.34.15:34469; closing connection"
  ],
  "llm_template": "Error sending result <*>} to mesos-slave-05/<*>; closing connection",
  "cluster_id": 195,
  "update_success": true,
  "template": "Error sending result RpcResponse{requestId=<*>, body=<*>} to <*>; closing connection"
 },
 {
  "iter": 214,
  "logs_to_query": [
   "Failed to send RPC 8473052502767486838 to mesos-master-1/10.10.34.11:51096: java.nio.channels.ClosedChannelException",
   "Failed to send RPC 7405029226587737407 to mesos-master-3/10.10.34.13:41309: java.nio.channels.ClosedChannelException",
   "Failed to send RPC 8476040790958197306 to mesos-master-2/10.10.34.12:56239: java.nio.channels.ClosedChannelException"
  ],
  "logs_to_query_regex": [
   "Failed to send RPC 8473052502767486838 to mesos-master-1/10.10.34.11:51096: java.nio.channels.ClosedChannelException",
   "Failed to send RPC 7405029226587737407 to mesos-master-3/10.10.34.13:41309: java.nio.channels.ClosedChannelException",
   "Failed to send RPC 8476040790958197306 to mesos-master-2/10.10.34.12:56239: java.nio.channels.ClosedChannelException"
  ],
  "llm_template": "Failed to send RPC <*> to <*>: java.nio.channels.ClosedChannelException",
  "cluster_id": 168,
  "update_success": true,
  "template": "Failed to send RPC <*> to <*>: <*>"
 },
 {
  "iter": 215,
  "logs_to_query": [
   "Lost executor 8 on mesos-slave-05: Executor heartbeat timed out after 167979 ms",
   "Lost executor 4 on mesos-slave-26: Executor heartbeat timed out after 142872 ms",
   "Lost executor 12 on mesos-master-1: Executor heartbeat timed out after 125162 ms"
  ],
  "logs_to_query_regex": [
   "Lost executor 8 on mesos-slave-05: Executor heartbeat timed out after 167979 ms",
   "Lost executor 4 on mesos-slave-26: Executor heartbeat timed out after 142872 ms",
   "Lost executor 12 on mesos-master-1: Executor heartbeat timed out after 125162 ms"
  ],
  "llm_template": "Lost executor <*> on <*>: Executor heartbeat timed out after <*> ms",
  "cluster_id": 199,
  "update_success": true,
  "template": "Lost executor <*> on <*>: Executor heartbeat timed out after <*> ms"
 },
 {
  "iter": 216,
  "logs_to_query": [
   "Driver terminated or disconnected! Shutting down. 10.10.34.11:51096",
   "Driver terminated or disconnected! Shutting down. 10.10.34.11:45410"
  ],
  "logs_to_query_regex": [
   "Driver terminated or disconnected! Shutting down. 10.10.34.11:51096",
   "Driver terminated or disconnected! Shutting down. 10.10.34.11:45410"
  ],
  "llm_template": "Driver terminated or disconnected! Shutting down. <*>",
  "cluster_id": 164,
  "update_success": true,
  "template": "Driver terminated or disconnected! Shutting down. <*>"
 },
 {
  "iter": 217,
  "logs_to_query": [
   "ResultStage 8 (count at pnmf_dblp.py:425) failed in 96.238 s",
   "ResultStage 8 (count at pnmf_dblp.py:425) failed in 80.005 s",
   "ResultStage 8 (count at pnmf_dblp.py:425) failed in 54.519 s"
  ],
  "logs_to_query_regex": [
   "ResultStage 8 (count at pnmf_dblp.py:425) failed in 96.238 s",
   "ResultStage 8 (count at pnmf_dblp.py:425) failed in 80.005 s",
   "ResultStage 8 (count at pnmf_dblp.py:425) failed in 54.519 s"
  ],
  "llm_template": "ResultStage <*> (count at <*>) failed in <*> s",
  "cluster_id": 181,
  "update_success": true,
  "template": "ResultStage <*> (count at <*>) failed in <*> s"
 },
 {
  "iter": 218,
  "logs_to_query": [
   "Error sending result RpcResponse{requestId=8368974885283700037, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=81 cap=81]}} to mesos-slave-10/10.10.34.20:43262; closing connection",
   "Error sending result RpcResponse{requestId=7448052572406148083, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=81 cap=81]}} to mesos-slave-11/10.10.34.21:43114; closing connection",
   "Error sending result RpcResponse{requestId=4830902348053461854, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=81 cap=81]}} to mesos-slave-10/10.10.34.20:51768; closing connection"
  ],
  "logs_to_query_regex": [
   "Error sending result RpcResponse{requestId=8368974885283700037, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=81 cap=81]}} to mesos-slave-10/10.10.34.20:43262; closing connection",
   "Error sending result RpcResponse{requestId=7448052572406148083, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=81 cap=81]}} to mesos-slave-11/10.10.34.21:43114; closing connection",
   "Error sending result RpcResponse{requestId=4830902348053461854, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=81 cap=81]}} to mesos-slave-10/10.10.34.20:51768; closing connection"
  ],
  "llm_template": "Error sending result <*>} to mesos-slave-<*>; closing connection",
  "cluster_id": 195,
  "update_success": true,
  "template": "Error sending result RpcResponse{requestId=<*>, body=<*>} to <*>; closing connection"
 },
 {
  "iter": 219,
  "logs_to_query": [
   "Lost task 0.0 in stage 5.0 (TID 8) on executor mesos-slave-21: java.lang.OutOfMemoryError (null) [duplicate 1]"
  ],
  "logs_to_query_regex": [
   "Lost task 0.0 in stage 5.0 (TID 8) on executor mesos-slave-21: java.lang.OutOfMemoryError (null) [duplicate 1]"
  ],
  "llm_template": "Lost task <*> in stage <*> (TID <*>) on executor <*>: java.lang.OutOfMemoryError (<*>) [duplicate <*>]",
  "cluster_id": 211,
  "update_success": true,
  "template": "Lost task <*> in stage <*> (TID <*>) on executor <*>: java.lang.OutOfMemoryError (<*>) [duplicate <*>]"
 },
 {
  "iter": 220,
  "logs_to_query": [
   "Exception while deleting local spark dir: /opt/hdfs/nodemanager/usercache/curi/appcache/application_1448006111297_0137/blockmgr-6b9f7dbf-6753-44ec-aef0-44a90e30f5bf",
   "Exception while deleting local spark dir: /opt/hdfs/nodemanager/usercache/curi/appcache/application_1485248649253_0076/blockmgr-5eefbe39-4165-4647-82c9-2b9bcb93625d",
   "Exception while deleting local spark dir: /opt/hdfs/nodemanager/usercache/curi/appcache/application_1485248649253_0027/blockmgr-7dedbb8f-c971-49f9-bd46-5cf4c541eb5c"
  ],
  "logs_to_query_regex": [
   "Exception while deleting local spark dir: /opt/hdfs/nodemanager/usercache/curi/appcache/application_1448006111297_0137/blockmgr-6b9f7dbf-6753-44ec-aef0-44a90e30f5bf",
   "Exception while deleting local spark dir: /opt/hdfs/nodemanager/usercache/curi/appcache/application_1485248649253_0076/blockmgr-5eefbe39-4165-4647-82c9-2b9bcb93625d",
   "Exception while deleting local spark dir: /opt/hdfs/nodemanager/usercache/curi/appcache/application_1485248649253_0027/blockmgr-7dedbb8f-c971-49f9-bd46-5cf4c541eb5c"
  ],
  "llm_template": "Exception while deleting local spark dir: /opt/hdfs/nodemanager/usercache/curi/appcache/application_<*>/blockmgr-<*>",
  "cluster_id": 164,
  "update_success": true,
  "template": "Exception while deleting local spark dir: <*>"
 },
 {
  "iter": 221,
  "logs_to_query": [
   "Ignoring task-finished event for 3.0 in stage 3.0 because task 3 has already completed successfully",
   "Ignoring task-finished event for 1.0 in stage 3.0 because task 1 has already completed successfully",
   "Ignoring task-finished event for 0.0 in stage 5.1 because task 0 has already completed successfully"
  ],
  "logs_to_query_regex": [
   "Ignoring task-finished event for 3.0 in stage 3.0 because task 3 has already completed successfully",
   "Ignoring task-finished event for 1.0 in stage 3.0 because task 1 has already completed successfully",
   "Ignoring task-finished event for 0.0 in stage 5.1 because task 0 has already completed successfully"
  ],
  "llm_template": "Ignoring task-finished event for <*> in stage <*> because task <*> has already completed successfully",
  "cluster_id": 211,
  "update_success": true,
  "template": "Ignoring task-finished event for <*> in stage <*> because task <*> has already completed successfully"
 },
 {
  "iter": 222,
  "logs_to_query": [
   "Task 25 in stage 1.0 failed 4 times; aborting job",
   "Task 44 in stage 1.0 failed 4 times; aborting job",
   "Task 28 in stage 2.0 failed 4 times; aborting job"
  ],
  "logs_to_query_regex": [
   "Task 25 in stage 1.0 failed 4 times; aborting job",
   "Task 44 in stage 1.0 failed 4 times; aborting job",
   "Task 28 in stage 2.0 failed 4 times; aborting job"
  ],
  "llm_template": "Task <*> in stage <*> failed <*> times; aborting job",
  "cluster_id": 187,
  "update_success": true,
  "template": "Task <*> in stage <*> failed <*> times; aborting job"
 },
 {
  "iter": 223,
  "logs_to_query": [
   "Remoting started; listening on addresses :[akka.tcp://driverPropsFetcher@mesos-slave-30:44924]",
   "Remoting started; listening on addresses :[akka.tcp://driverPropsFetcher@mesos-slave-30:38752]",
   "Remoting started; listening on addresses :[akka.tcp://sparkExecutor@mesos-slave-30:33003]"
  ],
  "logs_to_query_regex": [
   "Remoting started; listening on addresses :[akka.tcp://driverPropsFetcher@mesos-slave-30:44924]",
   "Remoting started; listening on addresses :[akka.tcp://driverPropsFetcher@mesos-slave-30:38752]",
   "Remoting started; listening on addresses :[akka.tcp://sparkExecutor@mesos-slave-30:33003]"
  ],
  "llm_template": "Remoting started; listening on addresses :[akka.tcp://<*>@mesos-slave-<*>]",
  "cluster_id": 133,
  "update_success": true,
  "template": "Remoting started; listening on addresses :[<*>]"
 },
 {
  "iter": 224,
  "logs_to_query": [
   "Removing block manager BlockManagerId(5, mesos-master-2, 56213)",
   "Removing block manager BlockManagerId(7, mesos-master-2, 42517)",
   "Removing block manager BlockManagerId(8, mesos-master-2, 49207)"
  ],
  "logs_to_query_regex": [
   "Removing block manager BlockManagerId(5, mesos-master-2, 56213)",
   "Removing block manager BlockManagerId(7, mesos-master-2, 42517)",
   "Removing block manager BlockManagerId(8, mesos-master-2, 49207)"
  ],
  "llm_template": "Removing block manager BlockManagerId(<*>, mesos-master-<*>, <*>)",
  "cluster_id": 140,
  "update_success": true,
  "template": "Removing block manager BlockManagerId(<*>, <*>, <*>)"
 },
 {
  "iter": 225,
  "logs_to_query": [
   "Resubmitting ShuffleMapStage 2 (reduceByKey at pnmf4.py:371) because some of its tasks had failed: 0, 1",
   "Resubmitting ShuffleMapStage 4 (reduceByKey at pnmf4.py:371) because some of its tasks had failed: 1, 3",
   "Resubmitting ShuffleMapStage 4 (reduceByKey at pnmf4.py:371) because some of its tasks had failed: 1, 2"
  ],
  "logs_to_query_regex": [
   "Resubmitting ShuffleMapStage 2 (reduceByKey at pnmf4.py:371) because some of its tasks had failed: 0, 1",
   "Resubmitting ShuffleMapStage 4 (reduceByKey at pnmf4.py:371) because some of its tasks had failed: 1, 3",
   "Resubmitting ShuffleMapStage 4 (reduceByKey at pnmf4.py:371) because some of its tasks had failed: 1, 2"
  ],
  "llm_template": "Resubmitting ShuffleMapStage <*> (reduceByKey at pnmf4.py:<*>) because some of its tasks had failed: <*>",
  "cluster_id": 211,
  "update_success": true,
  "template": "Resubmitting ShuffleMapStage <*> (reduceByKey at <*>) because some of its tasks had failed: <*>"
 },
 {
  "iter": 226,
  "logs_to_query": [
   "Exception in connection from mesos-master-1/10.10.34.11:60970",
   "Exception in connection from mesos-master-3/10.10.34.13:41763",
   "Exception in connection from /10.10.34.27:58087"
  ],
  "logs_to_query_regex": [
   "Exception in connection from mesos-master-1/10.10.34.11:60970",
   "Exception in connection from mesos-master-3/10.10.34.13:41763",
   "Exception in connection from /10.10.34.27:58087"
  ],
  "llm_template": "Exception in connection from <*>",
  "cluster_id": 126,
  "update_success": true,
  "template": "Exception in connection from <*>"
 },
 {
  "iter": 227,
  "logs_to_query": [
   "Got told to re-register updating block broadcast_16_piece0",
   "Got told to re-register updating block broadcast_10_piece0",
   "Got told to re-register updating block broadcast_11_piece0"
  ],
  "logs_to_query_regex": [
   "Got told to re-register updating block broadcast_16_piece0",
   "Got told to re-register updating block broadcast_10_piece0",
   "Got told to re-register updating block broadcast_11_piece0"
  ],
  "llm_template": "Got told to re-register updating block <*>",
  "cluster_id": 164,
  "update_success": true,
  "template": "Got told to re-register updating block <*>"
 },
 {
  "iter": 228,
  "logs_to_query": [
   "Lost task 30.0 in stage 1.0 (TID 32, mesos-slave-05): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 167979 ms",
   "Lost task 20.0 in stage 11.0 (TID 1595, mesos-master-1): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 125162 ms",
   "Lost task 0.0 in stage 3.0 (TID 10, mesos-slave-13): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 137299 ms"
  ],
  "logs_to_query_regex": [
   "Lost task 30.0 in stage 1.0 (TID 32, mesos-slave-05): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 167979 ms",
   "Lost task 20.0 in stage 11.0 (TID 1595, mesos-master-1): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 125162 ms",
   "Lost task 0.0 in stage 3.0 (TID 10, mesos-slave-13): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 137299 ms"
  ],
  "llm_template": "Lost task <*> in stage <*> (TID <*> <*>): ExecutorLostFailure (executor <*> exited caused by one of the running tasks) Reason: Executor heartbeat timed out after <*> ms",
  "cluster_id": 229,
  "update_success": true,
  "template": "Lost task <*> in stage <*> (TID <*>, <*>): ExecutorLostFailure (executor <*> exited caused by one of the running tasks) Reason: Executor heartbeat timed out after <*> ms"
 },
 {
  "iter": 229,
  "logs_to_query": [
   "waiting: Set(ShuffleMapStage 2, ResultStage 3)"
  ],
  "logs_to_query_regex": [
   "waiting: Set(ShuffleMapStage 2, ResultStage 3)"
  ],
  "llm_template": "waiting: Set(<*>, <*>)",
  "cluster_id": 132,
  "update_success": true,
  "template": "waiting: Set(ShuffleMapStage <*>, ResultStage <*>)"
 },
 {
  "iter": 230,
  "logs_to_query": [
   "SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(21, mesos-slave-13, 57719),broadcast_5_piece323,StorageLevel(false, true, false, false, 1),4194304,0,0))",
   "SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(19, mesos-slave-27, 34660),broadcast_4_piece119,StorageLevel(false, true, false, false, 1),4194304,0,0))",
   "SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(20, mesos-slave-25, 55339),rdd_27_9,StorageLevel(false, true, false, false, 1),26,0,0))"
  ],
  "logs_to_query_regex": [
   "SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(21, mesos-slave-13, 57719),broadcast_5_piece323,StorageLevel(false, true, false, false, 1),4194304,0,0))",
   "SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(19, mesos-slave-27, 34660),broadcast_4_piece119,StorageLevel(false, true, false, false, 1),4194304,0,0))",
   "SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(20, mesos-slave-25, 55339),rdd_27_9,StorageLevel(false, true, false, false, 1),26,0,0))"
  ],
  "llm_template": "SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(<*>, <*> <*>),<*>,StorageLevel(<*>, <*> <*>),<*>))",
  "cluster_id": 204,
  "update_success": true,
  "template": "SparkListenerBus has already stopped! Dropping event <*>"
 },
 {
  "iter": 231,
  "logs_to_query": [
   "Final app status: FAILED, exitCode: 10, (reason: Uncaught exception: org.apache.spark.SparkException: Failed to connect to driver!)"
  ],
  "logs_to_query_regex": [
   "Final app status: FAILED, exitCode: 10, (reason: Uncaught exception: org.apache.spark.SparkException: Failed to connect to driver!)"
  ],
  "llm_template": "Final app status: FAILED, exitCode: <*> (reason: Uncaught exception: <*>)",
  "cluster_id": 211,
  "update_success": true,
  "template": "Final app status: <*>, exitCode: <*>, (reason: <*>)"
 },
 {
  "iter": 232,
  "logs_to_query": [
   "Error sending message [message = UpdateBlockInfo(BlockManagerId(4, mesos-slave-13, 40185),broadcast_114_piece0,StorageLevel(false, false, false, false, 1),0,0,0)] in 1 attempts",
   "Error sending message [message = UpdateBlockInfo(BlockManagerId(20, mesos-slave-25, 55339),rdd_27_6,StorageLevel(false, true, false, false, 1),26,0,0)] in 1 attempts",
   "Error sending message [message = UpdateBlockInfo(BlockManagerId(10, mesos-slave-13, 60845),rdd_27_70,StorageLevel(false, true, false, false, 1),117473855,0,0)] in 1 attempts"
  ],
  "logs_to_query_regex": [
   "Error sending message [message = UpdateBlockInfo(BlockManagerId(4, mesos-slave-13, 40185),broadcast_114_piece0,StorageLevel(false, false, false, false, 1),0,0,0)] in 1 attempts",
   "Error sending message [message = UpdateBlockInfo(BlockManagerId(20, mesos-slave-25, 55339),rdd_27_6,StorageLevel(false, true, false, false, 1),26,0,0)] in 1 attempts",
   "Error sending message [message = UpdateBlockInfo(BlockManagerId(10, mesos-slave-13, 60845),rdd_27_70,StorageLevel(false, true, false, false, 1),117473855,0,0)] in 1 attempts"
  ],
  "llm_template": "Error sending message [message = UpdateBlockInfo(BlockManagerId(<*>, <*> <*>),<*>,StorageLevel(<*>, <*> <*>),<*>)] in <*> attempts",
  "cluster_id": 211,
  "update_success": true,
  "template": "Error sending message [message = <*>] in <*> attempts"
 },
 {
  "iter": 233,
  "logs_to_query": [
   "Lost task 1.0 in stage 5.0 (TID 9, mesos-slave-08): java.lang.OutOfMemoryError: Requested array size exceeds VM limit",
   "Lost task 1.0 in stage 3.0 (TID 9, mesos-master-2): java.lang.OutOfMemoryError: Requested array size exceeds VM limit",
   "Lost task 0.0 in stage 3.0 (TID 8, mesos-slave-06): java.lang.OutOfMemoryError: Requested array size exceeds VM limit"
  ],
  "logs_to_query_regex": [
   "Lost task 1.0 in stage 5.0 (TID 9, mesos-slave-08): java.lang.OutOfMemoryError: Requested array size exceeds VM limit",
   "Lost task 1.0 in stage 3.0 (TID 9, mesos-master-2): java.lang.OutOfMemoryError: Requested array size exceeds VM limit",
   "Lost task 0.0 in stage 3.0 (TID 8, mesos-slave-06): java.lang.OutOfMemoryError: Requested array size exceeds VM limit"
  ],
  "llm_template": "Lost task <*> in stage <*> (TID <*> <*>): java.lang.OutOfMemoryError: Requested array size exceeds VM limit",
  "cluster_id": 212,
  "update_success": true,
  "template": "Lost task <*> in stage <*> (TID <*>, <*>): java.lang.OutOfMemoryError: Requested array size exceeds VM limit"
 },
 {
  "iter": 234,
  "logs_to_query": [
   "Resubmitted ShuffleMapTask(11, 1), so marking it as still running",
   "Resubmitted ShuffleMapTask(11, 50), so marking it as still running",
   "Resubmitted ShuffleMapTask(11, 90), so marking it as still running"
  ],
  "logs_to_query_regex": [
   "Resubmitted ShuffleMapTask(11, 1), so marking it as still running",
   "Resubmitted ShuffleMapTask(11, 50), so marking it as still running",
   "Resubmitted ShuffleMapTask(11, 90), so marking it as still running"
  ],
  "llm_template": "Resubmitted ShuffleMapTask(<*>, <*>), so marking it as still running",
  "cluster_id": 181,
  "update_success": true,
  "template": "Resubmitted ShuffleMapTask(<*>, <*>), so marking it as still running"
 },
 {
  "iter": 235,
  "logs_to_query": [
   "Ignored failure: java.io.IOException: Connection from mesos-master-1/10.10.34.11:34641 closed",
   "Ignored failure: java.io.IOException: Connection from mesos-master-1/10.10.34.11:33750 closed",
   "Ignored failure: java.io.IOException: Connection from mesos-master-1/10.10.34.11:47966 closed"
  ],
  "logs_to_query_regex": [
   "Ignored failure: java.io.IOException: Connection from mesos-master-1/10.10.34.11:34641 closed",
   "Ignored failure: java.io.IOException: Connection from mesos-master-1/10.10.34.11:33750 closed",
   "Ignored failure: java.io.IOException: Connection from mesos-master-1/10.10.34.11:47966 closed"
  ],
  "llm_template": "Ignored failure: java.io.IOException: Connection from mesos-master-1/<*> closed",
  "cluster_id": 164,
  "update_success": true,
  "template": "Ignored failure: java.io.IOException: Connection from <*> closed"
 },
 {
  "iter": 236,
  "logs_to_query": [
   "Uncaught exception in thread Thread[Executor task launch worker-1,5,main]",
   "Uncaught exception in thread Thread[Executor task launch worker-0,5,main]"
  ],
  "logs_to_query_regex": [
   "Uncaught exception in thread Thread[Executor task launch worker-1,5,main]",
   "Uncaught exception in thread Thread[Executor task launch worker-0,5,main]"
  ],
  "llm_template": "Uncaught exception in thread Thread[Executor task launch worker-<*>,main]",
  "cluster_id": 170,
  "update_success": true,
  "template": "Uncaught exception in thread Thread[Executor task launch <*>,<*>]"
 },
 {
  "iter": 237,
  "logs_to_query": [
   "Resubmitting ShuffleMapStage 4 (reduceByKey at pnmf4.py:371) and ResultStage 5 (collect at pnmf4.py:377) due to fetch failure",
   "Resubmitting ShuffleMapStage 2 (reduceByKey at pnmf4.py:371) and ResultStage 3 (collect at pnmf4.py:376) due to fetch failure"
  ],
  "logs_to_query_regex": [
   "Resubmitting ShuffleMapStage 4 (reduceByKey at pnmf4.py:371) and ResultStage 5 (collect at pnmf4.py:377) due to fetch failure",
   "Resubmitting ShuffleMapStage 2 (reduceByKey at pnmf4.py:371) and ResultStage 3 (collect at pnmf4.py:376) due to fetch failure"
  ],
  "llm_template": "Resubmitting ShuffleMapStage <*> (reduceByKey at pnmf4.py:<*>) and ResultStage <*> (collect at pnmf4.py:<*>) due to fetch failure",
  "cluster_id": 212,
  "update_success": true,
  "template": "Resubmitting ShuffleMapStage <*> (reduceByKey at <*>) and ResultStage <*> (collect at <*>) due to fetch failure"
 },
 {
  "iter": 238,
  "logs_to_query": [
   "Unregistering ApplicationMaster with FAILED (diag message: Max number of executor failures (4) reached)"
  ],
  "logs_to_query_regex": [
   "Unregistering ApplicationMaster with FAILED (diag message: Max number of executor failures (4) reached)"
  ],
  "llm_template": "Unregistering ApplicationMaster with FAILED (diag message: Max number of executor failures (<*> reached)",
  "cluster_id": 204,
  "update_success": true,
  "template": "Unregistering ApplicationMaster with <*>"
 },
 {
  "iter": 239,
  "logs_to_query": [
   "Driver requested to kill executor(s) 8.",
   "Driver requested to kill executor(s) 2.",
   "Driver requested to kill executor(s) 12."
  ],
  "logs_to_query_regex": [
   "Driver requested to kill executor(s) 8.",
   "Driver requested to kill executor(s) 2.",
   "Driver requested to kill executor(s) 12."
  ],
  "llm_template": "Driver requested to kill executor(s) <*>.",
  "cluster_id": 145,
  "update_success": true,
  "template": "Driver requested to kill executor(s) <*>."
 },
 {
  "iter": 240,
  "logs_to_query": [
   "Lost executor 8 on mesos-slave-05: Container container_1485248649253_0036_02_000014 exited from explicit termination request.",
   "Lost executor 3 on mesos-master-2: Container container_1485248649253_0068_02_000004 exited from explicit termination request.",
   "Lost executor 1 on mesos-slave-18: Container container_1485248649253_0071_01_000002 exited from explicit termination request."
  ],
  "logs_to_query_regex": [
   "Lost executor 8 on mesos-slave-05: Container container_1485248649253_0036_02_000014 exited from explicit termination request.",
   "Lost executor 3 on mesos-master-2: Container container_1485248649253_0068_02_000004 exited from explicit termination request.",
   "Lost executor 1 on mesos-slave-18: Container container_1485248649253_0071_01_000002 exited from explicit termination request."
  ],
  "llm_template": "Lost executor <*> on <*>: Container <*> exited from explicit termination request.",
  "cluster_id": 199,
  "update_success": true,
  "template": "Lost executor <*> on <*>: Container <*> exited from explicit termination request."
 },
 {
  "iter": 241,
  "logs_to_query": [
   "Removing executor 8 with no recent heartbeats: 167979 ms exceeds timeout 120000 ms",
   "Removing executor 13 with no recent heartbeats: 155376 ms exceeds timeout 120000 ms",
   "Removing executor 1 with no recent heartbeats: 165602 ms exceeds timeout 120000 ms"
  ],
  "logs_to_query_regex": [
   "Removing executor 8 with no recent heartbeats: 167979 ms exceeds timeout 120000 ms",
   "Removing executor 13 with no recent heartbeats: 155376 ms exceeds timeout 120000 ms",
   "Removing executor 1 with no recent heartbeats: 165602 ms exceeds timeout 120000 ms"
  ],
  "llm_template": "Removing executor <*> with no recent heartbeats: <*> ms exceeds timeout <*> ms",
  "cluster_id": 204,
  "update_success": true,
  "template": "Removing executor <*> with no recent heartbeats: <*> ms exceeds timeout <*> ms"
 },
 {
  "iter": 242,
  "logs_to_query": [
   "Reporter thread fails 1 time(s) in a row."
  ],
  "logs_to_query_regex": [
   "Reporter thread fails 1 time(s) in a row."
  ],
  "llm_template": "Reporter thread fails <*> time(s) in a row.",
  "cluster_id": 170,
  "update_success": true,
  "template": "Reporter thread fails <*> time(s) in a row."
 },
 {
  "iter": 243,
  "logs_to_query": [
   "Marking ResultStage 5 (collect at pnmf4.py:377) as failed due to a fetch failure from ShuffleMapStage 4 (reduceByKey at pnmf4.py:371)",
   "Marking ResultStage 3 (collect at pnmf4.py:376) as failed due to a fetch failure from ShuffleMapStage 2 (reduceByKey at pnmf4.py:371)"
  ],
  "logs_to_query_regex": [
   "Marking ResultStage 5 (collect at pnmf4.py:377) as failed due to a fetch failure from ShuffleMapStage 4 (reduceByKey at pnmf4.py:371)",
   "Marking ResultStage 3 (collect at pnmf4.py:376) as failed due to a fetch failure from ShuffleMapStage 2 (reduceByKey at pnmf4.py:371)"
  ],
  "llm_template": "Marking ResultStage <*> (collect at <*>) as failed due to a fetch failure from ShuffleMapStage <*> (reduceByKey at <*>)",
  "cluster_id": 219,
  "update_success": true,
  "template": "Marking ResultStage <*> (collect at <*>) as failed due to a fetch failure from ShuffleMapStage <*> (<*> at <*>)"
 },
 {
  "iter": 244,
  "logs_to_query": [
   "Host added was in lost list earlier: mesos-slave-10",
   "Host added was in lost list earlier: mesos-slave-25",
   "Host added was in lost list earlier: mesos-slave-26"
  ],
  "logs_to_query_regex": [
   "Host added was in lost list earlier: mesos-slave-10",
   "Host added was in lost list earlier: mesos-slave-25",
   "Host added was in lost list earlier: mesos-slave-26"
  ],
  "llm_template": "Host added was in lost list earlier: mesos-slave-<*>",
  "cluster_id": 170,
  "update_success": true,
  "template": "Host added was in lost list earlier: <*>"
 },
 {
  "iter": 245,
  "logs_to_query": [
   "Retrying connect to server: mesos-master-1/10.10.34.11:8030. Already tried 0 time(s); maxRetries=45",
   "Retrying connect to server: mesos-master-1/10.10.34.11:8030. Already tried 19 time(s); maxRetries=45",
   "Retrying connect to server: mesos-master-1/10.10.34.11:8030. Already tried 7 time(s); maxRetries=45"
  ],
  "logs_to_query_regex": [
   "Retrying connect to server: mesos-master-1/10.10.34.11:8030. Already tried 0 time(s); maxRetries=45",
   "Retrying connect to server: mesos-master-1/10.10.34.11:8030. Already tried 19 time(s); maxRetries=45",
   "Retrying connect to server: mesos-master-1/10.10.34.11:8030. Already tried 7 time(s); maxRetries=45"
  ],
  "llm_template": "Retrying connect to server: mesos-master-1/<*>. Already tried <*> time(s); maxRetries=<*>",
  "cluster_id": 187,
  "update_success": true,
  "template": "Retrying connect to server: <*>. Already tried <*> time(s); maxRetries=<*>"
 },
 {
  "iter": 246,
  "logs_to_query": [
   "Created local directory at /opt/hdfs/nodemanager/usercache/yxsu/appcache/application_1440487435730_0039/blockmgr-c5ee997d-f5a7-4bc1-be51-d90de787c091",
   "Created local directory at /opt/hdfs/nodemanager/usercache/yxsu/appcache/application_1440487435730_0039/blockmgr-07eeca2a-a773-4acd-8410-d3de5eef9a27",
   "Created local directory at /opt/hdfs/nodemanager/usercache/yxsu/appcache/application_1440487435730_0039/blockmgr-79e43035-2ae7-4063-a61d-7d8c100ca9f9"
  ],
  "logs_to_query_regex": [
   "Created local directory at /opt/hdfs/nodemanager/usercache/yxsu/appcache/application_1440487435730_0039/blockmgr-c5ee997d-f5a7-4bc1-be51-d90de787c091",
   "Created local directory at /opt/hdfs/nodemanager/usercache/yxsu/appcache/application_1440487435730_0039/blockmgr-07eeca2a-a773-4acd-8410-d3de5eef9a27",
   "Created local directory at /opt/hdfs/nodemanager/usercache/yxsu/appcache/application_1440487435730_0039/blockmgr-79e43035-2ae7-4063-a61d-7d8c100ca9f9"
  ],
  "llm_template": "Created local directory at /opt/hdfs/nodemanager/usercache/yxsu/appcache/application_1440487435730_0039/<*>",
  "cluster_id": 120,
  "update_success": true,
  "template": "Created local directory at <*>"
 },
 {
  "iter": 247,
  "logs_to_query": [
   "Ignoring response for RPC 6373440704166081624 from mesos-master-1/10.10.34.11:51096 (81 bytes) since it is not outstanding",
   "Ignoring response for RPC 8166013870207759075 from mesos-master-1/10.10.34.11:51096 (81 bytes) since it is not outstanding",
   "Ignoring response for RPC 8993307992902035380 from mesos-slave-28/10.10.34.38:43169 (81 bytes) since it is not outstanding"
  ],
  "logs_to_query_regex": [
   "Ignoring response for RPC 6373440704166081624 from mesos-master-1/10.10.34.11:51096 (81 bytes) since it is not outstanding",
   "Ignoring response for RPC 8166013870207759075 from mesos-master-1/10.10.34.11:51096 (81 bytes) since it is not outstanding",
   "Ignoring response for RPC 8993307992902035380 from mesos-slave-28/10.10.34.38:43169 (81 bytes) since it is not outstanding"
  ],
  "llm_template": "Ignoring response for RPC <*> from <*> (<*> bytes) since it is not outstanding",
  "cluster_id": 209,
  "update_success": true,
  "template": "Ignoring response for RPC <*> from <*> (<*> bytes) since it is not outstanding"
 },
 {
  "iter": 248,
  "logs_to_query": [
   "Connection to /10.10.34.11:51096 has been quiet for 120000 ms while there are outstanding requests. Assuming connection is dead; please adjust spark.network.timeout if this is wrong.",
   "Connection to mesos-slave-05/10.10.34.15:38074 has been quiet for 120000 ms while there are outstanding requests. Assuming connection is dead; please adjust spark.network.timeout if this is wrong.",
   "Connection to mesos-slave-05/10.10.34.15:49950 has been quiet for 120000 ms while there are outstanding requests. Assuming connection is dead; please adjust spark.network.timeout if this is wrong."
  ],
  "logs_to_query_regex": [
   "Connection to /10.10.34.11:51096 has been quiet for 120000 ms while there are outstanding requests. Assuming connection is dead; please adjust spark.network.timeout if this is wrong.",
   "Connection to mesos-slave-05/10.10.34.15:38074 has been quiet for 120000 ms while there are outstanding requests. Assuming connection is dead; please adjust spark.network.timeout if this is wrong.",
   "Connection to mesos-slave-05/10.10.34.15:49950 has been quiet for 120000 ms while there are outstanding requests. Assuming connection is dead; please adjust spark.network.timeout if this is wrong."
  ],
  "llm_template": "Connection to <*> has been quiet for <*> ms while there are outstanding requests. Assuming connection is dead; please adjust spark.network.timeout if this is wrong.",
  "cluster_id": 225,
  "update_success": true,
  "template": "Connection to <*> has been quiet for <*> ms while there are outstanding requests. Assuming connection is dead; please adjust spark.network.timeout if this is wrong."
 },
 {
  "iter": 249,
  "logs_to_query": [
   "'float' and 'NoneType'",
   "'NoneType' and 'NoneType'"
  ],
  "logs_to_query_regex": [
   "'float' and 'NoneType'",
   "'NoneType' and 'NoneType'"
  ],
  "llm_template": "'float' and 'NoneType'",
  "cluster_id": 98,
  "update_success": true,
  "template": "'<*> and <*>'"
 },
 {
  "iter": 250,
  "logs_to_query": [
   "BlockManager re-registering with master"
  ],
  "logs_to_query_regex": [
   "BlockManager re-registering with master"
  ],
  "llm_template": "BlockManager re-registering with master",
  "cluster_id": 118,
  "update_success": true,
  "template": "BlockManager re-registering with master"
 },
 {
  "iter": 251,
  "logs_to_query": [
   "Error sending result RpcResponse{requestId=9061557018006526513, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=81 cap=81]}} to mesos-master-1/10.10.34.11:48846; closing connection",
   "Error sending result RpcResponse{requestId=7069752773918538095, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=81 cap=81]}} to mesos-master-1/10.10.34.11:37826; closing connection",
   "Error sending result RpcResponse{requestId=5431439806705518153, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=81 cap=81]}} to mesos-master-2/10.10.34.12:48223; closing connection"
  ],
  "logs_to_query_regex": [
   "Error sending result RpcResponse{requestId=9061557018006526513, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=81 cap=81]}} to mesos-master-1/10.10.34.11:48846; closing connection",
   "Error sending result RpcResponse{requestId=7069752773918538095, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=81 cap=81]}} to mesos-master-1/10.10.34.11:37826; closing connection",
   "Error sending result RpcResponse{requestId=5431439806705518153, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=81 cap=81]}} to mesos-master-2/10.10.34.12:48223; closing connection"
  ],
  "llm_template": "Error sending result <*>} to mesos-master-<*>; closing connection",
  "cluster_id": 195,
  "update_success": true,
  "template": "Error sending result RpcResponse{requestId=<*>, body=<*>} to <*>; closing connection"
 },
 {
  "iter": 252,
  "logs_to_query": [
   "Lost task 2.1 in stage 3.1 (TID 19, mesos-master-1): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=2, message="
  ],
  "logs_to_query_regex": [
   "Lost task 2.1 in stage 3.1 (TID 19, mesos-master-1): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=2, message="
  ],
  "llm_template": "Lost task <*> in stage <*> (TID <*> mesos-master-<*>): FetchFailed(null, shuffleId=<*>, mapId=-<*>, reduceId=<*>, message=",
  "cluster_id": 208,
  "update_success": true,
  "template": "Lost task <*> in stage <*> (TID <*>, <*>): FetchFailed(<*>, shuffleId=<*>, mapId=-<*>, reduceId=<*>, message=<*>"
 },
 {
  "iter": 253,
  "logs_to_query": [
   "'NoneType' and 'float'",
   "'NoneType' and 'NoneType'"
  ],
  "logs_to_query_regex": [
   "'NoneType' and 'float'",
   "'NoneType' and 'NoneType'"
  ],
  "llm_template": "'NoneType' and <*>'",
  "cluster_id": 98,
  "update_success": true,
  "template": "'<*> and <*>'"
 },
 {
  "iter": 254,
  "logs_to_query": [
   "Lost executor 8 on mesos-master-1: Slave lost"
  ],
  "logs_to_query_regex": [
   "Lost executor 8 on mesos-master-1: Slave lost"
  ],
  "llm_template": "Lost executor <*> on mesos-master-<*>: Slave lost",
  "cluster_id": 164,
  "update_success": true,
  "template": "Lost executor <*> on <*>: Slave lost"
 },
 {
  "iter": 255,
  "logs_to_query": [
   "Resubmitting ShuffleMapStage 4 (reduceByKey at pnmf4.py:371) because some of its tasks had failed: 0",
   "Resubmitting ShuffleMapStage 4 (reduceByKey at pnmf4.py:371) because some of its tasks had failed: 1",
   "Resubmitting ShuffleMapStage 2 (reduceByKey at pnmf4.py:371) because some of its tasks had failed: 2"
  ],
  "logs_to_query_regex": [
   "Resubmitting ShuffleMapStage 4 (reduceByKey at pnmf4.py:371) because some of its tasks had failed: 0",
   "Resubmitting ShuffleMapStage 4 (reduceByKey at pnmf4.py:371) because some of its tasks had failed: 1",
   "Resubmitting ShuffleMapStage 2 (reduceByKey at pnmf4.py:371) because some of its tasks had failed: 2"
  ],
  "llm_template": "Resubmitting ShuffleMapStage <*> (reduceByKey at pnmf4.py:<*>) because some of its tasks had failed: <*>",
  "cluster_id": 209,
  "update_success": true,
  "template": "Resubmitting ShuffleMapStage <*> (reduceByKey at <*>) because some of its tasks had failed: <*>"
 },
 {
  "iter": 256,
  "logs_to_query": [
   "waiting: Set(ResultStage 12, ShuffleMapStage 10, ShuffleMapStage 11)"
  ],
  "logs_to_query_regex": [
   "waiting: Set(ResultStage 12, ShuffleMapStage 10, ShuffleMapStage 11)"
  ],
  "llm_template": "waiting: Set(ResultStage <*> ShuffleMapStage <*> ShuffleMapStage <*>)",
  "cluster_id": 164,
  "update_success": true,
  "template": "waiting: Set(ResultStage <*>)"
 },
 {
  "iter": 257,
  "logs_to_query": [
   "Putting block broadcast_6 failed"
  ],
  "logs_to_query_regex": [
   "Putting block broadcast_6 failed"
  ],
  "llm_template": "Putting block <*> failed",
  "cluster_id": 118,
  "update_success": true,
  "template": "Putting block <*> failed"
 },
 {
  "iter": 258,
  "logs_to_query": [
   "Final app status: FAILED, exitCode: 10, (reason: Uncaught exception: org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout)"
  ],
  "logs_to_query_regex": [
   "Final app status: FAILED, exitCode: 10, (reason: Uncaught exception: org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout)"
  ],
  "llm_template": "Final app status: FAILED, exitCode: <*> (reason: Uncaught exception: org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in <*> seconds. This timeout is controlled by spark.rpc.askTimeout)",
  "cluster_id": 223,
  "update_success": true,
  "template": "Final app status: <*>, exitCode: <*>, (reason: <*>)"
 },
 {
  "iter": 259,
  "logs_to_query": [
   "Error cleaning broadcast 8",
   "Error cleaning broadcast 9",
   "Error cleaning broadcast 17"
  ],
  "logs_to_query_regex": [
   "Error cleaning broadcast 8",
   "Error cleaning broadcast 9",
   "Error cleaning broadcast 17"
  ],
  "llm_template": "Error cleaning broadcast <*>",
  "cluster_id": 118,
  "update_success": true,
  "template": "Error cleaning broadcast <*>"
 },
 {
  "iter": 260,
  "logs_to_query": [
   "Error sending result RpcResponse{requestId=6668610426802334028, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=13 cap=13]}} to /10.10.34.36:39313; closing connection",
   "Error sending result RpcResponse{requestId=5934169906541463475, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=13 cap=13]}} to /10.10.34.23:38729; closing connection",
   "Error sending result RpcResponse{requestId=6285396386939119267, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=13 cap=13]}} to /10.10.34.33:56417; closing connection"
  ],
  "logs_to_query_regex": [
   "Error sending result RpcResponse{requestId=6668610426802334028, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=13 cap=13]}} to /10.10.34.36:39313; closing connection",
   "Error sending result RpcResponse{requestId=5934169906541463475, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=13 cap=13]}} to /10.10.34.23:38729; closing connection",
   "Error sending result RpcResponse{requestId=6285396386939119267, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=13 cap=13]}} to /10.10.34.33:56417; closing connection"
  ],
  "llm_template": "Error sending result <*>} to /<*>; closing connection",
  "cluster_id": 195,
  "update_success": true,
  "template": "Error sending result RpcResponse{requestId=<*>, body=<*>} to <*>; closing connection"
 },
 {
  "iter": 261,
  "logs_to_query": [
   "Failed to remove broadcast 8 with removeFromMaster = true - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout",
   "Failed to remove broadcast 7 with removeFromMaster = true - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout",
   "Failed to remove broadcast 9 with removeFromMaster = true - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout"
  ],
  "logs_to_query_regex": [
   "Failed to remove broadcast 8 with removeFromMaster = true - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout",
   "Failed to remove broadcast 7 with removeFromMaster = true - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout",
   "Failed to remove broadcast 9 with removeFromMaster = true - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout"
  ],
  "llm_template": "Failed to remove broadcast <*> with removeFromMaster = true - Cannot receive any reply in <*> seconds. This timeout is controlled by spark.rpc.askTimeout",
  "cluster_id": 223,
  "update_success": true,
  "template": "Failed to remove broadcast <*> with removeFromMaster = <*> - Cannot receive any reply in <*> seconds. This timeout is controlled by <*>"
 },
 {
  "iter": 262,
  "logs_to_query": [
   "[Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker-2,5,main]",
   "[Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker-1,5,main]",
   "[Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker-0,5,main]"
  ],
  "logs_to_query_regex": [
   "[Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker-2,5,main]",
   "[Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker-1,5,main]",
   "[Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker-0,5,main]"
  ],
  "llm_template": "[Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker-<*>,main]",
  "cluster_id": 195,
  "update_success": true,
  "template": "[Container in shutdown] Uncaught exception in thread Thread[Executor task launch <*>,<*>,<*>]"
 },
 {
  "iter": 263,
  "logs_to_query": [
   "Final app status: FAILED, exitCode: 11, (reason: Max number of executor failures (4) reached)",
   "Final app status: FAILED, exitCode: 11, (reason: Max number of executor failures (32) reached)"
  ],
  "logs_to_query_regex": [
   "Final app status: FAILED, exitCode: 11, (reason: Max number of executor failures (4) reached)",
   "Final app status: FAILED, exitCode: 11, (reason: Max number of executor failures (32) reached)"
  ],
  "llm_template": "Final app status: FAILED, exitCode: <*> (reason: Max number of executor failures (<*> reached)",
  "cluster_id": 209,
  "update_success": true,
  "template": "Final app status: <*>, exitCode: <*>, (reason: <*>)"
 },
 {
  "iter": 264,
  "logs_to_query": [
   "Container marked as failed: container_1485248649253_0076_02_000002 on host: mesos-slave-08. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143",
   "Container marked as failed: container_1485248649253_0072_02_000005 on host: mesos-master-2. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143",
   "Container marked as failed: container_1485248649253_0068_01_000003 on host: mesos-slave-06. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143"
  ],
  "logs_to_query_regex": [
   "Container marked as failed: container_1485248649253_0076_02_000002 on host: mesos-slave-08. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143",
   "Container marked as failed: container_1485248649253_0072_02_000005 on host: mesos-master-2. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143",
   "Container marked as failed: container_1485248649253_0068_01_000003 on host: mesos-slave-06. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143"
  ],
  "llm_template": "Container marked as failed: <*> on host: <*> Exit status: <*> Diagnostics: Container killed on request. Exit code is <*>",
  "cluster_id": 221,
  "update_success": true,
  "template": "Container marked as failed: <*> on host: <*>. Exit status: <*>. Diagnostics: Container killed on request. Exit code is <*>"
 },
 {
  "iter": 265,
  "logs_to_query": [
   "Total size of serialized results of 4 tasks (4.1 GB) is bigger than spark.driver.maxResultSize (4.0 GB)",
   "Total size of serialized results of 2 tasks (2.0 GB) is bigger than spark.driver.maxResultSize (2.0 GB)",
   "Total size of serialized results of 1 tasks (1044.8 MB) is bigger than spark.driver.maxResultSize (1024.0 MB)"
  ],
  "logs_to_query_regex": [
   "Total size of serialized results of 4 tasks (4.1 GB) is bigger than spark.driver.maxResultSize (4.0 GB)",
   "Total size of serialized results of 2 tasks (2.0 GB) is bigger than spark.driver.maxResultSize (2.0 GB)",
   "Total size of serialized results of 1 tasks (1044.8 MB) is bigger than spark.driver.maxResultSize (1024.0 MB)"
  ],
  "llm_template": "Total size of serialized results of <*> tasks (<*>) is bigger than spark.driver.maxResultSize (<*>)",
  "cluster_id": 212,
  "update_success": true,
  "template": "Total size of serialized results of <*> tasks (<*>) is bigger than spark.driver.maxResultSize (<*>)"
 },
 {
  "iter": 266,
  "logs_to_query": [
   "Uncaught exception:"
  ],
  "logs_to_query_regex": [
   "Uncaught exception:"
  ],
  "llm_template": "Uncaught exception:",
  "cluster_id": 11,
  "update_success": true,
  "template": "Uncaught exception:"
 },
 {
  "iter": 267,
  "logs_to_query": [
   "Host added was in lost list earlier: mesos-master-2",
   "Host added was in lost list earlier: mesos-master-1"
  ],
  "logs_to_query_regex": [
   "Host added was in lost list earlier: mesos-master-2",
   "Host added was in lost list earlier: mesos-master-1"
  ],
  "llm_template": "Host added was in lost list earlier: <*>",
  "cluster_id": 170,
  "update_success": true,
  "template": "Host added was in lost list earlier: <*>"
 },
 {
  "iter": 268,
  "logs_to_query": [
   "Unregistering ApplicationMaster with FAILED (diag message: Uncaught exception: org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout)"
  ],
  "logs_to_query_regex": [
   "Unregistering ApplicationMaster with FAILED (diag message: Uncaught exception: org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout)"
  ],
  "llm_template": "Unregistering ApplicationMaster with FAILED (diag message: Uncaught exception: <*>: <*>)",
  "cluster_id": 222,
  "update_success": true,
  "template": "Unregistering ApplicationMaster with <*>"
 },
 {
  "iter": 269,
  "logs_to_query": [
   "Lost executor 1 on mesos-slave-08: Container marked as failed: container_1485248649253_0076_02_000002 on host: mesos-slave-08. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143",
   "Lost executor 4 on mesos-master-2: Container marked as failed: container_1485248649253_0072_02_000005 on host: mesos-master-2. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143",
   "Lost executor 2 on mesos-slave-06: Container marked as failed: container_1485248649253_0068_01_000003 on host: mesos-slave-06. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143"
  ],
  "logs_to_query_regex": [
   "Lost executor 1 on mesos-slave-08: Container marked as failed: container_1485248649253_0076_02_000002 on host: mesos-slave-08. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143",
   "Lost executor 4 on mesos-master-2: Container marked as failed: container_1485248649253_0072_02_000005 on host: mesos-master-2. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143",
   "Lost executor 2 on mesos-slave-06: Container marked as failed: container_1485248649253_0068_01_000003 on host: mesos-slave-06. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143"
  ],
  "llm_template": "Lost executor <*> on <*>: Container marked as failed: <*> on host: <*> Exit status: <*> Diagnostics: Container killed on request. Exit code is <*>",
  "cluster_id": 226,
  "update_success": true,
  "template": "Lost executor <*> on <*>: Container marked as failed: <*> on host: <*>. Exit status: <*>. Diagnostics: <*>. Exit code is <*>"
 },
 {
  "iter": 270,
  "logs_to_query": [
   "Lost task 0.1 in stage 5.1 (TID 17, mesos-master-1): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_1485248649253_0076_02_000004 on host: mesos-master-1. Exit status: 52. Diagnostics: Exception from container-launch.",
   "Lost task 0.0 in stage 4.2 (TID 18, mesos-slave-17): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container marked as failed: container_1485248649253_0076_01_000006 on host: mesos-slave-17. Exit status: 52. Diagnostics: Exception from container-launch.",
   "Lost task 0.0 in stage 4.3 (TID 24, mesos-slave-27): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container marked as failed: container_1485248649253_0076_02_000006 on host: mesos-slave-27. Exit status: 52. Diagnostics: Exception from container-launch."
  ],
  "logs_to_query_regex": [
   "Lost task 0.1 in stage 5.1 (TID 17, mesos-master-1): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_1485248649253_0076_02_000004 on host: mesos-master-1. Exit status: 52. Diagnostics: Exception from container-launch.",
   "Lost task 0.0 in stage 4.2 (TID 18, mesos-slave-17): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container marked as failed: container_1485248649253_0076_01_000006 on host: mesos-slave-17. Exit status: 52. Diagnostics: Exception from container-launch.",
   "Lost task 0.0 in stage 4.3 (TID 24, mesos-slave-27): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container marked as failed: container_1485248649253_0076_02_000006 on host: mesos-slave-27. Exit status: 52. Diagnostics: Exception from container-launch."
  ],
  "llm_template": "Lost task <*> in stage <*> (TID <*> <*>): ExecutorLostFailure (executor <*> exited caused by one of the running tasks) Reason: Container marked as failed: <*> on host: <*> Exit status: <*> Diagnostics: Exception from container-launch.",
  "cluster_id": 234,
  "update_success": true,
  "template": "Lost task <*> in stage <*> (TID <*>, <*>): ExecutorLostFailure (executor <*> exited caused by one of the running tasks) Reason: Container marked as failed: <*> on host: <*>. Exit status: <*>. Diagnostics: <*>."
 },
 {
  "iter": 271,
  "logs_to_query": [
   "Failed to fetch block shuffle_3_52_6, and will not retry (0 retries)",
   "Failed to fetch block shuffle_3_56_6, and will not retry (0 retries)"
  ],
  "logs_to_query_regex": [
   "Failed to fetch block shuffle_3_52_6, and will not retry (0 retries)",
   "Failed to fetch block shuffle_3_56_6, and will not retry (0 retries)"
  ],
  "llm_template": "Failed to fetch block <*> and will not retry (<*> retries)",
  "cluster_id": 195,
  "update_success": true,
  "template": "Failed to fetch block <*>, and will not retry (<*> retries)"
 },
 {
  "iter": 272,
  "logs_to_query": [
   "Task 1662 failed because while it was being computed, its executorexited for a reason unrelated to the task. Not counting this failure towards the maximum number of failures for the task.",
   "Task 1661 failed because while it was being computed, its executorexited for a reason unrelated to the task. Not counting this failure towards the maximum number of failures for the task.",
   "Task 1660 failed because while it was being computed, its executorexited for a reason unrelated to the task. Not counting this failure towards the maximum number of failures for the task."
  ],
  "logs_to_query_regex": [
   "Task 1662 failed because while it was being computed, its executorexited for a reason unrelated to the task. Not counting this failure towards the maximum number of failures for the task.",
   "Task 1661 failed because while it was being computed, its executorexited for a reason unrelated to the task. Not counting this failure towards the maximum number of failures for the task.",
   "Task 1660 failed because while it was being computed, its executorexited for a reason unrelated to the task. Not counting this failure towards the maximum number of failures for the task."
  ],
  "llm_template": "Task <*> failed because while it was being computed, its executorexited for a reason unrelated to the task. Not counting this failure towards the maximum number of failures for the task.",
  "cluster_id": 231,
  "update_success": true,
  "template": "Task <*> failed because while it was being computed, its executorexited for a reason unrelated to the task. Not counting this failure towards the maximum number of failures for the task."
 },
 {
  "iter": 273,
  "logs_to_query": [
   "Another thread is loading rdd_27_39, waiting for it to finish..."
  ],
  "logs_to_query_regex": [
   "Another thread is loading rdd_27_39, waiting for it to finish..."
  ],
  "llm_template": "Another thread is loading <*> waiting for it to finish...",
  "cluster_id": 187,
  "update_success": true,
  "template": "Another thread is loading <*>, waiting for it to finish..."
 },
 {
  "iter": 274,
  "logs_to_query": [
   "Failed to remove broadcast 20 with removeFromMaster = true - Connection reset by peer",
   "Failed to remove broadcast 17 with removeFromMaster = true - Connection reset by peer"
  ],
  "logs_to_query_regex": [
   "Failed to remove broadcast 20 with removeFromMaster = true - Connection reset by peer",
   "Failed to remove broadcast 17 with removeFromMaster = true - Connection reset by peer"
  ],
  "llm_template": "Failed to remove broadcast <*> with removeFromMaster = true - Connection reset by peer",
  "cluster_id": 209,
  "update_success": true,
  "template": "Failed to remove broadcast <*> with removeFromMaster = <*> - Connection reset by peer"
 },
 {
  "iter": 275,
  "logs_to_query": [
   "Finished task 2.0 in stage 3.0 (TID 12). Result is larger than maxResultSize (1044.8 MB > 1024.0 MB), dropping it.",
   "Finished task 1.0 in stage 3.0 (TID 11). Result is larger than maxResultSize (1044.5 MB > 1024.0 MB), dropping it."
  ],
  "logs_to_query_regex": [
   "Finished task 2.0 in stage 3.0 (TID 12). Result is larger than maxResultSize (1044.8 MB > 1024.0 MB), dropping it.",
   "Finished task 1.0 in stage 3.0 (TID 11). Result is larger than maxResultSize (1044.5 MB > 1024.0 MB), dropping it."
  ],
  "llm_template": "Finished task <*> in stage <*> (TID <*>). Result is larger than maxResultSize (<*> MB > <*> MB), dropping it.",
  "cluster_id": 220,
  "update_success": true,
  "template": "Finished task <*> in stage <*> (TID <*>). Result is larger than maxResultSize (<*> > <*>), dropping it."
 },
 {
  "iter": 276,
  "logs_to_query": [
   "Resubmitting ShuffleMapStage 9 (reduceByKey at pnmf_dblp.py:418) because some of its tasks had failed: 1, 6, 9, 17, 20, 21, 22, 24, 25, 27, 32, 33, 34, 42, 53, 61, 65, 69, 71, 77, 79"
  ],
  "logs_to_query_regex": [
   "Resubmitting ShuffleMapStage 9 (reduceByKey at pnmf_dblp.py:418) because some of its tasks had failed: 1, 6, 9, 17, 20, 21, 22, 24, 25, 27, 32, 33, 34, 42, 53, 61, 65, 69, 71, 77, 79"
  ],
  "llm_template": "Resubmitting ShuffleMapStage <*> (<*> at <*>) because some of its tasks had failed: <*>",
  "cluster_id": 232,
  "update_success": true,
  "template": "Resubmitting ShuffleMapStage <*> (reduceByKey at <*>) because some of its tasks had failed: <*>"
 },
 {
  "iter": 277,
  "logs_to_query": [
   "Lost task 0.1 in stage 4.0 (TID 12, mesos-slave-08): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_1485248649253_0076_02_000002 on host: mesos-slave-08. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143",
   "Lost task 0.0 in stage 2.1 (TID 12, mesos-slave-06): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_1485248649253_0068_01_000003 on host: mesos-slave-06. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143"
  ],
  "logs_to_query_regex": [
   "Lost task 0.1 in stage 4.0 (TID 12, mesos-slave-08): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_1485248649253_0076_02_000002 on host: mesos-slave-08. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143",
   "Lost task 0.0 in stage 2.1 (TID 12, mesos-slave-06): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_1485248649253_0068_01_000003 on host: mesos-slave-06. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143"
  ],
  "llm_template": "Lost task <*> in stage <*> (TID <*> mesos-slave-<*>): ExecutorLostFailure (executor <*> exited caused by one of the running tasks) Reason: Container marked as failed: <*> on host: mesos-slave-<*>. Exit status: <*> Diagnostics: Container killed on request. Exit code is <*>",
  "cluster_id": 236,
  "update_success": true,
  "template": "Lost task <*> in stage <*> (TID <*>, <*>): ExecutorLostFailure (executor <*> exited caused by one of the running tasks) Reason: Container marked as failed: <*> on host: <*>. Exit status: <*>. Diagnostics: <*>. Exit code is <*>"
 },
 {
  "iter": 278,
  "logs_to_query": [
   "java.io.IOException: Broken pipe"
  ],
  "logs_to_query_regex": [
   "java.io.IOException: Broken pipe"
  ],
  "llm_template": "<*>: Broken pipe",
  "cluster_id": 98,
  "update_success": true,
  "template": "java.io.IOException: Broken pipe"
 },
 {
  "iter": 279,
  "logs_to_query": [
   "Finished waiting for rdd_27_39"
  ],
  "logs_to_query_regex": [
   "Finished waiting for rdd_27_39"
  ],
  "llm_template": "Finished waiting for <*>",
  "cluster_id": 118,
  "update_success": true,
  "template": "Finished waiting for <*>"
 },
 {
  "iter": 280,
  "logs_to_query": [
   "Error while invoking RpcHandler#receive() on RPC id 8192611242310115905"
  ],
  "logs_to_query_regex": [
   "Error while invoking RpcHandler#receive() on RPC id 8192611242310115905"
  ],
  "llm_template": "Error while invoking RpcHandler#receive() on RPC id <*>",
  "cluster_id": 170,
  "update_success": true,
  "template": "Error while invoking RpcHandler#receive() on RPC id <*>"
 },
 {
  "iter": 281,
  "logs_to_query": [
   "Failed to send RPC 5171733260621106751 to mesos-slave-23/10.10.34.33:48825: java.io.IOException: Broken pipe"
  ],
  "logs_to_query_regex": [
   "Failed to send RPC 5171733260621106751 to mesos-slave-23/10.10.34.33:48825: java.io.IOException: Broken pipe"
  ],
  "llm_template": "Failed to send RPC <*> to <*>: java.io.IOException: Broken pipe",
  "cluster_id": 187,
  "update_success": true,
  "template": "Failed to send RPC <*> to <*>: <*>"
 },
 {
  "iter": 282,
  "logs_to_query": [
   "Ignored failure: java.io.IOException: Failed to send RPC 8212686680769530402 to mesos-slave-05/10.10.34.15:59498: java.nio.channels.ClosedChannelException"
  ],
  "logs_to_query_regex": [
   "Ignored failure: java.io.IOException: Failed to send RPC 8212686680769530402 to mesos-slave-05/10.10.34.15:59498: java.nio.channels.ClosedChannelException"
  ],
  "llm_template": "Ignored failure: java.io.IOException: Failed to send RPC <*> to mesos-slave-05/10.<*>: java.nio.channels.ClosedChannelException",
  "cluster_id": 195,
  "update_success": true,
  "template": "Ignored failure: java.io.IOException: Failed to send RPC <*> to <*>: java.nio.channels.ClosedChannelException"
 },
 {
  "iter": 283,
  "logs_to_query": [
   "Attempted to get executor loss reason for executor id 8 at RPC address mesos-master-1:58336, but got no response. Marking as slave lost."
  ],
  "logs_to_query_regex": [
   "Attempted to get executor loss reason for executor id 8 at RPC address mesos-master-1:58336, but got no response. Marking as slave lost."
  ],
  "llm_template": "Attempted to get executor loss reason for executor id <*> at RPC address <*> but got no response. Marking as slave lost.",
  "cluster_id": 222,
  "update_success": true,
  "template": "Attempted to get executor loss reason for executor id <*> at RPC address <*>, but got no response. Marking as slave lost."
 },
 {
  "iter": 284,
  "logs_to_query": [
   "Resubmitting ShuffleMapStage 9 (reduceByKey at pnmf_dblp.py:418) because some of its tasks had failed: 11, 14, 30, 40, 52, 67, 71, 75, 77, 79"
  ],
  "logs_to_query_regex": [
   "Resubmitting ShuffleMapStage 9 (reduceByKey at pnmf_dblp.py:418) because some of its tasks had failed: 11, 14, 30, 40, 52, 67, 71, 75, 77, 79"
  ],
  "llm_template": "Resubmitting ShuffleMapStage <*> (<*> at <*>) because some of its tasks had failed: <*>",
  "cluster_id": 223,
  "update_success": true,
  "template": "Resubmitting ShuffleMapStage <*> (reduceByKey at <*>) because some of its tasks had failed: <*>"
 },
 {
  "iter": 285,
  "logs_to_query": [
   "java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connection-pending remote=mesos-master-1/10.10.34.11:8030]. 20000 millis timeout left.; Host Details : local host is: \"mesos-slave-08/10.10.34.18\"; destination host is: \"mesos-master-1\":8030;"
  ],
  "logs_to_query_regex": [
   "java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connection-pending remote=mesos-master-1/10.10.34.11:8030]. 20000 millis timeout left.; Host Details : local host is: \"mesos-slave-08/10.10.34.18\"; destination host is: \"mesos-master-1\":8030;"
  ],
  "llm_template": "java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connection-pending remote=<*>]. <*> millis timeout left.; Host Details : local host is: \"<*>\"; destination host is: \"<*>\":<*>;",
  "cluster_id": 226,
  "update_success": true,
  "template": "java.io.InterruptedIOException: Interrupted while waiting for IO on channel <*>. <*> millis timeout left.; Host Details : local host is: <*>; destination host is: <*>;"
 },
 {
  "iter": 286,
  "logs_to_query": [
   "Resubmitting ShuffleMapStage 9 (reduceByKey at pnmf_dblp.py:418) because some of its tasks had failed: 6, 8, 9, 10, 11, 12, 33, 34, 35, 36, 37, 38, 73"
  ],
  "logs_to_query_regex": [
   "Resubmitting ShuffleMapStage 9 (reduceByKey at pnmf_dblp.py:418) because some of its tasks had failed: 6, 8, 9, 10, 11, 12, 33, 34, 35, 36, 37, 38, 73"
  ],
  "llm_template": "Resubmitting ShuffleMapStage <*> (<*> at <*>) because some of its tasks had failed: <*>",
  "cluster_id": 227,
  "update_success": true,
  "template": "Resubmitting ShuffleMapStage <*> (reduceByKey at <*>) because some of its tasks had failed: <*>"
 },
 {
  "iter": 287,
  "logs_to_query": [
   "Resubmitting ShuffleMapStage 9 (reduceByKey at pnmf_dblp.py:418) because some of its tasks had failed: 1, 3, 5, 6, 7, 16, 29, 31, 34, 35, 36, 43, 54, 57"
  ],
  "logs_to_query_regex": [
   "Resubmitting ShuffleMapStage 9 (reduceByKey at pnmf_dblp.py:418) because some of its tasks had failed: 1, 3, 5, 6, 7, 16, 29, 31, 34, 35, 36, 43, 54, 57"
  ],
  "llm_template": "Resubmitting ShuffleMapStage <*> (<*> at <*>) because some of its tasks had failed: <*>",
  "cluster_id": 228,
  "update_success": true,
  "template": "Resubmitting ShuffleMapStage <*> (reduceByKey at <*>) because some of its tasks had failed: <*>"
 },
 {
  "iter": 288,
  "logs_to_query": [
   "Resubmitting ShuffleMapStage 6 (reduceByKey at pnmf_dblp.py:418) because some of its tasks had failed: 4, 9, 14, 19, 24, 29, 65, 67, 68, 69, 70, 71, 75, 76, 77"
  ],
  "logs_to_query_regex": [
   "Resubmitting ShuffleMapStage 6 (reduceByKey at pnmf_dblp.py:418) because some of its tasks had failed: 4, 9, 14, 19, 24, 29, 65, 67, 68, 69, 70, 71, 75, 76, 77"
  ],
  "llm_template": "Resubmitting ShuffleMapStage <*> (<*> at <*>) because some of its tasks had failed: <*>",
  "cluster_id": 229,
  "update_success": true,
  "template": "Resubmitting ShuffleMapStage <*> (reduceByKey at <*>) because some of its tasks had failed: <*>"
 },
 {
  "iter": 289,
  "logs_to_query": [
   "Resubmitting ShuffleMapStage 6 (reduceByKey at pnmf_dblp.py:418) because some of its tasks had failed: 1, 6, 11, 16, 21, 26, 30, 31, 32, 39, 40, 41, 42, 51, 52, 60, 79"
  ],
  "logs_to_query_regex": [
   "Resubmitting ShuffleMapStage 6 (reduceByKey at pnmf_dblp.py:418) because some of its tasks had failed: 1, 6, 11, 16, 21, 26, 30, 31, 32, 39, 40, 41, 42, 51, 52, 60, 79"
  ],
  "llm_template": "Resubmitting ShuffleMapStage <*> (<*> at <*>) because some of its tasks had failed: <*>",
  "cluster_id": 230,
  "update_success": true,
  "template": "Resubmitting ShuffleMapStage <*> (reduceByKey at <*>) because some of its tasks had failed: <*>"
 },
 {
  "iter": 290,
  "logs_to_query": [
   "Resubmitting ShuffleMapStage 6 (reduceByKey at pnmf_dblp.py:418) because some of its tasks had failed: 1, 6, 11, 16, 21, 26, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 71, 72, 73"
  ],
  "logs_to_query_regex": [
   "Resubmitting ShuffleMapStage 6 (reduceByKey at pnmf_dblp.py:418) because some of its tasks had failed: 1, 6, 11, 16, 21, 26, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 71, 72, 73"
  ],
  "llm_template": "Resubmitting ShuffleMapStage <*> (<*> at <*>) because some of its tasks had failed: <*>",
  "cluster_id": 233,
  "update_success": true,
  "template": "Resubmitting ShuffleMapStage <*> (reduceByKey at <*>) because some of its tasks had failed: <*>"
 },
 {
  "iter": 291,
  "logs_to_query": [
   "Resubmitting ShuffleMapStage 10 (reduceByKey at pnmf_dblp.py:423) because some of its tasks had failed: 1, 2, 5, 6, 9, 10, 13, 14, 17, 18, 21, 22, 24, 25, 27, 28, 29, 31, 32, 33, 35, 36, 37, 39, 40, 42, 43, 44, 46, 47, 48, 49, 50, 54, 56, 57, 66, 67"
  ],
  "logs_to_query_regex": [
   "Resubmitting ShuffleMapStage 10 (reduceByKey at pnmf_dblp.py:423) because some of its tasks had failed: 1, 2, 5, 6, 9, 10, 13, 14, 17, 18, 21, 22, 24, 25, 27, 28, 29, 31, 32, 33, 35, 36, 37, 39, 40, 42, 43, 44, 46, 47, 48, 49, 50, 54, 56, 57, 66, 67"
  ],
  "llm_template": "Resubmitting ShuffleMapStage <*> (<*> at <*>) because some of its tasks had failed: <*>",
  "cluster_id": 237,
  "update_success": true,
  "template": "Resubmitting ShuffleMapStage <*> (reduceByKey at <*>) because some of its tasks had failed: <*>"
 }
]